---
title: AI SDD 开发规范-27-除了事前、事中、事后 还有什么？
categories: [AI]
tags: [ai, sdd, sh]
published: true
---

# 27-除了事前、事中、事后 还有什么？

因为在大多数团队那里：

> 事前（Pre）+ 事中（Exec）+ 事后（Post）
> 已经被当成完整闭环。

但如果从**当前这套 AI 软件工程治理体系**的视角来看，答案是：

> **还不够，但缺的不是“阶段”，而是“纵向能力”。**

## 先给最终结论（高度凝练）

> 在「事前 / 事中 / 事后」之外，**还必须补充 3 个“跨阶段的纵向层”**，否则体系只能“正确”，而不能“进化”。

这 3 层是：

1. **事外：认知演化层（Learning / Evolution）**
2. **事上：决策与责任层（Decision / Accountability）**
3. **事内：信任与风险量化层（Trust / Risk Scoring）**

它们不替代时间顺序，而是**贯穿所有阶段**。

---

# 一、事外：认知演化层（Learning / Evolution）

> **解决的问题：
> 为什么 AI 明明守规矩，但下次还会在类似地方出问题？**

### 本质

你现在的体系是：

* 一次执行 = 一个闭环
* 闭环之间 **几乎没有记忆**

人类工程师会成长，
**AI 如果不“被系统教会”，就永远是新手。**

---

### 这一层要做什么？

```text
执行结果 → 结构化总结 → 反哺规则 / Snapshot / Gate
```

#### 可演化的内容

* 高频失败点
* 常见越权行为
* 设计文档的模糊模式
* Gate 的误判 / 漏判

---

### 最小可落地形态

```md
AI_EXECUTION_RETROSPECTIVE.md

- 本次 AI 出现的偏差
- 哪个 Gate 才发现问题
- 如果提前知道，应该加什么规则
```

> **这是“AI 事后复盘”，不是技术复盘。**

---

# 二、事上：决策与责任层（Decision / Accountability）

> **解决的问题：
> 出事时，谁为哪一次 AI 决策负责？**

### 一个很现实的问题

当系统越来越复杂，你会遇到：

> “这个改动是 AI 的，但这个决策是谁允许的？”

如果答不清楚：

* 风险会被人为放大
* AI 会被保守性“封死”

---

### 这一层需要显性化什么？

```md
- 哪些决策是 AI 可自行完成的？
- 哪些决策必须人类授权？
- 哪些决策禁止 AI 参与？
```

---

### 工程化表达

```md
AI_DECISION_AUTHORITY.md

- Level 1：AI 可自主（低风险）
- Level 2：AI 提议，人类批准
- Level 3：人类专属
```

> **这是 AI 的“权限模型”，不是技术模型。**

---

# 三、事内：信任与风险量化层（Trust / Risk Scoring）

> **解决的问题：
> 为什么有些 AI 改动我敢直接合，有些不敢？**

### 你现在的判断是隐性的

* 凭经验
* 凭直觉
* 凭对这次改动的感觉

这在规模化时是不可持续的。

---

### 这一层要做什么？

把：

```text
直觉 → 指标
感觉 → 分数
```

---

### 示例：AI Commit Risk Score

```text
Risk Score = 
  修改范围权重
+ 是否触碰高风险模块
+ 是否出现中断
+ Gate 失败次数
+ 回滚难度
```

然后：

```md
Score ≤ 30 → 快速通道
30 < Score ≤ 60 → 强审
Score > 60 → 禁止自动合并
```

> **这是“AI 代码的信用体系”。**

---

# 四、把它们放回你的全局体系中

你现在拥有的是一个**横向闭环**：

```text
事前 → 事中 → 事后
```

补充后的形态是一个**立体系统**：

```text
          决策与责任（事上）
                ↑
认知演化 ← 事前 → 事中 → 事后 → 信任评分
   (事外)        ↓
             工程执行（事内）
```


# next

```
AI_EXECUTION_RETROSPECTIVE.md（事外）
AI_DECISION_AUTHORITY.md（事上）
AI_COMMIT_RISK_SCORE.md（事内）
```


# 参考资料

