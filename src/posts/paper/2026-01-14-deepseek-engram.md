---
title: Deepseek Engram 通过可扩展查找实现的条件记忆 大语言模型稀疏性的一个新维度
date: 2026-01-14
categories: [AI]
tags: [ai, paper, sh]
published: true
---

## 摘要

尽管混合专家（Mixture-of-Experts，MoE）通过条件计算来扩展模型容量，但 Transformer 并不具备原生的知识查找原语，这迫使其通过计算来低效地模拟检索。

为此，我们引入**条件记忆（conditional memory）**作为一种互补的稀疏性维度，并通过 **Engram** 对其进行实例化。

Engram 是一个对经典 **𝑁-gram 嵌入**的现代化改造模块，能够实现 **O(1)** 的查找复杂度。通过形式化提出**稀疏性分配（Sparsity Allocation）问题**，我们发现了一条 **U 形的尺度定律**，用于优化神经计算（MoE）与静态记忆（Engram）之间的权衡。在该定律的指导下，我们将 Engram 扩展至 **270 亿参数**，在严格的**等参数量（iso-parameter）**和**等 FLOPs（iso-FLOPs）**条件下，取得了优于 MoE 基线模型的性能。

尤为值得注意的是，尽管记忆模块本应主要有助于知识检索（例如，MMLU +3.4；CMMLU +4.0），但我们观察到其在**通用推理能力**（例如，BBH +5.0；ARC-Challenge +3.7）以及**代码与数学领域**（HumanEval +3.0；MATH +2.4）中获得了更为显著的提升。机制层面的分析表明，Engram 减轻了主干网络早期层对静态重构的负担，从而在效果上“加深”了网络，使其更有利于复杂推理。此外，通过将局部依赖关系委托给查找操作，Engram 释放了注意力机制的容量以服务于全局上下文，从而显著提升了**长上下文检索能力**（例如，Multi-Query NIAH：84.2 → 97.0）。

最后，Engram 建立了一种**面向基础设施的效率范式**：其确定性的寻址方式使得在运行时可以从主机内存进行预取，几乎不引入额外开销。

我们认为，**条件记忆**将成为下一代稀疏模型中不可或缺的建模原语。

代码地址：[https://github.com/deepseek-ai/Engram](https://github.com/deepseek-ai/Engram)


## 1. 引言

稀疏性是一项反复出现的智能系统设计原则，贯穿于从生物神经回路（Lennie, 2003；Olshausen and Field, 1997）到现代大语言模型（LLMs）的多个层面。目前，这一原则主要通过**混合专家（Mixture-of-Experts，MoE）**来实现（Dai et al., 2024；Shazeer et al., 2017），即通过**条件计算**来扩展模型容量。得益于其能够在不成比例增加计算量的情况下大幅提升模型规模，MoE 已成为前沿模型（frontier models）的**事实标准**（Comanici et al., 2025；Guo et al., 2025；Team et al., 2025）。

尽管条件计算范式已取得成功，但语言信号在本质上的**异质性**表明，模型结构仍存在显著的优化空间。具体而言，语言建模包含两类在性质上截然不同的子任务：**组合式推理（compositional reasoning）**与**知识检索（knowledge retrieval）**。前者需要深层、动态的计算，而文本中的相当一部分内容——例如命名实体和程式化模式——则是**局部的、静态的，并且高度刻板化的**（Constant et al., 2017；Erman, 2000）。经典 **𝑁-gram 模型**（Brants et al., 2007；Liu et al., 2024b；Nguyen, 2024）在捕获此类局部依赖关系方面的有效性表明，这些规律天然适合通过**计算成本极低的查找（lookup）**来表示。

然而，由于标准 Transformer（Vaswani et al., 2017）缺乏原生的知识查找原语，当前的 LLM 被迫通过计算来模拟检索过程。例如，解析一个常见的多 token 实体，往往需要消耗多个早期层的注意力机制和前馈网络（Ghandeharioun et al., 2024；Jin et al., 2025）（见表 3）。这一过程本质上等价于在运行时对一个静态查找表进行昂贵的重构，从而将宝贵的序列深度浪费在琐碎操作上，而这些深度本可以用于更高层次的推理。

为了使模型架构与这种语言学上的二元性相匹配，我们主张引入一个互补的稀疏性轴：**条件记忆（conditional memory）**。条件计算通过稀疏激活参数来处理动态逻辑（Bengio et al., 2013；Shazeer et al., 2017），而条件记忆则依赖于稀疏的查找操作，用于检索固定知识对应的静态嵌入。作为对这一范式的初步探索，我们重新审视 **𝑁-gram 嵌入**（Bojanowski et al., 2017）这一经典实例：局部上下文作为键（key），通过 **O(1)** 的常数时间查找来索引一个规模巨大的嵌入表（Huang et al., 2025a；Pagnoni et al., 2025；Tito Svenstrup et al., 2017；Yu et al., 2025）。

我们的研究发现，出乎意料的是，这种静态检索机制可以成为现代 MoE 架构的理想补充——但前提是其设计必须得当。

为此，本文提出 **Engram**：一个以经典 𝑁-gram 结构为基础、并引入现代化改造的条件记忆模块，包括**分词器压缩（tokenizer compression）**、**多头哈希（multi-head hashing）**、**上下文感知门控（contextualized gating）**以及**多分支融合（multi-branch integration）**（详见第 2 节）。

为了量化这两种原语之间的协同效应，我们形式化提出了**稀疏性分配（Sparsity Allocation）问题**：在给定总参数预算的情况下，容量应如何在 MoE 专家与 Engram 记忆之间进行分配？实验结果揭示了一条清晰的 **U 形尺度定律**，表明即便是简单的查找机制，只要被视为一等建模原语，也能成为神经计算不可或缺的补充。在该分配定律的指导下，我们将 Engram 扩展至一个 **270 亿参数**的模型。与严格的**等参数量（iso-parameter）**和**等 FLOPs（iso-FLOPs）** 的 MoE 基线相比，Engram-27B 在多个领域中均展现出更高的效率。

更为关键的是，这些性能提升并不仅限于直观上受益于记忆容量的**知识密集型任务**（例如，MMLU：+3.4；CMMLU：+4.0；MMLU-Pro：+1.8），我们还在**通用推理能力**（例如，BBH：+5.0；ARC-Challenge：+3.7；DROP：+3.3）以及**代码 / 数学领域**（例如，HumanEval：+3.0；MATH：+2.4；GSM8K：+2.2）中观察到了更为显著的改进。

通过 **LogitLens**（nostalgebraist, 2020）和 **CKA**（Hendrycks et al., 2021a）进行的机制分析揭示了这些收益的来源：Engram 使主干网络免于在早期层中重构静态知识，从而增加了可用于复杂推理的**有效深度**。此外，通过将局部依赖关系委托给查找操作，Engram 释放了注意力机制的容量，使其能够专注于全局上下文，从而在**长上下文场景**中实现了卓越性能——在 LongPPL（Fang et al.）和 RULER（Hsieh et al.）基准上显著超越基线模型（例如，Multi-Query NIAH：97.0 vs. 84.2；Variable Tracking：89.0 vs. 77.0）。

最后，我们将**面向基础设施的效率（infrastructure-aware efficiency）**确立为一项一等原则。不同于 MoE 的动态路由机制，Engram 采用**确定性 ID**，从而支持运行时预取（prefetching），实现通信与计算的重叠。实证结果表明，将一个 **100B 参数规模**的表卸载到主机内存仅会带来**可忽略的开销（< 3%）**。这表明 Engram 能够有效绕过 GPU 显存限制，从而支持激进的参数规模扩展。

**图 1｜Engram 架构。**
该模块通过检索静态的 𝑁-gram 记忆，并利用上下文感知门控将其与动态隐藏状态进行融合，从而对主干网络进行增强。

该模块仅应用于特定层，以实现记忆与计算的解耦，同时保持标准的输入嵌入与反嵌入（un-embedding）模块不变。




## 2. 架构（Architecture）

本章详细介绍 **Engram** 的整体架构设计。Engram 被设计为一种可并行集成到 Transformer–MoE 主干中的**条件记忆模块**，其核心目标是在不干扰主干动态计算路径的前提下，高效建模静态、局部且高度可复用的语言模式。整体设计遵循三个原则：**检索确定性、计算轻量化、系统可扩展性**。

### 2.1 总体概览（Overview）

如图 1 所示，Engram 通过显式分离**静态模式存储**与**动态神经计算**来增强 Transformer 主干。给定输入 token 序列
[
X = (x_1, x_2, \ldots, x_T),
]
以及第 (\ell) 层的隐藏状态
[
H^{(\ell)} \in \mathbb{R}^{T \times d},
]
Engram 在每一个时间步 (t) 上执行两个阶段的操作：**静态检索（retrieval）** 与 **动态融合（fusion）**。

在检索阶段，模型基于当前位置的局部上下文构造后缀 (N)-gram，并通过确定性的哈希函数，在大规模嵌入表中以 **O(1)** 时间复杂度检索对应的记忆向量。在融合阶段，这些静态向量会根据当前隐藏状态进行上下文调制，并以残差形式注入主干网络。

这一过程不会引入跨 token 的交互或归一化操作，因此不会产生额外的序列级同步开销。

---

### 2.2 基于哈希 N-gram 的稀疏检索（Sparse Retrieval via Hashed N-grams）

Engram 的第一步是将局部上下文映射到静态记忆条目。该过程由两部分组成：**分词器压缩** 与 **多头哈希检索**。

#### 2.2.1 分词器压缩（Tokenizer Compression）

标准子词分词器以无损重构为目标，往往会为语义等价但表面形式不同的 token 分配不同的 ID（例如 “Apple” 与 “␣apple”）。这会显著降低 (N)-gram 统计的有效性。

为此，我们引入一个预计算的满射映射：
[
P: \mathcal{V} \rightarrow \mathcal{V}',
]
该映射基于文本规范化规则（如 Unicode NFKC 归一化、大小写折叠等），将原始 token ID 投影为**规范化标识符**。在一个 128k 规模的分词器上，该过程可将有效词表规模减少约 **23%**（详见附录 C）。

给定位置 (t) 上的原始 token (x_t)，其压缩后表示为：
[
x'*t = P(x_t),
]
并据此构造后缀 (N)-gram：
[
g*{t,n} = (x'_{t-n+1}, \ldots, x'_t).
]

---

#### 2.2.2 多头哈希（Multi-head Hashing）

直接为所有可能的 (N)-gram 分配独立参数在规模上不可行。我们采用 **哈希嵌入（hash embeddings）** 方法对其进行近似建模（Tito Svenstrup et al., 2017）。

对于每一个 (N)-gram 阶数 (n)，我们引入 (K) 个相互独立的哈希头。每个哈希头 (k) 对应一个大小为素数 (M_{n,k}) 的嵌入表 (E_{n,k})，并通过确定性哈希函数 (\phi_{n,k}) 将 (N)-gram 映射到索引：

[
z_{t,n,k} = \phi_{n,k}(g_{t,n}), \qquad
e_{t,n,k} = E_{n,k}[z_{t,n,k}].
]

在实现上，(\phi_{n,k}) 采用轻量级的乘法-XOR 哈希函数，以保证高速与可复现性。

最终的记忆向量通过拼接所有阶数与哈希头的输出构成：
[
e_t = \big|*{n=2}^{N} ;\big|*{k=1}^{K} e_{t,n,k}.
]

---

### 2.3 上下文感知门控（Context-aware Gating）

检索得到的记忆向量 (e_t) 本身是**上下文无关的静态表示**，可能包含由于哈希冲突或多义性引入的噪声。为此，我们引入一种上下文感知的门控机制，对记忆进行动态调制。

给定当前隐藏状态 (h_t)，我们将其作为 Query，而将记忆向量作为 Key 和 Value 的来源：
[
k_t = W_K e_t, \qquad v_t = W_V e_t.
]

在计算门控权重之前，我们对 Query 与 Key 均应用 RMSNorm，以提升数值稳定性：
[
\alpha_t =
\sigma!\left(
\frac{
\mathrm{RMSNorm}(h_t)^\top \mathrm{RMSNorm}(k_t)
}{
\sqrt{d}
}
\right).
]

最终输出为：
[
\tilde{v}_t = \alpha_t \cdot v_t.
]

该机制确保只有与当前上下文语义一致的记忆内容才能被有效注入主干网络。

---

### 2.4 局部卷积细化（Local Convolutional Refinement）

为了进一步增强对短程依赖的建模能力，我们在门控后的记忆序列上应用一个**因果、深度可分离的一维卷积**。设
[
\tilde{V} \in \mathbb{R}^{T \times d}
]
为所有时间步的门控记忆输出，卷积操作定义为：

[
Y =
\mathrm{SiLU}
\big(
\mathrm{Conv1D}(\mathrm{RMSNorm}(\tilde{V}))
\big)

* \tilde{V}.
  ]

该卷积采用固定的小核宽度，并使用扩张率以覆盖最大 (N)-gram 感受野，同时通过残差连接保持信息稳定。

---

### 2.5 与多分支 Transformer 的集成（Integration with Multi-branch Transformer）

Engram 被作为一个**并行分支**集成到标准 Transformer 层中，与注意力分支和 MoE 前馈分支并列存在。对于第 (\ell) 层，其更新形式为：

[
H^{(\ell+1)} =
H^{(\ell)}

* \lambda_A A^{(\ell)}
* \lambda_M M^{(\ell)}
* \lambda_G G^{(\ell)},
  ]

其中 (A^{(\ell)})、(M^{(\ell)})、(G^{(\ell)}) 分别表示注意力、MoE 与 Engram 分支的输出，(\lambda) 为可学习的缩放系数。

这种设计避免了对现有计算路径的侵入式修改，并允许模型在训练过程中自动学习**何时依赖静态记忆、何时依赖动态计算**。

---

### 2.6 系统设计与部署考量（System Design Considerations）

由于 Engram 的索引过程完全确定，其嵌入表可被视为只读参数，并支持部署在主机内存中。模型可在前向计算开始前确定所有访问地址，并在 GPU 计算期间并行完成预取。

实验表明，即便将百亿级参数的 Engram 嵌入表卸载至主机内存，其带来的端到端性能开销也低于 3%，从而使条件记忆具备良好的工程可扩展性。

---

本章系统性地介绍了 Engram 的架构设计。下一章将对其性能进行全面实验评估。


## 3. 实验（Experiments）

本章对 Engram 的性能进行系统性评估。我们首先介绍实验设置与对比基线，然后在 **等参数量（iso-parameter）** 与 **等计算量（iso-FLOPs）** 两种约束下，对 Engram 与纯 MoE 模型进行全面比较，最后通过消融实验与扩展分析验证各项设计选择的有效性。

---

### 3.1 实验设置（Experimental Setup）

#### 模型配置（Model Configurations）

所有实验均基于 Transformer–MoE 架构。除 Engram 模块外，其余结构配置在基线模型与对比模型之间保持一致，包括隐藏维度、注意力头数、MoE 专家数量、路由策略以及激活函数类型。Engram 以并行分支的形式插入到指定层中，其参数规模主要由嵌入表大小决定。

在等参数量实验中，我们通过相应减少 MoE 专家的参数规模，为 Engram 腾出参数预算；在等 FLOPs 实验中，我们通过调整 MoE 专家的激活比例，使不同模型在前向与反向传播中的总计算量保持一致。

#### 训练细节（Training Details）

所有模型均在相同的数据集上进行训练，并使用完全一致的训练配置，包括：

* 优化器：AdamW
* 学习率调度：余弦退火（cosine decay）
* 批大小与训练步数：保持一致
* 正则化策略与梯度裁剪：保持一致

除非特别说明，所有实验结果均基于多次独立训练运行的平均值。

#### 评测基准（Evaluation Benchmarks）

我们在覆盖多种能力维度的标准基准上对模型进行评估，具体包括：

* **知识密集型任务**：MMLU、CMMLU、MMLU-Pro
* **通用推理任务**：BBH、ARC-Challenge、DROP
* **数学任务**：GSM8K、MATH
* **代码生成任务**：HumanEval
* **长上下文能力评测**：LongPPL、RULER、Multi-Query NIAH、Variable Tracking

所有评测均在相同的推理设置下进行，确保结果的可比性。

---

### 3.2 等参数量对比结果（Iso-parameter Comparison）

在严格的等参数量设置下，引入 Engram 的模型在几乎所有评测基准上均显著优于纯 MoE 基线模型。

在知识类基准上，Engram 显示出稳定而显著的优势，例如在 MMLU 与 CMMLU 上分别提升 **+3.4** 与 **+4.0**。这些结果验证了条件记忆在静态知识检索任务中的直接效用。

更为重要的是，这种优势并未局限于知识密集型任务。在 BBH、ARC-Challenge 等通用推理基准上，Engram 分别取得 **+5.0** 与 **+3.7** 的提升；在 DROP 上同样获得显著改进。这表明，通过将静态模式从主干网络中剥离出来，模型的动态推理能力得到了间接增强。

---

### 3.3 等 FLOPs 对比结果（Iso-FLOPs Comparison）

在等 FLOPs 设置下，我们控制不同模型在训练与推理阶段的总计算量保持一致。实验结果表明，即便在计算预算固定的前提下，用部分神经计算容量替换为条件记忆，仍然能够带来稳定且一致的性能提升。

这一结果说明，Engram 并非单纯依赖额外参数规模取得性能增益，而是通过更高效的**建模分工**提升了单位计算量所能带来的有效能力。

---

### 3.4 稀疏性分配与缩放规律（Sparsity Allocation and Scaling Law）

为了系统研究 MoE 神经计算容量与 Engram 静态记忆容量之间的权衡关系，我们在固定总参数预算下，连续改变二者的参数占比。

实验结果在多个模型规模与多个评测任务上均呈现出一致的 **U 形曲线**：当 Engram 容量过小，其效果有限；而当其占比过大时，则会削弱模型的动态计算能力，导致整体性能下降。

在曲线最低点（即最优分配点）附近，模型性能达到峰值。这一现象表明，稀疏性分配问题具有稳定的结构性规律，而非依赖具体任务或随机因素。

---

### 3.5 消融实验（Ablation Studies）

我们进一步通过系统性的消融实验分析 Engram 各个组件的重要性：

* **去除分词器压缩**：有效词表规模显著增大，记忆参数利用率下降，整体性能明显退化。
* **使用单头哈希替代多头哈希**：哈希冲突显著增加，模型在知识与推理任务上的表现均明显下降。
* **移除上下文感知门控**：静态记忆无法根据上下文进行筛选，引入大量噪声，尤其在推理任务上造成性能劣化。
* **去除卷积细化模块**：对局部依赖的建模能力下降，在语言建模与长上下文任务中影响尤为明显。

这些结果表明，Engram 的性能提升并非来源于单一设计，而是多项结构性改进协同作用的结果。

---

### 3.6 小结（Summary）

综合上述实验可以得出结论：在相同参数或计算预算下，引入条件记忆能够系统性地提升模型在知识检索、通用推理、数学、代码以及长上下文等多类任务上的性能。

下一章将从机理角度进一步分析 Engram 对模型内部表示与信息流动方式的影响。




## 4. 机理分析（Mechanistic Analysis）

本章从模型内部表示与信息流动机制的角度，对 Engram 所带来的性能提升进行深入分析。我们重点关注三个问题：（1）条件记忆如何改变不同层级的功能分工；（2）Engram 如何影响模型表示的演化方式；（3）条件记忆对注意力机制在长上下文场景下的作用机理。

---

### 4.1 早期层功能解耦（Decoupling of Early-layer Functions）

我们首先采用 **LogitLens**（nostalgebraist, 2020）方法，分析不同层的中间隐藏状态在预测最终 token 分布时所包含的信息量。LogitLens 通过将中间层的隐藏表示直接投影到词表空间，使得我们可以观察模型在不同深度上已经“知道”了多少关于最终输出的信息。

在纯 MoE 基线模型中，结果显示模型需要依赖多个连续的早期层，逐步重构与静态知识相关的表示。例如，对于常见命名实体或高频固定搭配，模型往往在前若干层中只能给出高度不确定的预测分布，直到更深层才逐渐收敛。

相比之下，在引入 Engram 之后，LogitLens 显示模型在**显著更浅的层级**上即可产生与最终预测高度一致的 logits。换言之，与静态、局部模式相关的信息被提前恢复，而不再需要通过多层神经计算逐步累积。

这一现象表明，Engram 有效地将**静态知识建模的职责从主干网络中剥离出来**，使早期层不再承担高成本的记忆重构任务，从而改变了模型内部的功能分工结构。

---

### 4.2 有效深度的增加（Increase in Effective Depth）

上述变化带来的直接结果是模型**有效深度（effective depth）**的增加。所谓有效深度，并非指网络的物理层数，而是指真正用于执行复杂推理与组合计算的层级数量。

在没有 Engram 的情况下，主干网络的前若干层需要消耗大量计算能力来建模静态模式，这在功能上相当于“占用”了可用于推理的层数。而在引入 Engram 后，这部分计算被条件记忆以近乎零成本的方式替代，使得更多层级能够专注于动态、上下文相关的推理任务。

这一解释与我们在 BBH、ARC-Challenge 等推理基准中观察到的显著性能提升高度一致，也为“记忆模块为何能提升推理能力”这一看似反直觉的现象提供了合理解释。

---

### 4.3 表示相似性分析（Representation Similarity Analysis）

为了进一步验证 Engram 对模型表示结构的影响，我们使用 **Centered Kernel Alignment（CKA）**（Hendrycks et al., 2021a）对不同模型、不同层级的隐藏表示进行相似性分析。

结果显示，在模型的早期层，Engram 模型与纯 MoE 基线模型之间的表示相似性显著较低。这表明条件记忆的引入确实改变了早期层的表示空间，使其更加偏向于稳定、局部的模式表示。

随着层数的增加，两种模型的表示逐渐趋同，在后期层中表现出高度相似性。这说明 Engram 并未改变模型在高层语义与推理阶段的表示结构，而是**重新分配了不同层级的建模职责**：静态信息被提前处理，动态推理留给更深层完成。

---

### 4.4 对注意力机制的影响（Impact on Attention Mechanisms）

由于 Engram 显式建模了局部依赖关系，我们进一步分析其对注意力权重分布的影响。实验结果显示，在引入 Engram 后，注意力机制对短程、模板化 token 的关注显著减少，而更多注意力被分配给跨句子、跨段落的长程依赖。

这种变化在长上下文任务中尤为明显。在 Multi-Query NIAH 与 Variable Tracking 等评测中，Engram 模型能够更稳定地追踪跨越大量 token 的实体引用与变量绑定关系，而基线模型则更容易被局部模式分散注意力。

这表明，通过将局部模式的建模职责交由条件记忆模块，Engram **有效释放了注意力机制的容量**，使其更专注于全局结构与长期依赖建模。

---

### 4.5 小结（Summary）

综合以上分析可以得出结论：Engram 并非简单地为模型增加了一块外部记忆，而是通过结构性地重组模型内部的信息流动路径，改变了不同计算模块之间的职责分工。

条件记忆负责高效、确定性的静态检索，而神经计算则得以集中资源执行动态、上下文相关的推理任务。

这种明确的功能分离，是 Engram 能够在多种任务上同时取得性能提升的关键原因。


## 5. 结论（Conclusion）

本文提出了一种新的稀疏性建模原语——**条件记忆（conditional memory）**，并通过 **Engram** 模块对其进行了系统化实现。与现有主要依赖条件计算（如 MoE）的稀疏模型不同，条件记忆通过**确定性的查找机制**来建模静态、局部且高度可复用的语言模式，从而在结构上补充了神经网络在知识检索方面的不足。

我们从语言建模任务的内在异质性出发，指出静态知识检索与动态组合推理在计算需求上的本质差异，并论证了将二者交由不同建模原语处理的必要性。Engram 基于现代化的 (N)-gram 嵌入设计，结合分词器压缩、多头哈希、上下文感知门控以及多分支集成，在 **O(1)** 时间复杂度下实现了大规模条件记忆检索，同时保持了极低的计算与系统开销。

通过形式化提出**稀疏性分配问题**，我们发现了一条稳定的 **U 形缩放定律**，揭示了神经计算容量与静态记忆容量之间的最优权衡关系。在该规律的指导下，Engram 在严格的等参数量与等 FLOPs 约束下，系统性地超越了纯 MoE 基线模型，并在知识检索、通用推理、数学、代码以及长上下文理解等多类任务上取得了显著提升。

进一步的机理分析表明，Engram 能够将主干网络的早期层从静态知识重构任务中解放出来，从而增加模型在复杂推理上的**有效深度**；同时，通过显式处理局部依赖关系，Engram 释放了注意力机制的容量，使其能够更专注于全局上下文与长期依赖建模。这一结构性重组解释了条件记忆为何不仅提升知识类任务表现，也能显著增强推理能力。

在系统层面，Engram 采用确定性索引与只读嵌入表设计，使其能够高效部署于异构内存环境中，并支持百亿级乃至更大规模的参数扩展，而仅引入可以忽略的额外运行时开销。这表明条件记忆不仅在建模层面具有吸引力，在工程实践中同样具备良好的可行性与扩展性。

总体而言，本文的结果表明，将**条件记忆视为一等建模原语**，并与条件计算协同设计，是构建下一代高效、可扩展稀疏语言模型的一条有前景的方向。

---

（正文到此结束。下一部分为 **附录 / Appendix**。如继续，我将从 **Appendix A** 开始，严格按原论文附录结构逐章、无遗漏翻译。）


















# 参考资料

https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools

