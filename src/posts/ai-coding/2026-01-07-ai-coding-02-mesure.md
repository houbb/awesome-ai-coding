---
title: AI 编码中的指标度量
date: 2026-01-07
categories: [AI-CODING] 
tags: [ai-coding, sh]
published: true
---

# AI 的指标度量

```
在引入 ai 编程的时候，如何对各项数据进行指标度量？

如何统计呢？比如代码库与AI服务端代码行对比之类的
```


下面会用**工程度量 + 数据治理**的视角，系统回答两件事：

1. **“度量什么”——指标体系怎么设计才不自欺欺人**
2. **“怎么统计”——代码库 / AI 服务端 / IDE / CI 如何打通**

# 一、先讲结论：代码行对比 ≠ 有效度量（但可以作为辅助）

> **单纯比较「AI 生成代码行数 / 仓库总代码行数」是一个低价值、甚至误导性的指标。**

原因：

* LOC 本身不是生产力指标
* AI 往往生成：

  * 大段样板代码（boilerplate）
  * 被人类重写 / 删除的代码
* 真正有价值的是：

  * **“哪些代码是 AI 主导产生的”**
  * **“最终进入主干的比例”**
  * **“减少了哪些人工工作量”**

因此，正确做法是：

> **LOC 只作为“底层原始数据”，必须经过语义与流程加工，才能成为管理指标。**

---

# 二、指标总体设计：四层度量模型（推荐）

```
L4  业务 / 价值层（ROI、交付速度）
L3  工程质量层（缺陷、返工、稳定性）
L2  工程过程层（AI 介入点、接受率）
L1  行为 / 原始数据层（代码、Prompt、Diff）
```

你问的 **“代码行统计”属于 L1**，而管理决策主要看 L3 / L4。

---

# 三、L1：原始数据层 ——“怎么采集”

## 1. 代码与 AI 的“来源标记”机制（非常重要）

### 原则

> **不要猜哪些代码是 AI 写的，要“显式标记”**

### 三种可行方式（可组合）

#### 方式 A：IDE 插件标记（最推荐）

在 IDE（Cursor / JetBrains）中：

* 每次 AI 生成代码：

  * 生成一个 **AI-Snippet-ID**
  * 记录：

    * prompt hash
    * 模型
    * 时间
    * 文件路径
* 插入为：

  * 直接生成
  * 或 comment / metadata（不可进生产）

👉 不要求把标记写入源码，可在本地插件日志中。

---

#### 方式 B：Git Diff 归因

在提交时：

* 自动分析 diff：

  * 是否包含 AI Snippet ID
  * 是否由 AI CLI / Agent 触发
* 将 commit 打上 tag：

  * `ai-assisted`
  * `ai-generated`

---

#### 方式 C：AI 服务端回传（你提到的）

AI 服务端记录：

* 每次响应的：

  * token 数
  * 生成代码字符数
  * 文件类型
  * task 类型

⚠️ 注意：

> **AI 服务端不知道哪些代码“最终被采纳”**

---

## 2. 原始指标示例（不要直接给老板看）

| 指标                | 含义             |
| ----------------- | -------------- |
| ai_generated_loc  | AI 生成的代码行数     |
| ai_inserted_loc   | 被插入代码库的行数      |
| ai_survived_loc   | 合并后 30 天仍存在的行数 |
| ai_modified_ratio | 被人工修改比例        |

---

# 四、L2：工程过程层 ——“哪些 AI 帮助真的生效了”

这是**最关键的一层**。

## 1. AI 接受率（Acceptance Rate）

**定义**

```
AI 接受率 = 最终进入主干的 AI 代码行 / AI 生成代码行
```

细化：

* 直接采纳
* 采纳后修改
* 被完全丢弃

👉 比 LOC 本身重要 10 倍。

---

## 2. AI 介入点分布

统计 AI 主要用在哪：

| 环节     | 占比 |
| ------ | -- |
| 新功能编码  |    |
| 重构     |    |
| 测试生成   |    |
| 文档     |    |
| Review |    |

意义：

* 判断 AI 是否只在“低价值环节打转”

---

## 3. 人工干预强度（Human Edit Ratio）

```
人工修改行数 / AI 初始生成行数
```

* 高：Prompt 不好 / 架构不清
* 低：AI 生成质量高

---

# 五、L3：工程质量层 ——“AI 有没有把质量搞坏”

这是管理层最关心、也最容易被忽略的。

## 1. AI 相关缺陷率

* 标记：

  * 缺陷是否源于 AI 生成代码
* 指标：

  * AI 代码缺陷密度
  * 对比人工代码

> 目标不是“AI 零缺陷”，而是 **不显著劣于人类**

---

## 2. AI 代码返工率

* AI 生成 → 合并 → 30 天内被重构 / 删除
* 是非常真实的质量信号

---

# 六、L4：价值层 ——“是否真的提效”

## 1. 时间节省模型（推荐）

不要问：

> “AI 写了多少代码？”

要问：

> **“如果没有 AI，这些代码需要多少人时？”**

方法：

* 建立基线：

  * 同类需求历史平均耗时
* 对比：

  * AI 介入后的实际耗时

---

## 2. ROI 公式（简化）

```
AI ROI =
（节省人力成本 + 提前交付价值）
—
（模型成本 + 工具成本 + 治理成本）
```

---

# 七、你关心的重点：代码库 vs AI 服务端行数，如何“正确”对比

### 错误做法

```
仓库总代码行数 vs AI 生成行数
```

### 正确做法（多层对比）

| 指标               | 含义       |
| ---------------- | -------- |
| AI Generated LOC | 服务端生成    |
| AI Inserted LOC  | 被提交      |
| AI Merged LOC    | 进入主干     |
| AI Survived LOC  | 30 天后仍存在 |

**真正有意义的是：**

```
AI Survived LOC / AI Generated LOC
```

以及：

```
AI Survived LOC / 总变更 LOC
```

---

# 八、实现架构（简化示意）

```
IDE Plugin
  ↓
AI Usage Event
  ↓
Event Collector
  ↓
Git / CI Analyzer
  ↓
Metrics Warehouse
  ↓
Dashboard
```

---

# 九、非常重要的提醒（经验之谈）

1. **不要一开始就“精确到行”**

   * 趋势 > 精度
2. **指标一开始不要太多**

   * 3–5 个核心指标即可
3. **指标是用来改进系统，不是考核人**

   * 否则数据一定被“刷”



# 参考资料

