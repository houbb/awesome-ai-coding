import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as e,a as s,o as a}from"./app-aSB-Rx6k.js";const n={};function l(r,t){return a(),e("div",null,[...t[0]||(t[0]=[s(`<h1 id="背景" tabindex="-1"><a class="header-anchor" href="#背景"><span>背景</span></a></h1><p>学习 <a href="https://github.com/Lordog/dive-into-llms" target="_blank" rel="noopener noreferrer">dive-into-llms</a></p><h1 id="chat" tabindex="-1"><a class="header-anchor" href="#chat"><span>chat</span></a></h1><h2 id="深度学习基础知识入门" tabindex="-1"><a class="header-anchor" href="#深度学习基础知识入门"><span>深度学习基础知识入门</span></a></h2><p>这一步正是「从会用大模型 → 理解大模型」的关键转折。</p><p>你想要快速建立「深度学习基础认知」——不是搞学术，而是能<strong>听懂大模型背后的逻辑</strong>。</p><p>那我们就用最通俗易懂的方式讲清楚。</p><h2 id="🧠-一、深度学习到底是什么" tabindex="-1"><a class="header-anchor" href="#🧠-一、深度学习到底是什么"><span>🧠 一、深度学习到底是什么？</span></a></h2><p>一句话讲清楚：</p><blockquote><p>深度学习就是「让计算机通过大量数据自动学出规律」的过程。</p></blockquote><p>比如：</p><ul><li>传统写规则：<code>if &quot;error&quot; in log: return &quot;异常&quot;</code></li><li>深度学习：给模型几百万条日志，它自己学出什么样的句子意味着“异常”。</li></ul><p>它的核心就是一个「神经网络（Neural Network）」——一种数学函数拟合器。<br> 它能“看例子”，自动调节参数，让输出更接近目标。</p><hr><h2 id="🧩-二、神经网络是什么" tabindex="-1"><a class="header-anchor" href="#🧩-二、神经网络是什么"><span>🧩 二、神经网络是什么？</span></a></h2><p>我们来看一张简化图👇</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>输入层 → 隐藏层 → 输出层</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>举个例子：<br> 你要让模型判断一条报警日志是不是「磁盘问题」。</p><table><thead><tr><th>输入（特征）</th><th>输出（预测）</th></tr></thead><tbody><tr><td>“磁盘已满” → [1,0,0]</td><td>✅ 磁盘问题</td></tr><tr><td>“CPU负载高” → [0,1,0]</td><td>❌ CPU问题</td></tr><tr><td>“网络中断” → [0,0,1]</td><td>❌ 网络问题</td></tr></tbody></table><p>模型内部其实是很多个加权求和 + 激活函数：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>y = f(Wx + b)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li><strong>x</strong>：输入（比如句子向量）</li><li><strong>W</strong>：权重（模型学习的参数）</li><li><strong>b</strong>：偏置（修正项）</li><li><strong>f()</strong>：非线性函数（ReLU / Sigmoid）</li></ul><p>每个神经元都像是在做一个“加权投票”：</p><blockquote><p>各个特征加权求和 → 判断结果 → 传递到下一层。</p></blockquote><p>层层堆叠，形成“深度网络”。</p><hr><h2 id="🔁-三、反向传播-backpropagation" tabindex="-1"><a class="header-anchor" href="#🔁-三、反向传播-backpropagation"><span>🔁 三、反向传播（Backpropagation）</span></a></h2><p>反向传播是训练神经网络的核心机制。</p><p>想象你教小孩投篮：</p><ul><li>他第一次扔太低 → 你告诉他“往上扔点”</li><li>第二次太远 → “往回一点”</li><li>经过反复试错，他学会合适的力度。</li></ul><p>神经网络也是这样：</p><ol><li><strong>前向传播（Forward）</strong>：计算预测结果；</li><li><strong>计算损失（Loss）</strong>：预测和真实差多少；</li><li><strong>反向传播（Backward）</strong>：根据差距更新权重（往“更好”的方向修正）。</li></ol><p>公式上：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>w_new = w_old - learning_rate * d(loss)/dw</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>这就是所谓的“梯度下降法（Gradient Descent）”。</p><hr><h2 id="⚙️-四、优化器-optimizer" tabindex="-1"><a class="header-anchor" href="#⚙️-四、优化器-optimizer"><span>⚙️ 四、优化器（Optimizer）</span></a></h2><p>优化器负责“怎么更新权重”，也就是训练策略。</p><table><thead><tr><th>优化器</th><th>特点</th><th>场景</th></tr></thead><tbody><tr><td>SGD</td><td>最基础的梯度下降</td><td>小模型、理论教学</td></tr><tr><td>Momentum</td><td>加速度，避免震荡</td><td>中小模型</td></tr><tr><td>Adam</td><td>动态调节学习率</td><td>主流模型几乎都用</td></tr><tr><td>RMSprop</td><td>稳定收敛</td><td>RNN/LSTM 常用</td></tr></tbody></table><p>现在几乎所有 Transformer 模型都默认用 <strong>AdamW</strong>（Adam + 权重衰减）。</p><hr><h2 id="💔-五、损失函数-loss-function" tabindex="-1"><a class="header-anchor" href="#💔-五、损失函数-loss-function"><span>💔 五、损失函数（Loss Function）</span></a></h2><p>损失函数告诉模型“你离正确答案还有多远”。</p><table><thead><tr><th>类型</th><th>示例任务</th><th>常用损失函数</th></tr></thead><tbody><tr><td>分类</td><td>日志分类、情感分析</td><td>交叉熵（CrossEntropy）</td></tr><tr><td>回归</td><td>异常值预测</td><td>均方误差（MSE）</td></tr><tr><td>生成</td><td>语言模型预测下一个词</td><td>交叉熵</td></tr><tr><td>嵌入</td><td>相似度计算</td><td>对比损失（Contrastive Loss）</td></tr></tbody></table><p>例子：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">Loss </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> -</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">Σ</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> (真实概率 × </span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">log</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(预测概率))</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>模型训练的目标就是：</p><blockquote><p>不断调整参数，使 Loss 越来越小。</p></blockquote><hr><h2 id="🎢-六、过拟合与欠拟合" tabindex="-1"><a class="header-anchor" href="#🎢-六、过拟合与欠拟合"><span>🎢 六、过拟合与欠拟合</span></a></h2><table><thead><tr><th>情况</th><th>含义</th><th>特征</th><th>解决办法</th></tr></thead><tbody><tr><td>过拟合</td><td>模型记住了训练数据，没学到规律</td><td>训练集很好，测试集差</td><td>增加数据、正则化、Dropout</td></tr><tr><td>欠拟合</td><td>模型太简单，学不动</td><td>训练和测试都差</td><td>加层数、换更复杂模型</td></tr></tbody></table><p>直观比喻👇</p><ul><li><strong>过拟合</strong>：背题（考试换题就不会）</li><li><strong>欠拟合</strong>：连题都没看懂</li></ul><hr><h2 id="🧮-七、深度学习工作流程全景图" tabindex="-1"><a class="header-anchor" href="#🧮-七、深度学习工作流程全景图"><span>🧮 七、深度学习工作流程全景图</span></a></h2><p>1️⃣ 准备数据</p><ul><li>收集、清洗、标注数据</li><li>转成张量格式（Tensor）</li></ul><p>2️⃣ 定义模型</p><ul><li>用 PyTorch / TensorFlow 定义结构</li><li>比如输入层、隐藏层、输出层</li></ul><p>3️⃣ 定义损失函数 &amp; 优化器</p><ul><li><code>loss_fn = nn.CrossEntropyLoss()</code></li><li><code>optimizer = torch.optim.Adam(model.parameters())</code></li></ul><p>4️⃣ 训练循环（epoch）</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">for</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> batch </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">in</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> data:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    pred </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> model</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(batch.x)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    loss </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> loss_fn</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(pred, batch.y)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    optimizer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">zero_grad</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">()</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    loss.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">backward</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">()</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    optimizer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">step</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">()</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>5️⃣ 评估与保存模型</p><ul><li>计算准确率、召回率</li><li>保存 <code>model.save_pretrained()</code></li></ul><hr><h2 id="📘-八、两本经典又易懂的-深度学习入门-资料" tabindex="-1"><a class="header-anchor" href="#📘-八、两本经典又易懂的-深度学习入门-资料"><span>📘 八、两本经典又易懂的“深度学习入门”资料</span></a></h2><h3 id="✅-1-《动手学深度学习-dive-into-deep-learning-》" tabindex="-1"><a class="header-anchor" href="#✅-1-《动手学深度学习-dive-into-deep-learning-》"><span>✅ 1. 《动手学深度学习（Dive into Deep Learning）》</span></a></h3><ul><li><p>作者：李沐、阿斯顿张等（亚马逊科学家）</p></li><li><p>免费在线阅读：<a href="https://zh.d2l.ai/" target="_blank" rel="noopener noreferrer">https://zh.d2l.ai/</a></p></li><li><p>特点：</p><ul><li>理论+代码结合，PyTorch/MXNet 双版本</li><li>从感知机讲到卷积、RNN、Transformer</li><li>中文解释非常通俗</li></ul></li><li><p>建议读：第 1～5 章（基础部分）+ 第 9 章（Transformer）</p></li></ul><hr><h3 id="✅-2-《深度学习入门-基于python的理论与实现》" tabindex="-1"><a class="header-anchor" href="#✅-2-《深度学习入门-基于python的理论与实现》"><span>✅ 2. 《深度学习入门：基于Python的理论与实现》</span></a></h3><ul><li><p>作者：斋藤康毅</p></li><li><p>特点：</p><ul><li>从零手写神经网络</li><li>不用任何框架，理解反向传播、梯度下降原理</li><li>代码短小易读</li></ul></li><li><p>适合：想真正理解“为什么模型能学”的人</p></li><li><p>可配合 B 站视频《3小时入门神经网络》一起看。</p></li></ul><hr><h2 id="🚀-九、建议学习顺序-快速通关版" tabindex="-1"><a class="header-anchor" href="#🚀-九、建议学习顺序-快速通关版"><span>🚀 九、建议学习顺序（快速通关版）</span></a></h2><table><thead><tr><th>周</th><th>主题</th><th>学习重点</th><th>实践</th></tr></thead><tbody><tr><td>第 1 周</td><td>神经网络与反向传播</td><td>理解神经元怎么“学”</td><td>用 <code>numpy</code> 实现两层网络</td></tr><tr><td>第 2 周</td><td>损失函数与优化器</td><td>梯度下降、Adam 原理</td><td>可视化 loss 下降过程</td></tr><tr><td>第 3 周</td><td>过拟合、正则化</td><td>Dropout、Early stopping</td><td>训练一个简单分类模型</td></tr><tr><td>第 4 周</td><td>Transformer 概念</td><td>注意力机制、预训练思想</td><td>跑一个 BERT 中文分类任务</td></tr></tbody></table>`,75)])])}const p=i(n,[["render",l]]),o=JSON.parse('{"path":"/posts/learn-llms/2025-11-03-dive-into-llms-02-basic-deep-learning-intro.html","title":"dive-into-llms-02-deeplearning 深度学习基础知识入门","lang":"zh-CN","frontmatter":{"title":"dive-into-llms-02-deeplearning 深度学习基础知识入门","date":"2025-11-03T00:00:00.000Z","categories":["AI"],"tags":["ai","learn-note"],"published":true,"description":"背景 学习 dive-into-llms chat 深度学习基础知识入门 这一步正是「从会用大模型 → 理解大模型」的关键转折。 你想要快速建立「深度学习基础认知」——不是搞学术，而是能听懂大模型背后的逻辑。 那我们就用最通俗易懂的方式讲清楚。 🧠 一、深度学习到底是什么？ 一句话讲清楚： 深度学习就是「让计算机通过大量数据自动学出规律」的过程。 比...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"dive-into-llms-02-deeplearning 深度学习基础知识入门\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-11-03T00:00:00.000Z\\",\\"dateModified\\":\\"2025-12-27T05:15:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"老马啸西风\\",\\"url\\":\\"https://houbb.github.io\\"}]}"],["meta",{"property":"og:url","content":"https://houbb.github.io/awesome-ai-coding/posts/learn-llms/2025-11-03-dive-into-llms-02-basic-deep-learning-intro.html"}],["meta",{"property":"og:site_name","content":"老马啸西风"}],["meta",{"property":"og:title","content":"dive-into-llms-02-deeplearning 深度学习基础知识入门"}],["meta",{"property":"og:description","content":"背景 学习 dive-into-llms chat 深度学习基础知识入门 这一步正是「从会用大模型 → 理解大模型」的关键转折。 你想要快速建立「深度学习基础认知」——不是搞学术，而是能听懂大模型背后的逻辑。 那我们就用最通俗易懂的方式讲清楚。 🧠 一、深度学习到底是什么？ 一句话讲清楚： 深度学习就是「让计算机通过大量数据自动学出规律」的过程。 比..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-27T05:15:15.000Z"}],["meta",{"property":"article:tag","content":"learn-note"}],["meta",{"property":"article:tag","content":"ai"}],["meta",{"property":"article:published_time","content":"2025-11-03T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-27T05:15:15.000Z"}]]},"git":{"createdTime":1766812269000,"updatedTime":1766812515000,"contributors":[{"name":"bbhou","username":"bbhou","email":"1557740299@qq.com","commits":3,"url":"https://github.com/bbhou"}]},"readingTime":{"minutes":4.46,"words":1337},"filePathRelative":"posts/learn-llms/2025-11-03-dive-into-llms-02-basic-deep-learning-intro.md","excerpt":"\\n<p>学习 <a href=\\"https://github.com/Lordog/dive-into-llms\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">dive-into-llms</a></p>\\n<h1>chat</h1>\\n<h2>深度学习基础知识入门</h2>\\n<p>这一步正是「从会用大模型 → 理解大模型」的关键转折。</p>\\n<p>你想要快速建立「深度学习基础认知」——不是搞学术，而是能<strong>听懂大模型背后的逻辑</strong>。</p>\\n<p>那我们就用最通俗易懂的方式讲清楚。</p>\\n<h2>🧠 一、深度学习到底是什么？</h2>","autoDesc":true}');export{p as comp,o as data};
