import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as r,a as e,o as d}from"./app-D4koU7iK.js";const s={};function n(o,t){return d(),r("div",null,[...t[0]||(t[0]=[e('<h2 id="第6章-线性模型家族" tabindex="-1"><a class="header-anchor" href="#第6章-线性模型家族"><span>第6章　线性模型家族</span></a></h2><p>线性模型是机器学习中最早、最经典、也是最具代表性的算法家族。</p><p>几乎所有复杂模型（包括神经网络）在本质上都可以看作“非线性的线性组合”。</p><p>线性模型的魅力在于：</p><ul><li><strong>简单而强大</strong> —— 可以解释、可计算、可扩展；</li><li><strong>理论完备</strong> —— 有清晰的概率解释和几何意义；</li><li><strong>实用性极高</strong> —— 仍被广泛用于工业界的特征工程、基线模型和可解释建模。</li></ul><hr><h3 id="_6-1-线性回归" tabindex="-1"><a class="header-anchor" href="#_6-1-线性回归"><span><strong>6.1 线性回归</strong></span></a></h3><h4 id="✅-一、问题定义" tabindex="-1"><a class="header-anchor" href="#✅-一、问题定义"><span>✅ 一、问题定义</span></a></h4><p>线性回归（Linear Regression）用于解决<strong>连续值预测问题</strong>。<br> 其假设是：输出 (y) 与输入特征 (x) 之间呈线性关系：</p><p>[<br> \\hat{y} = w^T x + b<br> ]</p><p>其中：</p><ul><li>(x = (x_1, x_2, ..., x_n))：输入特征向量</li><li>(w = (w_1, w_2, ..., w_n))：权重参数</li><li>(b)：偏置项（bias）</li></ul><h4 id="✅-二、目标函数" tabindex="-1"><a class="header-anchor" href="#✅-二、目标函数"><span>✅ 二、目标函数</span></a></h4><p>为了让模型预测值 (\\hat{y}) 尽可能接近真实值 (y)，<br> 最常用的损失函数是 <strong>均方误差（MSE）</strong>：</p><p>[<br> L(w,b) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - (w^T x_i + b))^2<br> ]</p><h4 id="✅-三、解析解-normal-equation" tabindex="-1"><a class="header-anchor" href="#✅-三、解析解-normal-equation"><span>✅ 三、解析解（Normal Equation）</span></a></h4><p>当特征维度不太高时，可以直接通过矩阵求导获得最优参数：</p><p>[<br> w^* = (X^T X)^{-1} X^T y<br> ]</p><p>其中 (X) 是特征矩阵，(y) 是目标向量。<br> 这种闭式解（Closed Form）虽然简单，但在高维或大数据集上会导致计算瓶颈，因此实践中常用 <strong>梯度下降（GD / SGD）</strong>。</p><h4 id="✅-四、几何与统计解释" tabindex="-1"><a class="header-anchor" href="#✅-四、几何与统计解释"><span>✅ 四、几何与统计解释</span></a></h4><ul><li>几何视角：线性回归相当于在高维空间中找到一个<strong>最贴近样本点的超平面</strong>。</li><li>统计视角：线性回归假设误差项服从高斯分布，等价于最大似然估计（MLE）。</li></ul><h4 id="✅-五、优缺点" tabindex="-1"><a class="header-anchor" href="#✅-五、优缺点"><span>✅ 五、优缺点</span></a></h4><p>✅ 优点：</p><ul><li>可解释性强；</li><li>计算效率高；</li><li>在噪声小、线性关系明显时表现好。</li></ul><p>❌ 缺点：</p><ul><li>无法建模非线性关系；</li><li>对异常值敏感；</li><li>可能发生多重共线性（特征相关）。</li></ul><hr><h3 id="_6-2-逻辑回归与分类边界" tabindex="-1"><a class="header-anchor" href="#_6-2-逻辑回归与分类边界"><span><strong>6.2 逻辑回归与分类边界</strong></span></a></h3><h4 id="✅-一、从回归到分类的跨越" tabindex="-1"><a class="header-anchor" href="#✅-一、从回归到分类的跨越"><span>✅ 一、从回归到分类的跨越</span></a></h4><p>逻辑回归（Logistic Regression）用于<strong>二分类任务</strong>。<br> 虽然名字里有“回归”，但本质上是一个<strong>分类模型</strong>。</p><h4 id="✅-二、模型假设" tabindex="-1"><a class="header-anchor" href="#✅-二、模型假设"><span>✅ 二、模型假设</span></a></h4><p>我们希望预测样本属于正类（y=1）的概率：</p><p>[<br> P(y=1|x) = \\sigma(w^T x + b)<br> ]</p><p>其中 (\\sigma(z)) 是 Sigmoid 函数：</p><p>[<br> \\sigma(z) = \\frac{1}{1 + e^{-z}}<br> ]</p><p>它将线性输出 (w^T x + b) 压缩到 (0, 1) 区间。</p><h4 id="✅-三、决策边界" tabindex="-1"><a class="header-anchor" href="#✅-三、决策边界"><span>✅ 三、决策边界</span></a></h4><p>分类边界由方程 (w^T x + b = 0) 决定。<br> 几何上，这是一个将样本空间划分为两部分的超平面。</p><ul><li>(w)：决定超平面的方向；</li><li>(b)：决定平面的位置。</li></ul><h4 id="✅-四、损失函数-对数似然" tabindex="-1"><a class="header-anchor" href="#✅-四、损失函数-对数似然"><span>✅ 四、损失函数：对数似然</span></a></h4><p>逻辑回归通过极大化样本出现的<strong>对数似然</strong>来训练：</p><p>[<br> L(w) = \\sum_i [y_i \\log p_i + (1 - y_i) \\log(1 - p_i)]<br> ]</p><p>通常优化其负数（即交叉熵损失）：</p><p>[<br> \\text{Loss} = -\\frac{1}{N}\\sum_i [y_i \\log \\hat{y}_i + (1 - y_i)\\log(1 - \\hat{y}_i)]<br> ]</p><h4 id="✅-五、多分类扩展" tabindex="-1"><a class="header-anchor" href="#✅-五、多分类扩展"><span>✅ 五、多分类扩展</span></a></h4><ul><li><strong>一对多 (OvR)</strong>：训练多个二分类器，每次区分“某一类 vs 其他类”；</li><li><strong>Softmax 回归</strong>：广义逻辑回归，使用 Softmax 函数输出多类概率：</li></ul><p>[<br> P(y=k|x) = \\frac{e<sup>{w_k</sup>T x}}{\\sum_j e<sup>{w_j</sup>T x}}<br> ]</p><h4 id="✅-六、概率与几何统一视角" tabindex="-1"><a class="header-anchor" href="#✅-六、概率与几何统一视角"><span>✅ 六、概率与几何统一视角</span></a></h4><p>逻辑回归既是<strong>线性分类器</strong>（超平面决策边界），<br> 又是<strong>概率模型</strong>（输出类别的概率）。<br> 这是线性模型家族的重要特征。</p><hr><h3 id="_6-3-多项式回归与岭回归" tabindex="-1"><a class="header-anchor" href="#_6-3-多项式回归与岭回归"><span><strong>6.3 多项式回归与岭回归</strong></span></a></h3><h4 id="✅-一、多项式回归-polynomial-regression" tabindex="-1"><a class="header-anchor" href="#✅-一、多项式回归-polynomial-regression"><span>✅ 一、多项式回归（Polynomial Regression）</span></a></h4><p>线性模型的核心假设是输入特征与输出呈线性关系。<br> 但通过构造“非线性特征”，它也能处理非线性问题。</p><p>例如：<br> [<br> y = w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\ldots<br> ]</p><p>实际上它仍然是<strong>线性模型</strong>，只是对“特征”线性，而非对“输入”线性。</p><p>这种做法体现了机器学习的核心思想：</p><blockquote><p>“用非线性特征去弥补线性模型的不足。”</p></blockquote><h4 id="✅-二、岭回归-ridge-regression" tabindex="-1"><a class="header-anchor" href="#✅-二、岭回归-ridge-regression"><span>✅ 二、岭回归（Ridge Regression）</span></a></h4><p>当特征之间存在高度相关（多重共线性）时，普通最小二乘解不稳定。<br> 岭回归通过在损失函数中加入 L2 正则项进行约束：</p><p>[<br> L(w) = ||y - Xw||^2 + \\lambda ||w||^2<br> ]</p><p>解析解为：</p><p>[<br> w^* = (X^T X + \\lambda I)^{-1} X^T y<br> ]</p><ul><li>当 (\\lambda = 0)，退化为普通线性回归；</li><li>当 (\\lambda) 较大，抑制权重震荡，提高泛化能力。</li></ul><h4 id="✅-三、lasso-与-elastic-net" tabindex="-1"><a class="header-anchor" href="#✅-三、lasso-与-elastic-net"><span>✅ 三、Lasso 与 Elastic Net</span></a></h4><ul><li><strong>Lasso（L1正则化）</strong>：鼓励稀疏性，能自动实现特征选择；</li><li><strong>Elastic Net</strong>：结合 L1 + L2，兼顾平滑与稀疏。</li></ul><hr><h3 id="_6-4-判别式与生成式模型比较" tabindex="-1"><a class="header-anchor" href="#_6-4-判别式与生成式模型比较"><span><strong>6.4 判别式与生成式模型比较</strong></span></a></h3><h4 id="✅-一、两种学习范式" tabindex="-1"><a class="header-anchor" href="#✅-一、两种学习范式"><span>✅ 一、两种学习范式</span></a></h4><table><thead><tr><th>模型类型</th><th>目标</th><th>代表算法</th><th>核心思想</th><th></th></tr></thead><tbody><tr><td><strong>判别式模型 (Discriminative)</strong></td><td>直接学习条件概率 (P(y</td><td>x)) 或决策边界</td><td>逻辑回归、SVM、神经网络</td><td>关注“如何区分”</td></tr><tr><td><strong>生成式模型 (Generative)</strong></td><td>学习联合分布 (P(x, y)) 并通过贝叶斯公式推断</td><td>朴素贝叶斯、LDA、高斯混合模型</td><td>关注“如何生成”</td><td></td></tr></tbody></table><h4 id="✅-二、比较分析" tabindex="-1"><a class="header-anchor" href="#✅-二、比较分析"><span>✅ 二、比较分析</span></a></h4><table><thead><tr><th>维度</th><th>判别式模型</th><th>生成式模型</th></tr></thead><tbody><tr><td>目标</td><td>学分类边界</td><td>建模数据分布</td></tr><tr><td>表现</td><td>通常分类效果更好</td><td>样本少时鲁棒性强</td></tr><tr><td>可解释性</td><td>弱（黑箱）</td><td>强（有生成过程）</td></tr><tr><td>计算</td><td>训练更快</td><td>通常更复杂</td></tr><tr><td>典型模型</td><td>Logistic Regression, SVM</td><td>Naive Bayes, GMM</td></tr></tbody></table><h4 id="✅-三、统一视角" tabindex="-1"><a class="header-anchor" href="#✅-三、统一视角"><span>✅ 三、统一视角</span></a></h4><p>逻辑回归与朴素贝叶斯，虽然看似不同，但在数学上可以看作同一问题的两种路径：</p><ul><li><strong>生成式路径</strong>：先学 (P(x|y))，再用贝叶斯定理求 (P(y|x))；</li><li><strong>判别式路径</strong>：直接学 (P(y|x))。</li></ul><p>这也是机器学习理论发展的一个缩影：</p><blockquote><p>从“生成数据” → “区分数据” → “理解数据”。</p></blockquote><hr><h3 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span><strong>总结</strong></span></a></h3><table><thead><tr><th>小节</th><th>核心思想</th><th>关键公式 / 方法</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>6.1 线性回归</td><td>用超平面拟合连续输出</td><td>( \\hat{y}=w^T x+b )</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>6.2 逻辑回归</td><td>概率化的线性分类</td><td>( P(y=1</td><td>x)=\\sigma(w^T x+b) )</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>6.3 多项式与岭回归</td><td>用非线性特征或正则化提升泛化</td><td>( L=</td><td></td><td>y-Xw</td><td></td><td>^2+\\lambda</td><td></td><td>w</td><td></td><td>^2 )</td></tr><tr><td>6.4 判别式与生成式</td><td>两种建模范式</td><td>学 (P(y</td><td>x)) vs 学 (P(x,y))</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table>',79)])])}const p=a(s,[["render",n]]),l=JSON.parse('{"path":"/posts/ml/2025-11-03-06-ml-book-line-model.html","title":"第6章　线性模型家族","lang":"zh-CN","frontmatter":{"title":"第6章　线性模型家族","date":"2025-11-03T00:00:00.000Z","categories":["AI"],"tags":["ai","learn-note"],"published":true,"description":"第6章 线性模型家族 线性模型是机器学习中最早、最经典、也是最具代表性的算法家族。 几乎所有复杂模型（包括神经网络）在本质上都可以看作“非线性的线性组合”。 线性模型的魅力在于： 简单而强大 —— 可以解释、可计算、可扩展； 理论完备 —— 有清晰的概率解释和几何意义； 实用性极高 —— 仍被广泛用于工业界的特征工程、基线模型和可解释建模。 6.1 线...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"第6章　线性模型家族\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-11-03T00:00:00.000Z\\",\\"dateModified\\":\\"2025-12-27T05:15:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"老马啸西风\\",\\"url\\":\\"https://houbb.github.io\\"}]}"],["meta",{"property":"og:url","content":"https://houbb.github.io/awesome-ai-coding/posts/ml/2025-11-03-06-ml-book-line-model.html"}],["meta",{"property":"og:site_name","content":"老马啸西风"}],["meta",{"property":"og:title","content":"第6章　线性模型家族"}],["meta",{"property":"og:description","content":"第6章 线性模型家族 线性模型是机器学习中最早、最经典、也是最具代表性的算法家族。 几乎所有复杂模型（包括神经网络）在本质上都可以看作“非线性的线性组合”。 线性模型的魅力在于： 简单而强大 —— 可以解释、可计算、可扩展； 理论完备 —— 有清晰的概率解释和几何意义； 实用性极高 —— 仍被广泛用于工业界的特征工程、基线模型和可解释建模。 6.1 线..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-27T05:15:15.000Z"}],["meta",{"property":"article:tag","content":"learn-note"}],["meta",{"property":"article:tag","content":"ai"}],["meta",{"property":"article:published_time","content":"2025-11-03T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-27T05:15:15.000Z"}]]},"git":{"createdTime":1766812269000,"updatedTime":1766812515000,"contributors":[{"name":"bbhou","username":"bbhou","email":"1557740299@qq.com","commits":3,"url":"https://github.com/bbhou"}]},"readingTime":{"minutes":5.18,"words":1554},"filePathRelative":"posts/ml/2025-11-03-06-ml-book-line-model.md","excerpt":"<h2>第6章　线性模型家族</h2>\\n<p>线性模型是机器学习中最早、最经典、也是最具代表性的算法家族。</p>\\n<p>几乎所有复杂模型（包括神经网络）在本质上都可以看作“非线性的线性组合”。</p>\\n<p>线性模型的魅力在于：</p>\\n<ul>\\n<li><strong>简单而强大</strong> —— 可以解释、可计算、可扩展；</li>\\n<li><strong>理论完备</strong> —— 有清晰的概率解释和几何意义；</li>\\n<li><strong>实用性极高</strong> —— 仍被广泛用于工业界的特征工程、基线模型和可解释建模。</li>\\n</ul>\\n<hr>\\n<h3><strong>6.1 线性回归</strong></h3>","autoDesc":true}');export{p as comp,l as data};
