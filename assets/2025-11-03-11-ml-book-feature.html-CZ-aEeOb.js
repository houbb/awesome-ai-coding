import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as r,a as n,o as a}from"./app-D4koU7iK.js";const o={};function i(l,t){return a(),r("div",null,[...t[0]||(t[0]=[n('<h1 id="第11章-特征工程与数据处理" tabindex="-1"><a class="header-anchor" href="#第11章-特征工程与数据处理"><span>第11章　特征工程与数据处理</span></a></h1><p>机器学习的成败，往往不在模型复杂度，而在于数据质量与特征表达能力。</p><p>这一章聚焦于如何「让模型看懂数据」，从特征提取、选择、编码到特征交互与自动化管理，系统解析特征工程的全流程。</p><hr><h2 id="_11-1-特征提取、选择与编码" tabindex="-1"><a class="header-anchor" href="#_11-1-特征提取、选择与编码"><span><strong>11.1 特征提取、选择与编码</strong></span></a></h2><h3 id="🧱-1-特征提取-feature-extraction" tabindex="-1"><a class="header-anchor" href="#🧱-1-特征提取-feature-extraction"><span>🧱 1. 特征提取（Feature Extraction）</span></a></h3><p><strong>目标：</strong> 将原始数据（文本、图像、日志、信号等）转化为机器可理解的数值表示。</p><p><strong>典型方式：</strong></p><ul><li><p><strong>文本数据：</strong></p><ul><li>Bag of Words（词袋模型）</li><li>TF-IDF（词频-逆文档频率）</li><li>Word2Vec / FastText（分布式词向量）</li><li>BERT embedding（上下文语义特征）</li></ul></li><li><p><strong>图像数据：</strong></p><ul><li>手工特征：SIFT、HOG、LBP</li><li>深度特征：CNN 提取的高维嵌入</li></ul></li><li><p><strong>时间序列 / 传感器：</strong></p><ul><li>滑动窗口、FFT（频域变换）、统计量（均值、方差、峰度）</li></ul></li><li><p><strong>结构化数据：</strong></p><ul><li>比例、差值、交叉项、聚合统计（groupby + mean）</li></ul></li></ul><p>💡 <em>特征提取的本质：用数学结构描述现实世界的规律。</em></p><hr><h3 id="🧮-2-特征选择-feature-selection" tabindex="-1"><a class="header-anchor" href="#🧮-2-特征选择-feature-selection"><span>🧮 2. 特征选择（Feature Selection）</span></a></h3><p><strong>目标：</strong> 在海量特征中挑出最有用的子集，降低过拟合、提升训练速度。</p><p><strong>方法分类：</strong></p><ul><li><strong>Filter（过滤法）</strong>：独立评估单个特征与目标的关系<br> 如：相关系数、卡方检验、互信息、方差选择。</li><li><strong>Wrapper（包装法）</strong>：用模型性能来评估特征子集<br> 如：递归特征消除（RFE）、前向/后向选择。</li><li><strong>Embedded（嵌入法）</strong>：模型训练过程中自动筛选特征<br> 如：L1 正则（Lasso）、树模型的特征重要性（XGBoost feature_importances_）。</li></ul><p>💡 <em>高维不等于高效，选择比堆砌更重要。</em></p><hr><h3 id="🔠-3-特征编码-feature-encoding" tabindex="-1"><a class="header-anchor" href="#🔠-3-特征编码-feature-encoding"><span>🔠 3. 特征编码（Feature Encoding）</span></a></h3><p><strong>目标：</strong> 将非数值型或类别特征转换为数值表示。</p><p><strong>常见方式：</strong></p><table><thead><tr><th>类型</th><th>编码方法</th><th>说明</th></tr></thead><tbody><tr><td>类别型</td><td>One-Hot</td><td>为每个类别创建独立维度</td></tr><tr><td>类别型</td><td>Label Encoding</td><td>将类别映射为整数（有序性假设风险）</td></tr><tr><td>类别型</td><td>Target Encoding</td><td>用类别对应的平均目标值代替</td></tr><tr><td>高基数类别</td><td>Embedding Encoding</td><td>用低维向量表示（常用于推荐系统）</td></tr><tr><td>时间型</td><td>周期编码</td><td>将时间特征映射为 sin/cos（处理周期性）</td></tr></tbody></table><p>💡 <em>正确的编码方式决定了模型是否能“理解”特征的语义结构。</em></p><hr><h2 id="_11-2-标准化、归一化、缺失值处理" tabindex="-1"><a class="header-anchor" href="#_11-2-标准化、归一化、缺失值处理"><span><strong>11.2 标准化、归一化、缺失值处理</strong></span></a></h2><h3 id="⚖️-1-标准化-standardization" tabindex="-1"><a class="header-anchor" href="#⚖️-1-标准化-standardization"><span>⚖️ 1. 标准化（Standardization）</span></a></h3><p>将数据转化为均值为 0、方差为 1 的分布。<br> 公式：<br> [<br> x&#39; = \\frac{x - \\mu}{\\sigma}<br> ]<br> 适用于梯度敏感算法（如线性回归、SVM、神经网络）。</p><hr><h3 id="📏-2-归一化-normalization" tabindex="-1"><a class="header-anchor" href="#📏-2-归一化-normalization"><span>📏 2. 归一化（Normalization）</span></a></h3><p>将数值压缩到指定区间（通常是 [0,1]）。<br> 公式：<br> [<br> x&#39; = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}<br> ]<br> 适用于基于距离的模型（如 KNN、K-means）。</p><hr><h3 id="⚠️-3-缺失值处理" tabindex="-1"><a class="header-anchor" href="#⚠️-3-缺失值处理"><span>⚠️ 3. 缺失值处理</span></a></h3><p><strong>缺失的原因</strong>：系统故障、用户跳过输入、采样问题。</p><p><strong>常见策略：</strong></p><ul><li>删除：样本或特征缺失太多。</li><li>填充：均值/中位数/众数/前值/模型预测。</li><li>增强：用缺失标志特征（is_null）标识。</li></ul><p>💡 <em>缺失值往往蕴含隐含信息，比如“没填性别”可能代表某类用户。</em></p><hr><h2 id="_11-3-特征交互与高维稀疏化" tabindex="-1"><a class="header-anchor" href="#_11-3-特征交互与高维稀疏化"><span><strong>11.3 特征交互与高维稀疏化</strong></span></a></h2><h3 id="🔄-1-特征交互-feature-interaction" tabindex="-1"><a class="header-anchor" href="#🔄-1-特征交互-feature-interaction"><span>🔄 1. 特征交互（Feature Interaction）</span></a></h3><p>通过组合多个特征，捕捉非线性关系。</p><p><strong>示例：</strong></p><ul><li>二阶交互项：<code>x1 * x2</code></li><li>逻辑交互：<code>(province, gender)</code> → “广东+女”</li><li>特征交叉编码（Cross Feature Embedding）：推荐系统中常见</li></ul><p><strong>自动化生成方式：</strong></p><ul><li>PolynomialFeatures（sklearn）</li><li>DeepFM / Wide&amp;Deep（深度模型自动学习交互项）</li></ul><hr><h3 id="🕳️-2-高维稀疏问题" tabindex="-1"><a class="header-anchor" href="#🕳️-2-高维稀疏问题"><span>🕳️ 2. 高维稀疏问题</span></a></h3><p>特征交叉和编码会让特征空间爆炸，比如 One-Hot 后从几百维变成几十万维。</p><p><strong>常见解决策略：</strong></p><ul><li>降维（PCA、SVD、AutoEncoder）</li><li>特征选择（L1正则）</li><li>Hash Trick（哈希技巧）</li><li>稀疏矩阵存储（scipy.sparse）</li></ul><p>💡 <em>高维并非高效，机器学习模型更怕噪声特征。</em></p><hr><h2 id="_11-4-自动特征工程与-feature-store" tabindex="-1"><a class="header-anchor" href="#_11-4-自动特征工程与-feature-store"><span><strong>11.4 自动特征工程与 Feature Store</strong></span></a></h2><h3 id="🤖-1-自动特征工程-auto-feature-engineering" tabindex="-1"><a class="header-anchor" href="#🤖-1-自动特征工程-auto-feature-engineering"><span>🤖 1. 自动特征工程（Auto Feature Engineering）</span></a></h3><p>随着 AutoML 的兴起，特征工程也进入自动化时代。</p><p><strong>典型方法：</strong></p><ul><li><strong>FeatureTools</strong>：基于时间序列关系自动生成交互特征。</li><li><strong>Auto-sklearn / TPOT</strong>：自动搜索特征组合 + 模型管道。</li><li><strong>深度特征学习</strong>：通过深度神经网络自动学习最优特征空间。</li></ul><p><strong>优点：</strong></p><ul><li>减少人工试错</li><li>适应大规模异构数据</li><li>支撑快速实验与生产部署</li></ul><hr><h3 id="🧱-2-feature-store-特征存储系统" tabindex="-1"><a class="header-anchor" href="#🧱-2-feature-store-特征存储系统"><span>🧱 2. Feature Store（特征存储系统）</span></a></h3><p>在企业级机器学习平台中，<strong>Feature Store</strong> 是特征工程的中枢。</p><p><strong>功能：</strong></p><ul><li>特征计算与管理（版本、权限、监控）</li><li>在线/离线特征一致性保障</li><li>跨模型特征共享与复用</li><li>特征数据血缘追踪</li></ul><p><strong>代表系统：</strong></p><ul><li>Uber Michelangelo Feature Store</li><li>Feast（Google 开源）</li><li>Tecton、Databricks Feature Store</li></ul><p>💡 <em>Feature Store 让“特征”成为可复用的资产，而不只是训练阶段的临时产物。</em></p><hr><h2 id="✅-本章小结" tabindex="-1"><a class="header-anchor" href="#✅-本章小结"><span>✅ 本章小结</span></a></h2><table><thead><tr><th>模块</th><th>目标</th><th>关键思想</th></tr></thead><tbody><tr><td>特征提取</td><td>从原始数据中提炼信号</td><td>“看懂”数据</td></tr><tr><td>特征选择</td><td>去除无效与冗余特征</td><td>降噪提效</td></tr><tr><td>特征编码</td><td>转化非数值型特征</td><td>可计算化</td></tr><tr><td>标准化与归一化</td><td>保持尺度一致</td><td>提升优化效率</td></tr><tr><td>特征交互</td><td>捕捉非线性模式</td><td>提升表达力</td></tr><tr><td>自动特征工程</td><td>让系统自动挖掘特征</td><td>提升迭代速度</td></tr><tr><td>Feature Store</td><td>管理特征全生命周期</td><td>工程化落地</td></tr></tbody></table>',68)])])}const p=e(o,[["render",i]]),h=JSON.parse('{"path":"/posts/ml/2025-11-03-11-ml-book-feature.html","title":"第11章　特征工程与数据处理","lang":"zh-CN","frontmatter":{"title":"第11章　特征工程与数据处理","date":"2025-11-03T00:00:00.000Z","categories":["AI"],"tags":["ai","learn-note"],"published":true,"description":"第11章 特征工程与数据处理 机器学习的成败，往往不在模型复杂度，而在于数据质量与特征表达能力。 这一章聚焦于如何「让模型看懂数据」，从特征提取、选择、编码到特征交互与自动化管理，系统解析特征工程的全流程。 11.1 特征提取、选择与编码 🧱 1. 特征提取（Feature Extraction） 目标： 将原始数据（文本、图像、日志、信号等）转化为...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"第11章　特征工程与数据处理\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-11-03T00:00:00.000Z\\",\\"dateModified\\":\\"2025-12-27T05:15:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"老马啸西风\\",\\"url\\":\\"https://houbb.github.io\\"}]}"],["meta",{"property":"og:url","content":"https://houbb.github.io/awesome-ai-coding/posts/ml/2025-11-03-11-ml-book-feature.html"}],["meta",{"property":"og:site_name","content":"老马啸西风"}],["meta",{"property":"og:title","content":"第11章　特征工程与数据处理"}],["meta",{"property":"og:description","content":"第11章 特征工程与数据处理 机器学习的成败，往往不在模型复杂度，而在于数据质量与特征表达能力。 这一章聚焦于如何「让模型看懂数据」，从特征提取、选择、编码到特征交互与自动化管理，系统解析特征工程的全流程。 11.1 特征提取、选择与编码 🧱 1. 特征提取（Feature Extraction） 目标： 将原始数据（文本、图像、日志、信号等）转化为..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-27T05:15:15.000Z"}],["meta",{"property":"article:tag","content":"learn-note"}],["meta",{"property":"article:tag","content":"ai"}],["meta",{"property":"article:published_time","content":"2025-11-03T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-27T05:15:15.000Z"}]]},"git":{"createdTime":1766812269000,"updatedTime":1766812515000,"contributors":[{"name":"bbhou","username":"bbhou","email":"1557740299@qq.com","commits":3,"url":"https://github.com/bbhou"}]},"readingTime":{"minutes":4.52,"words":1356},"filePathRelative":"posts/ml/2025-11-03-11-ml-book-feature.md","excerpt":"\\n<p>机器学习的成败，往往不在模型复杂度，而在于数据质量与特征表达能力。</p>\\n<p>这一章聚焦于如何「让模型看懂数据」，从特征提取、选择、编码到特征交互与自动化管理，系统解析特征工程的全流程。</p>\\n<hr>\\n<h2><strong>11.1 特征提取、选择与编码</strong></h2>\\n<h3>🧱 1. 特征提取（Feature Extraction）</h3>\\n<p><strong>目标：</strong> 将原始数据（文本、图像、日志、信号等）转化为机器可理解的数值表示。</p>\\n<p><strong>典型方式：</strong></p>\\n<ul>\\n<li>\\n<p><strong>文本数据：</strong></p>\\n<ul>\\n<li>Bag of Words（词袋模型）</li>\\n<li>TF-IDF（词频-逆文档频率）</li>\\n<li>Word2Vec / FastText（分布式词向量）</li>\\n<li>BERT embedding（上下文语义特征）</li>\\n</ul>\\n</li>\\n<li>\\n<p><strong>图像数据：</strong></p>\\n<ul>\\n<li>手工特征：SIFT、HOG、LBP</li>\\n<li>深度特征：CNN 提取的高维嵌入</li>\\n</ul>\\n</li>\\n<li>\\n<p><strong>时间序列 / 传感器：</strong></p>\\n<ul>\\n<li>滑动窗口、FFT（频域变换）、统计量（均值、方差、峰度）</li>\\n</ul>\\n</li>\\n<li>\\n<p><strong>结构化数据：</strong></p>\\n<ul>\\n<li>比例、差值、交叉项、聚合统计（groupby + mean）</li>\\n</ul>\\n</li>\\n</ul>","autoDesc":true}');export{p as comp,h as data};
