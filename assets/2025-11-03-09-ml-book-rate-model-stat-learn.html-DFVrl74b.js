import{_ as r}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as a,a as n,o as e}from"./app-D4koU7iK.js";const o={};function s(i,t){return e(),a("div",null,[...t[0]||(t[0]=[n('<h2 id="第9章-概率模型与统计学习" tabindex="-1"><a class="header-anchor" href="#第9章-概率模型与统计学习"><span><strong>第9章　概率模型与统计学习</strong></span></a></h2><h3 id="_9-1-朴素贝叶斯-naive-bayes" tabindex="-1"><a class="header-anchor" href="#_9-1-朴素贝叶斯-naive-bayes"><span><strong>9.1 朴素贝叶斯（Naive Bayes）</strong></span></a></h3><h4 id="_1-核心思想" tabindex="-1"><a class="header-anchor" href="#_1-核心思想"><span><strong>（1）核心思想</strong></span></a></h4><p>朴素贝叶斯是一种基于 <strong>贝叶斯定理（Bayes’ theorem）</strong> 和 <strong>特征条件独立假设</strong> 的概率分类方法。</p><p>贝叶斯定理：</p><p>[<br> P(y|x) = \\frac{P(x|y)P(y)}{P(x)}<br> ]</p><p>其中：</p><ul><li>(P(y))：先验概率（类别的总体概率）；</li><li>(P(x|y))：似然（在类别 y 下出现特征 x 的概率）；</li><li>(P(y|x))：后验概率（给定特征后属于类别 y 的概率）。</li></ul><p><strong>核心假设</strong>：<br> 所有特征 (x_i) 条件独立：<br> [<br> P(x|y) = \\prod_i P(x_i|y)<br> ]<br> 这就是“朴素（Naive）”的由来。</p><hr><h4 id="_2-分类过程" tabindex="-1"><a class="header-anchor" href="#_2-分类过程"><span><strong>（2）分类过程</strong></span></a></h4><ol><li>统计每个类别的先验概率 (P(y))；</li><li>统计每个特征在类别下的条件概率 (P(x_i|y))；</li><li>对新的样本 x，计算每个类别的后验概率：<br> [<br> P(y|x) \\propto P(y)\\prod_i P(x_i|y)<br> ]</li><li>选取后验概率最大的类别。</li></ol><h4 id="_3-常见变体" tabindex="-1"><a class="header-anchor" href="#_3-常见变体"><span><strong>（3）常见变体</strong></span></a></h4><ul><li><strong>高斯朴素贝叶斯（GaussianNB）</strong>：特征服从正态分布；</li><li><strong>多项式朴素贝叶斯（MultinomialNB）</strong>：用于文本词频；</li><li><strong>伯努利朴素贝叶斯（BernoulliNB）</strong>：用于二值特征。</li></ul><hr><h4 id="_4-优缺点" tabindex="-1"><a class="header-anchor" href="#_4-优缺点"><span><strong>（4）优缺点</strong></span></a></h4><table><thead><tr><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>简单高效，训练和预测速度快</td><td>条件独立假设过强</td></tr><tr><td>对小数据集鲁棒</td><td>不能捕捉特征间依赖</td></tr><tr><td>适合高维稀疏数据（如文本）</td><td>精度不及复杂模型</td></tr></tbody></table><hr><h4 id="_5-典型应用" tabindex="-1"><a class="header-anchor" href="#_5-典型应用"><span><strong>（5）典型应用</strong></span></a></h4><ul><li>垃圾邮件识别；</li><li>情感分类；</li><li>文本主题识别；</li><li>文档分类（如新闻、评论分类）。</li></ul><hr><h3 id="_9-2-隐马尔可夫模型-hidden-markov-model-hmm" tabindex="-1"><a class="header-anchor" href="#_9-2-隐马尔可夫模型-hidden-markov-model-hmm"><span><strong>9.2 隐马尔可夫模型（Hidden Markov Model, HMM）</strong></span></a></h3><h4 id="_1-核心思想-1" tabindex="-1"><a class="header-anchor" href="#_1-核心思想-1"><span><strong>（1）核心思想</strong></span></a></h4><p>HMM 是一个 <strong>带有隐状态的随机过程</strong>，假设：</p><ol><li>存在一个不可见的状态序列 (S = (s_1, s_2, ..., s_T))；</li><li>每个状态会生成一个可见的观测序列 (O = (o_1, o_2, ..., o_T))；</li><li>状态转移满足马尔可夫性：<br> [<br> P(s_t | s_{t-1}, ..., s_1) = P(s_t | s_{t-1})<br> ]</li></ol><p>换句话说：<strong>未来的状态只与当前状态有关，而与过去无关</strong>。</p><hr><h4 id="_2-模型参数" tabindex="-1"><a class="header-anchor" href="#_2-模型参数"><span><strong>（2）模型参数</strong></span></a></h4><p>HMM 通常由三个参数定义（记为 λ）：</p><p>[<br> \\lambda = (A, B, \\pi)<br> ]</p><ul><li>(A)：状态转移概率矩阵；</li><li>(B)：观测概率矩阵；</li><li>(\\pi)：初始状态分布。</li></ul><hr><h4 id="_3-三个核心问题" tabindex="-1"><a class="header-anchor" href="#_3-三个核心问题"><span><strong>（3）三个核心问题</strong></span></a></h4><ol><li><p><strong>评估问题（Evaluation）</strong><br> 计算给定模型和观测序列的概率 (P(O|\\lambda))。<br> → 用 <strong>前向-后向算法（Forward-Backward Algorithm）</strong>。</p></li><li><p><strong>解码问题（Decoding）</strong><br> 寻找最可能的状态序列。<br> → 用 <strong>维特比算法（Viterbi Algorithm）</strong>。</p></li><li><p><strong>学习问题（Learning）</strong><br> 根据观测数据估计模型参数 (A, B, \\pi)。<br> → 用 <strong>Baum-Welch 算法</strong>（EM思想的具体实现）。</p></li></ol><hr><h4 id="_4-典型应用" tabindex="-1"><a class="header-anchor" href="#_4-典型应用"><span><strong>（4）典型应用</strong></span></a></h4><ul><li><strong>语音识别</strong>（最早的成功应用）；</li><li><strong>词性标注（POS tagging）</strong>；</li><li><strong>命名实体识别（NER）</strong>；</li><li><strong>手写识别、基因序列建模</strong>。</li></ul><hr><h3 id="_9-3-条件随机场-conditional-random-field-crf" tabindex="-1"><a class="header-anchor" href="#_9-3-条件随机场-conditional-random-field-crf"><span><strong>9.3 条件随机场（Conditional Random Field, CRF）</strong></span></a></h3><h4 id="_1-hmm-的局限" tabindex="-1"><a class="header-anchor" href="#_1-hmm-的局限"><span><strong>（1）HMM 的局限</strong></span></a></h4><p>HMM 是生成式模型，只能建模：<br> [<br> P(X, Y) = P(Y)P(X|Y)<br> ]<br> 无法直接建模条件概率 (P(Y|X))，并且要求状态转移和观测独立。</p><h4 id="_2-crf-的思想" tabindex="-1"><a class="header-anchor" href="#_2-crf-的思想"><span><strong>（2）CRF 的思想</strong></span></a></h4><p>CRF 是一种 <strong>判别式的概率图模型</strong>，直接建模：<br> [<br> P(Y|X) = \\frac{1}{Z(X)} \\exp \\left( \\sum_k \\lambda_k f_k(Y, X) \\right)<br> ]<br> 其中：</p><ul><li>(f_k(Y, X))：特征函数；</li><li>(\\lambda_k)：特征权重；</li><li>(Z(X))：归一化因子。</li></ul><p><strong>核心思想</strong>：不用关心数据是怎么生成的，只需建模“在给定输入 X 时，标签序列 Y 的条件概率”。</p><hr><h4 id="_3-与-hmm-的关系" tabindex="-1"><a class="header-anchor" href="#_3-与-hmm-的关系"><span><strong>（3）与 HMM 的关系</strong></span></a></h4><table><thead><tr><th>比较项</th><th>HMM</th><th>CRF</th></tr></thead><tbody><tr><td>模型类型</td><td>生成式</td><td>判别式</td></tr><tr><td>依赖假设</td><td>强独立性</td><td>放宽独立性</td></tr><tr><td>特征使用</td><td>仅限观测概率</td><td>可引入任意特征</td></tr><tr><td>性能</td><td>一般</td><td>更高</td></tr></tbody></table><hr><h4 id="_4-crf-的训练与推断" tabindex="-1"><a class="header-anchor" href="#_4-crf-的训练与推断"><span><strong>（4）CRF 的训练与推断</strong></span></a></h4><ul><li><strong>训练</strong>：最大化对数似然函数；</li><li><strong>推断</strong>：常用维特比算法或前向-后向算法。</li></ul><hr><h4 id="_5-典型应用-1" tabindex="-1"><a class="header-anchor" href="#_5-典型应用-1"><span><strong>（5）典型应用</strong></span></a></h4><ul><li>序列标注任务（词性标注、命名实体识别、分词）；</li><li>生物信息学序列分析；</li><li>OCR（光学字符识别）；</li><li>对话状态追踪。</li></ul><hr><h3 id="_9-4-em-算法与期望最大化思想" tabindex="-1"><a class="header-anchor" href="#_9-4-em-算法与期望最大化思想"><span><strong>9.4 EM 算法与期望最大化思想</strong></span></a></h3><h4 id="_1-核心思想-2" tabindex="-1"><a class="header-anchor" href="#_1-核心思想-2"><span><strong>（1）核心思想</strong></span></a></h4><p>当数据中包含 <strong>隐变量（latent variables）</strong> 时，直接求解最大似然变得困难。<br><strong>EM算法（Expectation-Maximization）</strong> 提供了一种 <strong>迭代优化方法</strong>。</p><p>它交替执行两步：</p><ol><li><strong>E步（期望步）</strong>：在当前参数下，计算隐变量的期望；</li><li><strong>M步（最大化步）</strong>：在期望的基础上，重新估计模型参数以最大化似然。</li></ol><p>[<br> \\text{E-step: } Q(\\theta | \\theta^{(t)}) = E_{Z|X,\\theta^{(t)}}[\\log P(X,Z|\\theta)]<br> ]<br> [<br> \\text{M-step: } \\theta^{(t+1)} = \\arg\\max_\\theta Q(\\theta | \\theta^{(t)})<br> ]</p><hr><h4 id="_2-直观理解" tabindex="-1"><a class="header-anchor" href="#_2-直观理解"><span><strong>（2）直观理解</strong></span></a></h4><p>你可以把 EM 看作“猜测 + 修正”的过程：</p><ul><li>猜测隐变量的分布（E步）；</li><li>根据猜测重新估计模型参数（M步）；</li><li>反复执行，直到收敛。</li></ul><hr><h4 id="_3-典型应用" tabindex="-1"><a class="header-anchor" href="#_3-典型应用"><span><strong>（3）典型应用</strong></span></a></h4><ul><li>高斯混合模型（GMM）聚类；</li><li>隐马尔可夫模型（HMM）参数学习；</li><li>缺失数据的估计；</li><li>图像分割；</li><li>用户画像聚类（如软聚类）。</li></ul><hr><h3 id="✅-总结-概率模型的演进逻辑" tabindex="-1"><a class="header-anchor" href="#✅-总结-概率模型的演进逻辑"><span>✅ <strong>总结：概率模型的演进逻辑</strong></span></a></h3><table><thead><tr><th>模型</th><th>类型</th><th>核心思想</th><th>典型应用</th></tr></thead><tbody><tr><td>朴素贝叶斯</td><td>生成式</td><td>条件独立假设</td><td>文本分类、垃圾邮件检测</td></tr><tr><td>HMM</td><td>生成式</td><td>序列状态转移 + 发射概率</td><td>语音识别、词性标注</td></tr><tr><td>CRF</td><td>判别式</td><td>条件概率建模 + 全局最优</td><td>NER、分词、序列标注</td></tr><tr><td>EM算法</td><td>优化方法</td><td>隐变量 + 期望最大化</td><td>GMM、HMM、聚类</td></tr></tbody></table><blockquote><p>📘 <strong>一句话总结</strong>：</p></blockquote><blockquote><p>朴素贝叶斯是“概率学习的起点”，HMM 是“序列概率的里程碑”，CRF 是“结构化预测的代表”，而 EM 是“隐变量问题的通用解法”。</p></blockquote>',73)])])}const h=r(o,[["render",s]]),p=JSON.parse('{"path":"/posts/ml/2025-11-03-09-ml-book-rate-model-stat-learn.html","title":"第9章　概率模型与统计学习","lang":"zh-CN","frontmatter":{"title":"第9章　概率模型与统计学习","date":"2025-11-03T00:00:00.000Z","categories":["AI"],"tags":["ai","learn-note"],"published":true,"description":"第9章 概率模型与统计学习 9.1 朴素贝叶斯（Naive Bayes） （1）核心思想 朴素贝叶斯是一种基于 贝叶斯定理（Bayes’ theorem） 和 特征条件独立假设 的概率分类方法。 贝叶斯定理： [ P(y|x) = \\\\frac{P(x|y)P(y)}{P(x)} ] 其中： (P(y))：先验概率（类别的总体概率）； (P(x|y))：...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"第9章　概率模型与统计学习\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-11-03T00:00:00.000Z\\",\\"dateModified\\":\\"2025-12-27T05:15:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"老马啸西风\\",\\"url\\":\\"https://houbb.github.io\\"}]}"],["meta",{"property":"og:url","content":"https://houbb.github.io/awesome-ai-coding/posts/ml/2025-11-03-09-ml-book-rate-model-stat-learn.html"}],["meta",{"property":"og:site_name","content":"老马啸西风"}],["meta",{"property":"og:title","content":"第9章　概率模型与统计学习"}],["meta",{"property":"og:description","content":"第9章 概率模型与统计学习 9.1 朴素贝叶斯（Naive Bayes） （1）核心思想 朴素贝叶斯是一种基于 贝叶斯定理（Bayes’ theorem） 和 特征条件独立假设 的概率分类方法。 贝叶斯定理： [ P(y|x) = \\\\frac{P(x|y)P(y)}{P(x)} ] 其中： (P(y))：先验概率（类别的总体概率）； (P(x|y))：..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-27T05:15:15.000Z"}],["meta",{"property":"article:tag","content":"learn-note"}],["meta",{"property":"article:tag","content":"ai"}],["meta",{"property":"article:published_time","content":"2025-11-03T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-27T05:15:15.000Z"}]]},"git":{"createdTime":1766812269000,"updatedTime":1766812515000,"contributors":[{"name":"bbhou","username":"bbhou","email":"1557740299@qq.com","commits":3,"url":"https://github.com/bbhou"}]},"readingTime":{"minutes":4.66,"words":1398},"filePathRelative":"posts/ml/2025-11-03-09-ml-book-rate-model-stat-learn.md","excerpt":"<h2><strong>第9章　概率模型与统计学习</strong></h2>\\n<h3><strong>9.1 朴素贝叶斯（Naive Bayes）</strong></h3>\\n<h4><strong>（1）核心思想</strong></h4>\\n<p>朴素贝叶斯是一种基于 <strong>贝叶斯定理（Bayes’ theorem）</strong> 和 <strong>特征条件独立假设</strong> 的概率分类方法。</p>\\n<p>贝叶斯定理：</p>\\n<p>[<br>\\nP(y|x) = \\\\frac{P(x|y)P(y)}{P(x)}<br>\\n]</p>\\n<p>其中：</p>\\n<ul>\\n<li>(P(y))：先验概率（类别的总体概率）；</li>\\n<li>(P(x|y))：似然（在类别 y 下出现特征 x 的概率）；</li>\\n<li>(P(y|x))：后验概率（给定特征后属于类别 y 的概率）。</li>\\n</ul>","autoDesc":true}');export{h as comp,p as data};
