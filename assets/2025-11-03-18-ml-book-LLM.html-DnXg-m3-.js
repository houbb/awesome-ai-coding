import{_ as r}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as n,a as e,o}from"./app-C3GE2GkA.js";const a={};function s(l,t){return o(),n("div",null,[...t[0]||(t[0]=[e('<p>第18章是整本书的“现代 AI 篇”的核心部分——它讲述了从「词向量」到「BERT」，再到「GPT 与智能体（Agent）」的整条技术演化主线。</p><p>这一章不仅是机器学习与深度学习的融合点，更是当代人工智能（尤其是大语言模型）的技术根基。</p><hr><h1 id="第18章-大模型与预训练范式" tabindex="-1"><a class="header-anchor" href="#第18章-大模型与预训练范式"><span>第18章　大模型与预训练范式</span></a></h1><hr><h2 id="_18-1-从词向量到-bert" tabindex="-1"><a class="header-anchor" href="#_18-1-从词向量到-bert"><span><strong>18.1 从词向量到 BERT</strong></span></a></h2><h3 id="🔹-一、传统nlp的局限" tabindex="-1"><a class="header-anchor" href="#🔹-一、传统nlp的局限"><span>🔹 一、传统NLP的局限</span></a></h3><p>在早期的自然语言处理中，机器学习模型（如SVM、朴素贝叶斯）使用的特征往往是：</p><ul><li>词袋模型（Bag of Words）</li><li>TF-IDF 向量<br> 它们<strong>忽略了词序</strong>，也无法捕捉语义。例如：</li></ul><blockquote><p>“我爱学习” 与 “学习我爱” 的TF-IDF几乎一样。</p></blockquote><p>这种稀疏、高维、无语义的表示严重限制了机器对语言的理解。</p><hr><h3 id="🔹-二、分布式表示与词向量-word-embedding" tabindex="-1"><a class="header-anchor" href="#🔹-二、分布式表示与词向量-word-embedding"><span>🔹 二、分布式表示与词向量（Word Embedding）</span></a></h3><p>20世纪80年代后，学者提出了一个关键假设：</p><blockquote><p><strong>词义可以通过上下文来定义。</strong><br> ——“你能看出一个词的意义，就看它周围的词。”</p></blockquote><p>这催生了“分布式词表示”的思想：<br> 每个词都可以表示为一个低维、稠密的向量，语义相似的词在空间中也相近。</p><h4 id="代表模型" tabindex="-1"><a class="header-anchor" href="#代表模型"><span>代表模型：</span></a></h4><ul><li><p><strong>Word2Vec（2013, Google）</strong></p><ul><li><p><strong>Skip-gram / CBOW</strong> 模型：通过预测上下文或目标词学习词向量。</p></li><li><p>能捕捉“语义关系”：</p><blockquote><p>vector(&quot;国王&quot;) - vector(&quot;男人&quot;) + vector(&quot;女人&quot;) ≈ vector(&quot;王后&quot;)</p></blockquote></li></ul></li><li><p><strong>GloVe（2014, Stanford）</strong></p><ul><li>利用全局词频共现信息（统计方法 + 神经网络）。</li></ul></li></ul><p>这阶段的模型<strong>静态</strong>，同一个词无论出现在什么语境中都只有一个向量。</p><hr><h3 id="🔹-三、从静态到动态-上下文词向量" tabindex="-1"><a class="header-anchor" href="#🔹-三、从静态到动态-上下文词向量"><span>🔹 三、从静态到动态：上下文词向量</span></a></h3><p>自然语言的语义往往取决于上下文。<br> 例如：“bank”在以下两句话中的含义不同：</p><blockquote><ul><li>I went to the <strong>bank</strong> to deposit money.</li><li>The boat is on the <strong>bank</strong> of the river.</li></ul></blockquote><p>于是研究者开始探索<strong>上下文相关的词向量</strong>：</p><h4 id="📘-代表进展" tabindex="-1"><a class="header-anchor" href="#📘-代表进展"><span>📘 代表进展：</span></a></h4><ul><li><strong>ELMo（2018, AllenNLP）</strong>：双向LSTM建模上下文；</li><li><strong>ULMFiT（2018, <a href="http://fast.ai" target="_blank" rel="noopener noreferrer">fast.ai</a>）</strong>：提出“预训练 + 微调”的思想雏形；</li><li><strong>BERT（2018, Google）</strong>：基于 Transformer 的双向编码器，彻底引爆 NLP。</li></ul><hr><h3 id="🔹-四、bert-的关键创新" tabindex="-1"><a class="header-anchor" href="#🔹-四、bert-的关键创新"><span>🔹 四、BERT 的关键创新</span></a></h3><p>BERT（Bidirectional Encoder Representations from Transformers）将 NLP 带入了“预训练语言模型”时代。</p><p>核心思想：</p><ol><li><p><strong>双向上下文建模</strong>：通过 Transformer Encoder 同时看到左右语境；</p></li><li><p><strong>自监督预训练任务</strong>：</p><ul><li><strong>MLM（Masked Language Modeling）</strong>：随机遮盖部分词，预测被遮盖的词；</li><li><strong>NSP（Next Sentence Prediction）</strong>：预测句子是否连续；</li></ul></li><li><p><strong>通用语义表示</strong>：在大规模语料上预训练，再迁移到下游任务（如分类、问答、命名实体识别等）。</p></li></ol><hr><h2 id="_18-2-预训练—微调范式" tabindex="-1"><a class="header-anchor" href="#_18-2-预训练—微调范式"><span><strong>18.2 预训练—微调范式</strong></span></a></h2><h3 id="🔹-一、传统做法的缺陷" tabindex="-1"><a class="header-anchor" href="#🔹-一、传统做法的缺陷"><span>🔹 一、传统做法的缺陷</span></a></h3><p>过去的 NLP 模型都是“任务特定”的：</p><ul><li>每个任务（如情感分析、翻译、摘要）都需要独立训练；</li><li>需要大量标注数据；</li><li>模型之间无法共享知识。</li></ul><hr><h3 id="🔹-二、预训练—微调-pretrain-finetune-范式" tabindex="-1"><a class="header-anchor" href="#🔹-二、预训练—微调-pretrain-finetune-范式"><span>🔹 二、预训练—微调（Pretrain-Finetune）范式</span></a></h3><p>BERT 带来的最大变革，就是<strong>统一训练流程</strong>：</p><ol><li><p><strong>预训练（Pretraining）</strong></p><ul><li>在大规模无标签语料上，学习语言通用规律；</li><li>模型掌握“语言常识”和“语义知识”。</li></ul></li><li><p><strong>微调（Finetuning）</strong></p><ul><li>在下游任务的小数据集上，调整模型参数；</li><li>学会特定任务的模式。</li></ul></li></ol><p>这种方法极大地提升了模型的<strong>泛化能力</strong>与<strong>数据效率</strong>。</p><hr><h3 id="🔹-三、进一步的演化" tabindex="-1"><a class="header-anchor" href="#🔹-三、进一步的演化"><span>🔹 三、进一步的演化</span></a></h3><ul><li><strong>GPT 系列（OpenAI）</strong>：使用“自回归”结构，只预测下一个词；</li><li><strong>BART、T5（Google）</strong>：采用“Encoder-Decoder”架构，支持生成任务；</li><li><strong>RoBERTa、ELECTRA</strong>：改进预训练任务与数据规模。</li></ul><hr><h2 id="_18-3-大语言模型-llm-的架构与原理" tabindex="-1"><a class="header-anchor" href="#_18-3-大语言模型-llm-的架构与原理"><span><strong>18.3 大语言模型（LLM）的架构与原理</strong></span></a></h2><h3 id="🔹-一、从预训练到大规模预训练" tabindex="-1"><a class="header-anchor" href="#🔹-一、从预训练到大规模预训练"><span>🔹 一、从预训练到大规模预训练</span></a></h3><p>当模型参数从百万级增长到<strong>百亿、千亿</strong>级时，出现了“涌现能力（Emergent Abilities）”：</p><ul><li>能理解复杂语义；</li><li>会生成连贯、上下文一致的文本；</li><li>甚至具备逻辑推理与编程能力。</li></ul><hr><h3 id="🔹-二、llm-的架构核心-transformer-decoder" tabindex="-1"><a class="header-anchor" href="#🔹-二、llm-的架构核心-transformer-decoder"><span>🔹 二、LLM 的架构核心：Transformer Decoder</span></a></h3><p>大语言模型（如 GPT 系列）通常只使用 Transformer 的 <strong>Decoder</strong> 部分：</p><ul><li><strong>自回归机制</strong>：预测下一个 token；</li><li><strong>多层堆叠的注意力网络</strong>；</li><li><strong>大规模并行训练（分布式）</strong>；</li><li><strong>位置编码 + LayerNorm + 残差连接</strong> 提高稳定性。</li></ul><p>核心目标函数：<br> [<br> \\text{maximize } P(w_t | w_1, w_2, ..., w_{t-1})<br> ]</p><hr><h3 id="🔹-三、训练要素" tabindex="-1"><a class="header-anchor" href="#🔹-三、训练要素"><span>🔹 三、训练要素</span></a></h3><ul><li><strong>数据规模</strong>：数千亿 tokens；</li><li><strong>参数规模</strong>：从 1B → 175B（GPT-3）→ 万亿（GPT-4）；</li><li><strong>硬件支持</strong>：GPU/TPU 集群、张量并行、模型并行；</li><li><strong>优化方法</strong>：AdamW、混合精度训练、梯度检查点、ZeRO。</li></ul><hr><h3 id="🔹-四、涌现能力与知识涌现" tabindex="-1"><a class="header-anchor" href="#🔹-四、涌现能力与知识涌现"><span>🔹 四、涌现能力与知识涌现</span></a></h3><p>随着模型规模扩大，出现了人类未显式设计的“智能特性”：</p><ul><li>上下文学习（In-context learning）；</li><li>零样本 / 少样本推理；</li><li>逻辑推断；</li><li>多模态融合（语言 + 图像 + 音频）。</li></ul><p>这些特性让 LLM 从“语言模型”逐渐进化为“通用智能系统”的雏形。</p><hr><h2 id="_18-4-从-gpt-到-agent-智能体的出现" tabindex="-1"><a class="header-anchor" href="#_18-4-从-gpt-到-agent-智能体的出现"><span><strong>18.4 从 GPT 到 Agent：智能体的出现</strong></span></a></h2><h3 id="🔹-一、llm-不再只是-说话的模型" tabindex="-1"><a class="header-anchor" href="#🔹-一、llm-不再只是-说话的模型"><span>🔹 一、LLM 不再只是“说话的模型”</span></a></h3><p>当大模型具备理解、推理、规划、调用工具的能力时，它就开始具备“<strong>智能体（Agent）</strong>”的特征。</p><p>智能体不只是回答问题，而是能<strong>主动行动</strong>、<strong>调用外部工具</strong>、<strong>与环境交互</strong>。</p><hr><h3 id="🔹-二、从-gpt-到-chatgpt" tabindex="-1"><a class="header-anchor" href="#🔹-二、从-gpt-到-chatgpt"><span>🔹 二、从 GPT 到 ChatGPT</span></a></h3><ul><li><strong>GPT-2 → GPT-3</strong>：实现强大的语言生成；</li><li><strong>InstructGPT（2022）</strong>：引入人类反馈强化学习（RLHF），学会“听懂指令”；</li><li><strong>ChatGPT（2022）</strong>：对话式交互 + 工具调用；</li><li><strong>GPT-4（2023）</strong>：多模态输入 + 推理增强。</li></ul><hr><h3 id="🔹-三、agent-化的关键特征" tabindex="-1"><a class="header-anchor" href="#🔹-三、agent-化的关键特征"><span>🔹 三、Agent 化的关键特征</span></a></h3><table><thead><tr><th>能力</th><th>说明</th><th>示例</th></tr></thead><tbody><tr><td>目标驱动</td><td>能基于用户目标规划任务</td><td>“帮我预订东京的行程”</td></tr><tr><td>工具使用</td><td>能调用外部API/数据库</td><td>浏览器、代码执行器、SQL查询</td></tr><tr><td>记忆机制</td><td>具备长期与短期记忆</td><td>回忆过去对话或上下文</td></tr><tr><td>推理与规划</td><td>多步思考与自我纠错</td><td>Chain-of-Thought, ReAct</td></tr><tr><td>自主行动</td><td>能执行决策并返回结果</td><td>AutoGPT, LangChain Agent</td></tr></tbody></table><hr><h3 id="🔹-四、智能体生态" tabindex="-1"><a class="header-anchor" href="#🔹-四、智能体生态"><span>🔹 四、智能体生态</span></a></h3><ul><li><strong>LangChain / AutoGPT / BabyAGI</strong>：构建多步骤任务代理；</li><li><strong>OpenAI GPTs / Assistants API</strong>：让 LLM 化身为个人助理；</li><li><strong>Toolformer / ReAct</strong>：融合推理与工具使用；</li><li><strong>AI OS 概念</strong>：未来将是以 Agent 为核心的操作系统时代。</li></ul><hr><h3 id="🔹-五、从大模型到通用智能-agi-的路径" tabindex="-1"><a class="header-anchor" href="#🔹-五、从大模型到通用智能-agi-的路径"><span>🔹 五、从大模型到通用智能（AGI）的路径</span></a></h3><blockquote><p>机器学习 → 深度学习 → Transformer → 大模型 → 智能体 → <strong>通用人工智能（AGI）</strong></p></blockquote><p>大模型是“智能的核心引擎”，<br> 智能体是“智能的外在形态”，<br> 两者结合构成了未来 AI 的操作基座。</p><hr><h2 id="📘-小结" tabindex="-1"><a class="header-anchor" href="#📘-小结"><span>📘 小结</span></a></h2><table><thead><tr><th>阶段</th><th>关键技术</th><th>代表模型</th><th>特征</th><th>意义</th></tr></thead><tbody><tr><td>词向量</td><td>分布式语义</td><td>Word2Vec / GloVe</td><td>静态语义表示</td><td>语义理解的起点</td></tr><tr><td>上下文预训练</td><td>双向建模</td><td>ELMo / BERT</td><td>动态语义表示</td><td>NLP 通用模型诞生</td></tr><tr><td>自回归语言模型</td><td>Transformer Decoder</td><td>GPT 系列</td><td>生成式语言建模</td><td>通用生成能力</td></tr><tr><td>智能体（Agent）</td><td>LLM + 工具 + 记忆 + 推理</td><td>AutoGPT / LangChain</td><td>主动智能</td><td>向 AGI 迈进</td></tr></tbody></table>',83)])])}const h=r(a,[["render",s]]),p=JSON.parse('{"path":"/posts/ml/2025-11-03-18-ml-book-LLM.html","title":"第18章　大模型与预训练范式","lang":"zh-CN","frontmatter":{"title":"第18章　大模型与预训练范式","date":"2025-11-03T00:00:00.000Z","categories":["AI"],"tags":["ai","learn-note"],"published":true,"description":"第18章是整本书的“现代 AI 篇”的核心部分——它讲述了从「词向量」到「BERT」，再到「GPT 与智能体（Agent）」的整条技术演化主线。 这一章不仅是机器学习与深度学习的融合点，更是当代人工智能（尤其是大语言模型）的技术根基。 第18章 大模型与预训练范式 18.1 从词向量到 BERT 🔹 一、传统NLP的局限 在早期的自然语言处理中，机器...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"第18章　大模型与预训练范式\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-11-03T00:00:00.000Z\\",\\"dateModified\\":\\"2025-12-27T05:15:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"老马啸西风\\",\\"url\\":\\"https://houbb.github.io\\"}]}"],["meta",{"property":"og:url","content":"https://houbb.github.io/awesome-ai-coding/posts/ml/2025-11-03-18-ml-book-LLM.html"}],["meta",{"property":"og:site_name","content":"老马啸西风"}],["meta",{"property":"og:title","content":"第18章　大模型与预训练范式"}],["meta",{"property":"og:description","content":"第18章是整本书的“现代 AI 篇”的核心部分——它讲述了从「词向量」到「BERT」，再到「GPT 与智能体（Agent）」的整条技术演化主线。 这一章不仅是机器学习与深度学习的融合点，更是当代人工智能（尤其是大语言模型）的技术根基。 第18章 大模型与预训练范式 18.1 从词向量到 BERT 🔹 一、传统NLP的局限 在早期的自然语言处理中，机器..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-27T05:15:15.000Z"}],["meta",{"property":"article:tag","content":"learn-note"}],["meta",{"property":"article:tag","content":"ai"}],["meta",{"property":"article:published_time","content":"2025-11-03T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-27T05:15:15.000Z"}]]},"git":{"createdTime":1766812269000,"updatedTime":1766812515000,"contributors":[{"name":"bbhou","username":"bbhou","email":"1557740299@qq.com","commits":3,"url":"https://github.com/bbhou"}]},"readingTime":{"minutes":5.87,"words":1760},"filePathRelative":"posts/ml/2025-11-03-18-ml-book-LLM.md","excerpt":"<p>第18章是整本书的“现代 AI 篇”的核心部分——它讲述了从「词向量」到「BERT」，再到「GPT 与智能体（Agent）」的整条技术演化主线。</p>\\n<p>这一章不仅是机器学习与深度学习的融合点，更是当代人工智能（尤其是大语言模型）的技术根基。</p>\\n<hr>\\n<h1>第18章　大模型与预训练范式</h1>\\n<hr>\\n<h2><strong>18.1 从词向量到 BERT</strong></h2>\\n<h3>🔹 一、传统NLP的局限</h3>\\n<p>在早期的自然语言处理中，机器学习模型（如SVM、朴素贝叶斯）使用的特征往往是：</p>\\n<ul>\\n<li>词袋模型（Bag of Words）</li>\\n<li>TF-IDF 向量<br>\\n它们<strong>忽略了词序</strong>，也无法捕捉语义。例如：</li>\\n</ul>","autoDesc":true}');export{h as comp,p as data};
