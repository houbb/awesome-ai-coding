import{_ as r}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as a,a as n,o}from"./app-BCq4GIDA.js";const s={};function i(e,t){return o(),a("div",null,[...t[0]||(t[0]=[n('<h2 id="第8章-决策树与集成学习" tabindex="-1"><a class="header-anchor" href="#第8章-决策树与集成学习"><span><strong>第8章　决策树与集成学习</strong></span></a></h2><hr><h3 id="_8-1-决策树构建原理-id3、c4-5、cart" tabindex="-1"><a class="header-anchor" href="#_8-1-决策树构建原理-id3、c4-5、cart"><span><strong>8.1 决策树构建原理（ID3、C4.5、CART）</strong></span></a></h3><h4 id="_1-核心思想" tabindex="-1"><a class="header-anchor" href="#_1-核心思想"><span><strong>（1）核心思想</strong></span></a></h4><p>决策树（Decision Tree）是一种 <strong>基于规则的监督学习方法</strong>。</p><p>它通过在特征空间中 <strong>逐步划分样本</strong>，最终形成一个 <strong>树状结构</strong> 来进行分类或回归。</p><p>每个内部节点表示一个“特征判定条件”，每个叶子节点对应一个“类别”或“预测值”。</p><p><strong>直观理解</strong>：<br> 就像人类判断事物一样，“如果温度高→再看湿度→如果湿度低→晴天”，形成一棵“判断树”。</p><h4 id="_2-构建流程" tabindex="-1"><a class="header-anchor" href="#_2-构建流程"><span><strong>（2）构建流程</strong></span></a></h4><ol><li>从数据集中选择一个最优特征；</li><li>根据该特征的不同取值划分数据；</li><li>对每个子集递归构建子树；</li><li>直到满足停止条件（如样本纯度高、深度限制等）。</li></ol><hr><h4 id="_3-id3-算法" tabindex="-1"><a class="header-anchor" href="#_3-id3-算法"><span><strong>（3）ID3 算法</strong></span></a></h4><ul><li><p><strong>核心指标</strong>：信息增益（Information Gain）<br> 用信息论的角度衡量“某个特征对分类的不确定性减少程度”。</p><p>[<br> IG(D, A) = H(D) - H(D|A)<br> ]<br> 其中 (H(D)) 为数据集的熵，(H(D|A)) 为按特征 A 划分后的条件熵。</p></li><li><p><strong>问题</strong>：ID3 只能处理离散特征，对连续特征和缺失值支持差。</p></li></ul><hr><h4 id="_4-c4-5-算法" tabindex="-1"><a class="header-anchor" href="#_4-c4-5-算法"><span><strong>（4）C4.5 算法</strong></span></a></h4><ul><li><p><strong>改进点</strong>：</p><ul><li>支持连续特征；</li><li>使用“信息增益率”（Information Gain Ratio）；</li><li>支持缺失值；</li><li>使用剪枝（Pruning）防止过拟合。</li></ul></li><li><p><strong>信息增益率定义</strong>：<br> [<br> GainRatio(D, A) = \\frac{IG(D, A)}{H_A(D)}<br> ]<br> 其中 (H_A(D)) 为特征 A 的“分裂信息熵”。</p></li></ul><hr><h4 id="_5-cart-算法" tabindex="-1"><a class="header-anchor" href="#_5-cart-算法"><span><strong>（5）CART 算法</strong></span></a></h4><ul><li><p><strong>Classification And Regression Tree</strong>：既可用于分类，也可用于回归。</p></li><li><p><strong>核心指标</strong>：</p><ul><li>分类任务：基尼指数（Gini Index）；</li><li>回归任务：均方误差（MSE）。</li></ul></li><li><p><strong>特点</strong>：</p><ul><li>使用二叉树结构；</li><li>连续特征按阈值二分；</li><li>剪枝方法采用 <strong>代价复杂度剪枝（Cost-Complexity Pruning）</strong>。</li></ul></li></ul><hr><h4 id="_6-优缺点" tabindex="-1"><a class="header-anchor" href="#_6-优缺点"><span><strong>（6）优缺点</strong></span></a></h4><table><thead><tr><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>直观易解释，可视化友好</td><td>容易过拟合</td></tr><tr><td>可处理非线性关系</td><td>对数据噪声敏感</td></tr><tr><td>可处理离散与连续特征</td><td>树不稳定（小数据扰动可能导致结构大变）</td></tr></tbody></table><hr><h3 id="_8-2-随机森林与特征重要性" tabindex="-1"><a class="header-anchor" href="#_8-2-随机森林与特征重要性"><span><strong>8.2 随机森林与特征重要性</strong></span></a></h3><h4 id="_1-随机森林-random-forest-简介" tabindex="-1"><a class="header-anchor" href="#_1-随机森林-random-forest-简介"><span><strong>（1）随机森林（Random Forest）简介</strong></span></a></h4><p>随机森林是 <strong>Bagging（自助采样法）+ 决策树</strong> 的集成模型。<br> 它通过构建多棵互相独立的树，再让它们进行投票或平均预测，从而提高泛化性能。</p><h4 id="_2-核心机制" tabindex="-1"><a class="header-anchor" href="#_2-核心机制"><span><strong>（2）核心机制</strong></span></a></h4><ul><li><strong>样本随机性</strong>：每棵树使用不同的 Bootstrap 样本；</li><li><strong>特征随机性</strong>：每个节点分裂时随机选择部分特征；</li><li><strong>集成预测</strong>：分类取多数投票，回归取平均值。</li></ul><h4 id="_3-优点" tabindex="-1"><a class="header-anchor" href="#_3-优点"><span><strong>（3）优点</strong></span></a></h4><ul><li>大幅降低过拟合；</li><li>对高维数据、缺失数据、离群点鲁棒；</li><li>可计算“特征重要性”。</li></ul><h4 id="_4-特征重要性计算方法" tabindex="-1"><a class="header-anchor" href="#_4-特征重要性计算方法"><span><strong>（4）特征重要性计算方法</strong></span></a></h4><ul><li><strong>基于分裂增益</strong>：统计特征在所有树中带来的信息增益之和；</li><li><strong>基于置换（Permutation Importance）</strong>：打乱某个特征的取值，看模型性能下降幅度。</li></ul><h4 id="_5-典型应用" tabindex="-1"><a class="header-anchor" href="#_5-典型应用"><span><strong>（5）典型应用</strong></span></a></h4><p>随机森林是工业界最实用、最稳健的机器学习模型之一，常用于：</p><ul><li>信贷风险评估；</li><li>医学诊断；</li><li>客户流失预测；</li><li>特征筛选。</li></ul><hr><h3 id="_8-3-boosting、bagging、xgboost、lightgbm" tabindex="-1"><a class="header-anchor" href="#_8-3-boosting、bagging、xgboost、lightgbm"><span><strong>8.3 Boosting、Bagging、XGBoost、LightGBM</strong></span></a></h3><h4 id="_1-bagging-与-boosting-的区别" tabindex="-1"><a class="header-anchor" href="#_1-bagging-与-boosting-的区别"><span><strong>（1）Bagging 与 Boosting 的区别</strong></span></a></h4><table><thead><tr><th>对比维度</th><th>Bagging</th><th>Boosting</th></tr></thead><tbody><tr><td>训练方式</td><td>并行</td><td>串行（逐步改进）</td></tr><tr><td>目标</td><td>降低方差</td><td>降低偏差</td></tr><tr><td>代表算法</td><td>随机森林</td><td>AdaBoost、XGBoost、LightGBM</td></tr></tbody></table><hr><h4 id="_2-adaboost-adaptive-boosting" tabindex="-1"><a class="header-anchor" href="#_2-adaboost-adaptive-boosting"><span><strong>（2）AdaBoost（Adaptive Boosting）</strong></span></a></h4><ul><li>每一轮训练一个弱分类器（如浅层决策树）；</li><li>赋予被错分样本更高的权重；</li><li>最终用加权投票融合所有分类器。</li></ul><p><strong>思想核心</strong>：让模型聚焦于“前一轮没学好的样本”。</p><hr><h4 id="_3-xgboost-extreme-gradient-boosting" tabindex="-1"><a class="header-anchor" href="#_3-xgboost-extreme-gradient-boosting"><span><strong>（3）XGBoost（Extreme Gradient Boosting）</strong></span></a></h4><ul><li>基于梯度提升（Gradient Boosting）的高效实现；</li><li>引入 <strong>二阶导信息</strong>，更快收敛；</li><li>具有 <strong>剪枝、正则化、列抽样、并行化</strong> 等优化。</li></ul><p><strong>目标函数：</strong><br> [<br> Obj = \\sum_i l(y_i, \\hat{y}_i) + \\sum_k \\Omega(f_k)<br> ]<br> 其中 (\\Omega(f_k)) 是树的复杂度正则项。</p><hr><h4 id="_4-lightgbm" tabindex="-1"><a class="header-anchor" href="#_4-lightgbm"><span><strong>（4）LightGBM</strong></span></a></h4><ul><li>微软开源，进一步优化 XGBoost 的速度和内存占用；</li><li>采用 <strong>Histogram-based 分裂法</strong>（将连续特征分桶）；</li><li>使用 <strong>Leaf-wise 策略</strong>（更深的生长方向）；</li><li>对大规模、高维数据表现优异。</li></ul><hr><h4 id="_5-catboost" tabindex="-1"><a class="header-anchor" href="#_5-catboost"><span><strong>（5）CatBoost</strong></span></a></h4><ul><li>Yandex 提出的 Boosting 框架；</li><li>针对分类特征进行了特别优化；</li><li>使用对称树结构（balanced tree），训练更稳定；</li><li>避免了“目标泄漏”（target leakage）。</li></ul><hr><h3 id="_8-4-集成方法的偏差-方差机制分析" tabindex="-1"><a class="header-anchor" href="#_8-4-集成方法的偏差-方差机制分析"><span><strong>8.4 集成方法的偏差-方差机制分析</strong></span></a></h3><h4 id="_1-偏差-方差分解回顾" tabindex="-1"><a class="header-anchor" href="#_1-偏差-方差分解回顾"><span><strong>（1）偏差-方差分解回顾</strong></span></a></h4><p>模型误差通常可分解为三部分：<br> [<br> E_{total} = Bias^2 + Variance + Noise<br> ]</p><ul><li><strong>偏差（Bias）</strong>：模型拟合能力不足；</li><li><strong>方差（Variance）</strong>：模型对样本扰动敏感；</li><li><strong>噪声（Noise）</strong>：数据中不可消除的随机误差。</li></ul><h4 id="_2-bagging-的作用" tabindex="-1"><a class="header-anchor" href="#_2-bagging-的作用"><span><strong>（2）Bagging 的作用</strong></span></a></h4><ul><li>降低方差；</li><li>提高稳定性；</li><li>对高方差模型（如决策树）尤其有效。</li></ul><h4 id="_3-boosting-的作用" tabindex="-1"><a class="header-anchor" href="#_3-boosting-的作用"><span><strong>（3）Boosting 的作用</strong></span></a></h4><ul><li>降低偏差；</li><li>提升拟合能力；</li><li>对弱学习器组合后可形成强模型。</li></ul><h4 id="_4-集成学习的本质" tabindex="-1"><a class="header-anchor" href="#_4-集成学习的本质"><span><strong>（4）集成学习的本质</strong></span></a></h4><blockquote><p>“弱者联合可以成为强者”——通过组合多个不完美模型，形成一个强大且泛化良好的整体。</p></blockquote><p>这正是现代机器学习中“模型融合（Model Ensemble）”与“多专家协作（Mixture of Experts）”思想的理论根基。</p><hr><h3 id="✅-总结" tabindex="-1"><a class="header-anchor" href="#✅-总结"><span>✅ <strong>总结</strong></span></a></h3><table><thead><tr><th>方法</th><th>特点</th><th>优点</th><th>缺点</th><th>应用场景</th></tr></thead><tbody><tr><td>决策树</td><td>规则可解释</td><td>可视化、易解释</td><td>容易过拟合</td><td>特征分析、教学</td></tr><tr><td>随机森林</td><td>Bagging 集成</td><td>稳定、高精度</td><td>模型大、解释性弱</td><td>工业生产模型</td></tr><tr><td>AdaBoost</td><td>串行提升</td><td>精度高</td><td>噪声敏感</td><td>小样本分类</td></tr><tr><td>XGBoost</td><td>二阶提升+正则化</td><td>高性能、稳定</td><td>参数多</td><td>Kaggle 常胜算法</td></tr><tr><td>LightGBM</td><td>基于直方图</td><td>速度快、资源低</td><td>可解释性弱</td><td>大数据任务</td></tr></tbody></table>',68)])])}const g=r(s,[["render",i]]),h=JSON.parse('{"path":"/posts/ml/2025-11-03-08-ml-book-decision-tree.html","title":"第8章　决策树与集成学习","lang":"zh-CN","frontmatter":{"title":"第8章　决策树与集成学习","date":"2025-11-03T00:00:00.000Z","categories":["AI"],"tags":["ai","learn-note"],"published":true,"description":"第8章 决策树与集成学习 8.1 决策树构建原理（ID3、C4.5、CART） （1）核心思想 决策树（Decision Tree）是一种 基于规则的监督学习方法。 它通过在特征空间中 逐步划分样本，最终形成一个 树状结构 来进行分类或回归。 每个内部节点表示一个“特征判定条件”，每个叶子节点对应一个“类别”或“预测值”。 直观理解： 就像人类判断事物...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"第8章　决策树与集成学习\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-11-03T00:00:00.000Z\\",\\"dateModified\\":\\"2025-12-27T05:15:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"老马啸西风\\",\\"url\\":\\"https://houbb.github.io\\"}]}"],["meta",{"property":"og:url","content":"https://houbb.github.io/awesome-ai-coding/posts/ml/2025-11-03-08-ml-book-decision-tree.html"}],["meta",{"property":"og:site_name","content":"老马啸西风"}],["meta",{"property":"og:title","content":"第8章　决策树与集成学习"}],["meta",{"property":"og:description","content":"第8章 决策树与集成学习 8.1 决策树构建原理（ID3、C4.5、CART） （1）核心思想 决策树（Decision Tree）是一种 基于规则的监督学习方法。 它通过在特征空间中 逐步划分样本，最终形成一个 树状结构 来进行分类或回归。 每个内部节点表示一个“特征判定条件”，每个叶子节点对应一个“类别”或“预测值”。 直观理解： 就像人类判断事物..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-27T05:15:15.000Z"}],["meta",{"property":"article:tag","content":"learn-note"}],["meta",{"property":"article:tag","content":"ai"}],["meta",{"property":"article:published_time","content":"2025-11-03T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-27T05:15:15.000Z"}]]},"git":{"createdTime":1766812269000,"updatedTime":1766812515000,"contributors":[{"name":"bbhou","username":"bbhou","email":"1557740299@qq.com","commits":3,"url":"https://github.com/bbhou"}]},"readingTime":{"minutes":4.81,"words":1444},"filePathRelative":"posts/ml/2025-11-03-08-ml-book-decision-tree.md","excerpt":"<h2><strong>第8章　决策树与集成学习</strong></h2>\\n<hr>\\n<h3><strong>8.1 决策树构建原理（ID3、C4.5、CART）</strong></h3>\\n<h4><strong>（1）核心思想</strong></h4>\\n<p>决策树（Decision Tree）是一种 <strong>基于规则的监督学习方法</strong>。</p>\\n<p>它通过在特征空间中 <strong>逐步划分样本</strong>，最终形成一个 <strong>树状结构</strong> 来进行分类或回归。</p>\\n<p>每个内部节点表示一个“特征判定条件”，每个叶子节点对应一个“类别”或“预测值”。</p>","autoDesc":true}');export{g as comp,h as data};
