import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as d,a as e,b as t,e as r,o as l}from"./app-CIr_XAMB.js";const s={};function o(i,a){return l(),d("div",null,[...a[0]||(a[0]=[e('<h2 id="第7章-基于距离与相似度的方法" tabindex="-1"><a class="header-anchor" href="#第7章-基于距离与相似度的方法"><span>第7章　基于距离与相似度的方法</span></a></h2><p>在机器学习的世界里，有一类算法不依赖复杂的参数学习过程，而是通过“比较样本间的相似性”来进行预测。<br> 这种思想源于人类的直觉学习方式——我们看到一个新事物时，会下意识地问：</p><blockquote><p>“它像不像我见过的某个东西？”</p></blockquote><p>这类方法的代表是 <strong>k-近邻算法（K-Nearest Neighbors, KNN）</strong>。<br> 它是“基于实例”的学习（Instance-based Learning），<br> 也是“惰性学习”（Lazy Learning）的典型代表。</p><hr><h3 id="_7-1-k-近邻算法-knn" tabindex="-1"><a class="header-anchor" href="#_7-1-k-近邻算法-knn"><span><strong>7.1 k-近邻算法（KNN）</strong></span></a></h3><h4 id="✅-一、算法思想" tabindex="-1"><a class="header-anchor" href="#✅-一、算法思想"><span>✅ 一、算法思想</span></a></h4><p>KNN 的核心思想非常简单：</p><blockquote><p>如果一个样本在特征空间中与某类样本“很接近”，<br> 那它也很可能属于这一类。</p></blockquote><p>KNN 不需要显式训练模型，而是在预测时直接利用整个训练数据。</p><h4 id="✅-二、算法流程" tabindex="-1"><a class="header-anchor" href="#✅-二、算法流程"><span>✅ 二、算法流程</span></a></h4><p>1️⃣ <strong>计算距离</strong>：<br> 对测试样本 (x)，计算它与训练集中所有样本 (x_i) 的距离。</p><p>2️⃣ <strong>选择邻居</strong>：<br> 选取距离最近的 (k) 个样本。</p><p>3️⃣ <strong>投票或加权平均</strong>：</p><ul><li>分类任务：多数投票法决定类别。</li><li>回归任务：取邻居标签的平均值（或加权平均）。</li></ul>',15),t("p",null,[r("["),t("br"),r(" \\hat{y} ="),t("br"),r(" \\begin{cases}"),t("br"),r(" \\text{mode}({y_i}"),t("em",{i:"1"},[r("{i=1}^k), & \\text{分类} "),t("br"),r(" \\frac{1}{k} \\sum")]),r("^k y_i, & \\text{回归}"),t("br"),r(" \\end{cases}"),t("br"),r(" ]")],-1),t("h4",{id:"✅-三、常见距离度量",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#✅-三、常见距离度量"},[t("span",null,"✅ 三、常见距离度量")])],-1),t("table",null,[t("thead",null,[t("tr",null,[t("th",null,"距离度量"),t("th",null,"数学形式"),t("th",null,"适用场景"),t("th"),t("th"),t("th"),t("th"),t("th"),t("th"),t("th"),t("th")])]),t("tbody",null,[t("tr",null,[t("td",null,"欧式距离"),t("td",null,"( d(x,y) = \\sqrt{\\sum_i (x_i - y_i)^2} )"),t("td",null,"连续特征常用"),t("td"),t("td"),t("td"),t("td"),t("td"),t("td"),t("td"),t("td")]),t("tr",null,[t("td",null,"曼哈顿距离"),t("td",null,"( d(x,y) = \\sum_i"),t("td",null,"x_i - y_i"),t("td",null,")"),t("td",null,"稀疏数据或L1空间"),t("td"),t("td"),t("td"),t("td"),t("td"),t("td")]),t("tr",null,[t("td",null,"明可夫斯基距离"),t("td",null,"( d(x,y) = (\\sum_i"),t("td",null,"x_i - y_i"),t("td",null,[t("sup",{"1p":""},"p)"),r(" )")]),t("td",null,"通用形式"),t("td"),t("td"),t("td"),t("td"),t("td"),t("td")]),t("tr",null,[t("td",null,"余弦相似度"),t("td",null,"( \\cos(\\theta) = \\frac{x \\cdot y}{"),t("td"),t("td",null,"x"),t("td"),t("td",null,","),t("td"),t("td",null,"y"),t("td"),t("td",null,"} )"),t("td",null,"文本或高维方向性特征")]),t("tr",null,[t("td",null,"汉明距离"),t("td",null,"不同位数目"),t("td",null,"离散/二进制特征"),t("td"),t("td"),t("td"),t("td"),t("td"),t("td"),t("td"),t("td")])])],-1),e('<hr><h4 id="✅-四、k-的选择与模型表现" tabindex="-1"><a class="header-anchor" href="#✅-四、k-的选择与模型表现"><span>✅ 四、k 的选择与模型表现</span></a></h4><ul><li><strong>k 太小</strong> → 对噪声敏感，容易过拟合。</li><li><strong>k 太大</strong> → 模型过于平滑，可能欠拟合。</li></ul><p>通常通过交叉验证（Cross-Validation）选取最优的 (k)。</p><h4 id="✅-五、加权-knn" tabindex="-1"><a class="header-anchor" href="#✅-五、加权-knn"><span>✅ 五、加权 KNN</span></a></h4><p>不是所有邻居都一样重要——越近的样本越可信。<br> 因此可以按距离加权：</p><p>[<br> \\hat{y} = \\frac{\\sum_{i=1}^k \\frac{1}{d(x,x_i)} y_i}{\\sum_{i=1}^k \\frac{1}{d(x,x_i)}}<br> ]</p><p>这种方法称为 <strong>加权KNN（Weighted KNN）</strong>，在噪声数据上表现更稳健。</p><h4 id="✅-六、优缺点总结" tabindex="-1"><a class="header-anchor" href="#✅-六、优缺点总结"><span>✅ 六、优缺点总结</span></a></h4><table><thead><tr><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>原理简单，无需训练</td><td>推理开销大（O(N)）</td></tr><tr><td>可用于分类与回归</td><td>对特征缩放敏感</td></tr><tr><td>可适应复杂边界</td><td>对噪声敏感</td></tr><tr><td>仅需样本间距离定义</td><td>难以处理高维数据（维度灾难）</td></tr></tbody></table><hr><h3 id="_7-2-相似度度量与特征归一化" tabindex="-1"><a class="header-anchor" href="#_7-2-相似度度量与特征归一化"><span><strong>7.2 相似度度量与特征归一化</strong></span></a></h3><p>KNN 的性能极度依赖于“距离度量”的定义。<br> 换句话说：<strong>如何衡量“像不像”</strong> 是算法的灵魂。</p><h4 id="✅-一、特征尺度问题" tabindex="-1"><a class="header-anchor" href="#✅-一、特征尺度问题"><span>✅ 一、特征尺度问题</span></a></h4><p>若特征量纲不同，例如：</p><ul><li>年龄范围是 0–100，</li><li>收入范围是 0–100000，</li></ul><p>则收入特征会主导距离计算，导致不公平。</p><h4 id="✅-二、常见的归一化方法" tabindex="-1"><a class="header-anchor" href="#✅-二、常见的归一化方法"><span>✅ 二、常见的归一化方法</span></a></h4><table><thead><tr><th>方法</th><th>公式</th><th>说明</th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td><strong>Min-Max 归一化</strong></td><td>( x&#39; = \\frac{x - x_{min}}{x_{max} - x_{min}} )</td><td>将特征缩放至 [0,1]</td><td></td><td></td><td></td><td></td></tr><tr><td><strong>Z-score 标准化</strong></td><td>( x&#39; = \\frac{x - \\mu}{\\sigma} )</td><td>均值为0，方差为1</td><td></td><td></td><td></td><td></td></tr><tr><td><strong>L2 归一化</strong></td><td>( x&#39; = \\frac{x}{</td><td></td><td>x</td><td></td><td>_2} )</td><td>保留方向信息</td></tr></tbody></table><h4 id="✅-三、特征加权" tabindex="-1"><a class="header-anchor" href="#✅-三、特征加权"><span>✅ 三、特征加权</span></a></h4><p>在某些场景中，特征的重要性不同，可以赋予不同权重：</p><p>[<br> d(x, y) = \\sqrt{\\sum_i w_i (x_i - y_i)^2}<br> ]</p><p>特征权重 (w_i) 可通过经验、统计分析或自动学习获得。</p><h4 id="✅-四、高维数据与相似度退化" tabindex="-1"><a class="header-anchor" href="#✅-四、高维数据与相似度退化"><span>✅ 四、高维数据与相似度退化</span></a></h4><p>当维度过高时：</p><ul><li>所有样本的距离趋于相同（“维度灾难”）；</li><li>相似度概念逐渐失效。</li></ul><p>常用解决方案：</p><ul><li><strong>特征选择</strong>（去冗余）；</li><li><strong>降维方法</strong>（如 PCA、t-SNE）；</li><li><strong>局部敏感哈希（LSH）</strong>：近似搜索加速。</li></ul><hr><h3 id="_7-3-实例学习与惰性学习思想" tabindex="-1"><a class="header-anchor" href="#_7-3-实例学习与惰性学习思想"><span><strong>7.3 实例学习与惰性学习思想</strong></span></a></h3><h4 id="✅-一、与参数学习的对比" tabindex="-1"><a class="header-anchor" href="#✅-一、与参数学习的对比"><span>✅ 一、与参数学习的对比</span></a></h4><p>传统机器学习（如线性回归、SVM）属于 <strong>“参数学习”（Parametric Learning）</strong>：<br> 通过训练学习参数 (w)，再用 (w) 进行预测。</p><p>而 KNN 属于 <strong>“实例学习”（Instance-based Learning）</strong>：<br> 它<strong>不学习显式参数</strong>，直接在样本间比较。</p><table><thead><tr><th>对比项</th><th>参数学习</th><th>实例学习</th></tr></thead><tbody><tr><td>学习目标</td><td>参数</td><td>样本之间的关系</td></tr><tr><td>训练阶段</td><td>显式建模</td><td>无需训练</td></tr><tr><td>推理阶段</td><td>快速</td><td>需大量计算</td></tr><tr><td>优化目标</td><td>最小化损失函数</td><td>基于距离度量</td></tr><tr><td>举例</td><td>逻辑回归、SVM</td><td>KNN、基于记忆的推理</td></tr></tbody></table><h4 id="✅-二、惰性学习-lazy-learning" tabindex="-1"><a class="header-anchor" href="#✅-二、惰性学习-lazy-learning"><span>✅ 二、惰性学习（Lazy Learning）</span></a></h4><p>KNN 在训练阶段什么也不做，只在预测时才开始“工作”。<br> 这种模式称为 <strong>惰性学习（Lazy Learning）</strong>，对应的是：</p><blockquote><p>“急切学习（Eager Learning）”——如神经网络。</p></blockquote><p>惰性学习的特点：</p><ul><li>学习延迟到预测阶段；</li><li>适合样本分布动态变化的环境；</li><li>但预测计算量大，难以在线扩展。</li></ul><h4 id="✅-三、从-knn-到记忆增强学习" tabindex="-1"><a class="header-anchor" href="#✅-三、从-knn-到记忆增强学习"><span>✅ 三、从 KNN 到记忆增强学习</span></a></h4><p>KNN 是早期的“记忆型学习”，后来这一思想被深度学习继承并扩展，如：</p><ul><li><strong>Memory Networks（记忆网络）</strong></li><li><strong>Retrieval-Augmented Generation（检索增强生成，RAG）</strong></li><li><strong>Nearest-Neighbor Language Models</strong></li></ul><p>这些现代模型的核心仍是相似度匹配，只是通过神经网络学会“如何计算距离”。</p><hr><h3 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span><strong>总结</strong></span></a></h3><table><thead><tr><th>小节</th><th>核心思想</th><th>关键点</th></tr></thead><tbody><tr><td>7.1 KNN 算法</td><td>用邻居投票或加权平均做预测</td><td>k 值选择、距离度量</td></tr><tr><td>7.2 相似度与归一化</td><td>距离定义决定算法灵魂</td><td>归一化、特征权重、高维问题</td></tr><tr><td>7.3 实例与惰性学习</td><td>不学习参数，而是存储样本</td><td>延迟学习思想与现代衍生</td></tr></tbody></table><hr><h3 id="🌱-思考题-延伸阅读" tabindex="-1"><a class="header-anchor" href="#🌱-思考题-延伸阅读"><span>🌱 思考题 / 延伸阅读</span></a></h3><ol><li>为什么 KNN 不需要训练阶段却仍然能“学习”？</li><li>如何在百万样本上高效实现 KNN？（提示：KD-Tree、Ball-Tree、近似搜索）</li><li>如果样本分布动态变化（如推荐系统），KNN 是否更有优势？</li><li>现代大型语言模型（LLM）的“向量检索”与 KNN 有何本质联系？</li></ol>',49)])])}const c=n(s,[["render",o]]),b=JSON.parse('{"path":"/posts/ml/2025-11-03-07-ml-book-knn.html","title":"第7章　基于距离与相似度的方法","lang":"zh-CN","frontmatter":{"title":"第7章　基于距离与相似度的方法","date":"2025-11-03T00:00:00.000Z","categories":["AI"],"tags":["ai","learn-note"],"published":true,"description":"第7章 基于距离与相似度的方法 在机器学习的世界里，有一类算法不依赖复杂的参数学习过程，而是通过“比较样本间的相似性”来进行预测。 这种思想源于人类的直觉学习方式——我们看到一个新事物时，会下意识地问： “它像不像我见过的某个东西？” 这类方法的代表是 k-近邻算法（K-Nearest Neighbors, KNN）。 它是“基于实例”的学习（Inst...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"第7章　基于距离与相似度的方法\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-11-03T00:00:00.000Z\\",\\"dateModified\\":\\"2025-12-27T05:15:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"老马啸西风\\",\\"url\\":\\"https://houbb.github.io\\"}]}"],["meta",{"property":"og:url","content":"https://houbb.github.io/awesome-ai-coding/posts/ml/2025-11-03-07-ml-book-knn.html"}],["meta",{"property":"og:site_name","content":"老马啸西风"}],["meta",{"property":"og:title","content":"第7章　基于距离与相似度的方法"}],["meta",{"property":"og:description","content":"第7章 基于距离与相似度的方法 在机器学习的世界里，有一类算法不依赖复杂的参数学习过程，而是通过“比较样本间的相似性”来进行预测。 这种思想源于人类的直觉学习方式——我们看到一个新事物时，会下意识地问： “它像不像我见过的某个东西？” 这类方法的代表是 k-近邻算法（K-Nearest Neighbors, KNN）。 它是“基于实例”的学习（Inst..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-27T05:15:15.000Z"}],["meta",{"property":"article:tag","content":"learn-note"}],["meta",{"property":"article:tag","content":"ai"}],["meta",{"property":"article:published_time","content":"2025-11-03T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-27T05:15:15.000Z"}]]},"git":{"createdTime":1766812269000,"updatedTime":1766812515000,"contributors":[{"name":"bbhou","username":"bbhou","email":"1557740299@qq.com","commits":3,"url":"https://github.com/bbhou"}]},"readingTime":{"minutes":5.04,"words":1513},"filePathRelative":"posts/ml/2025-11-03-07-ml-book-knn.md","excerpt":"<h2>第7章　基于距离与相似度的方法</h2>\\n<p>在机器学习的世界里，有一类算法不依赖复杂的参数学习过程，而是通过“比较样本间的相似性”来进行预测。<br>\\n这种思想源于人类的直觉学习方式——我们看到一个新事物时，会下意识地问：</p>\\n<blockquote>\\n<p>“它像不像我见过的某个东西？”</p>\\n</blockquote>\\n<p>这类方法的代表是 <strong>k-近邻算法（K-Nearest Neighbors, KNN）</strong>。<br>\\n它是“基于实例”的学习（Instance-based Learning），<br>\\n也是“惰性学习”（Lazy Learning）的典型代表。</p>","autoDesc":true}');export{c as comp,b as data};
