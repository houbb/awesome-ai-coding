import{_ as r}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as n,a,o as s}from"./app-D4koU7iK.js";const i={};function e(o,t){return s(),n("div",null,[...t[0]||(t[0]=[a('<h2 id="第10章-无监督学习与聚类" tabindex="-1"><a class="header-anchor" href="#第10章-无监督学习与聚类"><span><strong>第10章　无监督学习与聚类</strong></span></a></h2><hr><h3 id="_10-1-k-means-与高维空间的挑战" tabindex="-1"><a class="header-anchor" href="#_10-1-k-means-与高维空间的挑战"><span><strong>10.1 K-Means 与高维空间的挑战</strong></span></a></h3><h4 id="_1-核心思想" tabindex="-1"><a class="header-anchor" href="#_1-核心思想"><span><strong>（1）核心思想</strong></span></a></h4><p>K-Means 是最经典的无监督学习算法之一。它通过 <strong>最小化簇内样本的平方误差</strong>，将数据划分为 K 个相对紧密的簇。</p><p>其优化目标函数为：<br> [<br> J = \\sum_{i=1}^{K} \\sum_{x_j \\in C_i} ||x_j - \\mu_i||^2<br> ]<br> 其中：</p><ul><li>(C_i)：第 i 个簇；</li><li>(\\mu_i)：第 i 个簇的中心（均值）。</li></ul><hr><h4 id="_2-算法流程" tabindex="-1"><a class="header-anchor" href="#_2-算法流程"><span><strong>（2）算法流程</strong></span></a></h4><ol><li>随机选取 K 个样本作为初始中心；</li><li>计算每个样本到各中心的距离，分配到最近的中心；</li><li>更新每个簇的中心为簇内样本均值；</li><li>重复步骤2-3，直到收敛（中心不再变化或误差变化很小）。</li></ol><hr><h4 id="_3-优缺点" tabindex="-1"><a class="header-anchor" href="#_3-优缺点"><span><strong>（3）优缺点</strong></span></a></h4><table><thead><tr><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>简单高效、计算快速</td><td>需要指定 K</td></tr><tr><td>收敛性好（有限步内）</td><td>依赖初始中心，易陷局部最优</td></tr><tr><td>适合球形簇</td><td>对异常值敏感</td></tr></tbody></table><hr><h4 id="_4-改进方法" tabindex="-1"><a class="header-anchor" href="#_4-改进方法"><span><strong>（4）改进方法</strong></span></a></h4><ul><li><strong>K-Means++</strong>：通过距离加权的概率选择初始中心，提升稳定性；</li><li><strong>Mini-Batch K-Means</strong>：适合大规模数据；</li><li><strong>Bisecting K-Means</strong>：层次化地分裂簇，更稳健。</li></ul><hr><h4 id="_5-高维空间的挑战-curse-of-dimensionality" tabindex="-1"><a class="header-anchor" href="#_5-高维空间的挑战-curse-of-dimensionality"><span><strong>（5）高维空间的挑战（Curse of Dimensionality）</strong></span></a></h4><p>随着维度升高：</p><ul><li>距离的差异变小 → “最近点”和“最远点”几乎等距；</li><li>数据稀疏，聚类边界模糊；</li><li>相似性度量失效。</li></ul><p><strong>解决思路</strong>：</p><ul><li>降维（PCA、AutoEncoder）；</li><li>特征选择；</li><li>归一化或使用余弦相似度。</li></ul><hr><h3 id="_10-2-层次聚类与密度聚类-dbscan" tabindex="-1"><a class="header-anchor" href="#_10-2-层次聚类与密度聚类-dbscan"><span><strong>10.2 层次聚类与密度聚类（DBSCAN）</strong></span></a></h3><h4 id="_1-层次聚类-hierarchical-clustering" tabindex="-1"><a class="header-anchor" href="#_1-层次聚类-hierarchical-clustering"><span><strong>（1）层次聚类（Hierarchical Clustering）</strong></span></a></h4><p>基于样本间的距离层层合并或拆分，形成一棵“聚类树（dendrogram）”。</p><ul><li><strong>凝聚式（自下而上）</strong>：每个样本起初是一个簇，不断合并；</li><li><strong>分裂式（自上而下）</strong>：所有样本起初是一个大簇，不断拆分。</li></ul><p>常用距离度量：</p><ul><li>单链（最短距离）</li><li>全链（最长距离）</li><li>平均距离</li><li>Ward 距离（最小化方差增加）</li></ul><p><strong>特点</strong>：</p><ul><li>无需指定簇数；</li><li>适用于可解释分析；</li><li>计算复杂度高 (O(n^3))。</li></ul><hr><h4 id="_2-密度聚类-dbscan" tabindex="-1"><a class="header-anchor" href="#_2-密度聚类-dbscan"><span><strong>（2）密度聚类（DBSCAN）</strong></span></a></h4><p>DBSCAN（Density-Based Spatial Clustering of Applications with Noise）通过“<strong>密度可达性</strong>”定义簇，而非距离阈值。</p><p><strong>核心参数：</strong></p><ul><li><code>ε (eps)</code>：邻域半径；</li><li><code>minPts</code>：形成簇的最小点数。</li></ul><p><strong>基本概念：</strong></p><ul><li><strong>核心点</strong>：邻域内样本数 ≥ minPts；</li><li><strong>边界点</strong>：邻域内样本数 &lt; minPts，但在核心点邻域内；</li><li><strong>噪声点</strong>：既非核心也非边界点。</li></ul><hr><h4 id="_3-dbscan-的优势" tabindex="-1"><a class="header-anchor" href="#_3-dbscan-的优势"><span><strong>（3）DBSCAN 的优势</strong></span></a></h4><table><thead><tr><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>可发现任意形状的簇</td><td>对参数敏感（ε, minPts）</td></tr><tr><td>自动识别噪声</td><td>难处理不同密度的簇</td></tr><tr><td>不需指定簇数</td><td>在高维数据上表现较差</td></tr></tbody></table><hr><h4 id="_4-典型应用" tabindex="-1"><a class="header-anchor" href="#_4-典型应用"><span><strong>（4）典型应用</strong></span></a></h4><ul><li>空间数据聚类（地理、图像）；</li><li>异常检测；</li><li>轨迹聚类（如用户行为路径分析）。</li></ul><hr><h3 id="_10-3-pca、lda、ica、t-sne" tabindex="-1"><a class="header-anchor" href="#_10-3-pca、lda、ica、t-sne"><span><strong>10.3 PCA、LDA、ICA、t-SNE</strong></span></a></h3><h4 id="_1-pca-主成分分析" tabindex="-1"><a class="header-anchor" href="#_1-pca-主成分分析"><span><strong>（1）PCA（主成分分析）</strong></span></a></h4><p><strong>目标</strong>：在保留数据主要方差的同时，找到最优的低维线性子空间。</p><p>数学形式：<br> [<br> \\max_W ; \\text{Var}(W^T X) \\quad s.t. ; W^T W = I<br> ]<br> 等价于求解协方差矩阵的特征向量问题。</p><p><strong>结果解释</strong>：</p><ul><li>主成分 = 特征空间中最大方差方向；</li><li>保留前几个主成分 ≈ 保留主要信息。</li></ul><p><strong>应用场景</strong>：</p><ul><li>降维与可视化；</li><li>去噪；</li><li>特征压缩（图像、文本）。</li></ul><hr><h4 id="_2-lda-线性判别分析" tabindex="-1"><a class="header-anchor" href="#_2-lda-线性判别分析"><span><strong>（2）LDA（线性判别分析）</strong></span></a></h4><p>与 PCA 不同，LDA 是 <strong>有监督的降维方法</strong>。<br> 它最大化类间距离、最小化类内距离：<br> [<br> W = \\arg\\max_W \\frac{|W^T S_B W|}{|W^T S_W W|}<br> ]<br> 其中：</p><ul><li>(S_B)：类间散度矩阵；</li><li>(S_W)：类内散度矩阵。</li></ul><p><strong>应用</strong>：人脸识别（Fisherfaces）、分类前的特征压缩。</p><hr><h4 id="_3-ica-独立成分分析" tabindex="-1"><a class="header-anchor" href="#_3-ica-独立成分分析"><span><strong>（3）ICA（独立成分分析）</strong></span></a></h4><p>目标：找到线性变换，使得输出分量之间 <strong>统计独立</strong>。</p><p>[<br> X = A S \\Rightarrow S = W X<br> ]</p><ul><li>(X)：观测信号；</li><li>(S)：独立信号；</li><li>(A)：混合矩阵。</li></ul><p><strong>应用</strong>：</p><ul><li>信号分离（盲源分离，如鸡尾酒会问题）；</li><li>图像特征提取。</li></ul><hr><h4 id="_4-t-sne-t-distributed-stochastic-neighbor-embedding" tabindex="-1"><a class="header-anchor" href="#_4-t-sne-t-distributed-stochastic-neighbor-embedding"><span><strong>（4）t-SNE（t-Distributed Stochastic Neighbor Embedding）</strong></span></a></h4><p>一种非线性降维与可视化方法，特别适合高维数据（如词向量、图像特征）。</p><p>核心思想：</p><ul><li>在高维空间中计算相似度（高斯分布）；</li><li>在低维空间中重建相似关系（t分布）；</li><li>通过最小化 KL 散度使局部结构保持一致。</li></ul><p><strong>优点</strong>：</p><ul><li>保持局部结构；</li><li>可视化高维聚类效果佳。</li></ul><p><strong>缺点</strong>：</p><ul><li>非线性、不可解释；</li><li>计算复杂；</li><li>不适合大规模数据。</li></ul><hr><h3 id="_10-4-异常检测与流形学习" tabindex="-1"><a class="header-anchor" href="#_10-4-异常检测与流形学习"><span><strong>10.4 异常检测与流形学习</strong></span></a></h3><h4 id="_1-异常检测-anomaly-detection" tabindex="-1"><a class="header-anchor" href="#_1-异常检测-anomaly-detection"><span><strong>（1）异常检测（Anomaly Detection）</strong></span></a></h4><p>目标：发现那些“不符合常规模式”的样本。</p><p><strong>典型方法：</strong></p><ul><li><strong>基于距离</strong>：KNN、LOF（局部异常因子）；</li><li><strong>基于密度</strong>：Isolation Forest；</li><li><strong>基于概率模型</strong>：高斯分布建模；</li><li><strong>基于自编码器（AutoEncoder）</strong>：学习重建误差。</li></ul><p><strong>应用场景</strong>：</p><ul><li>欺诈检测；</li><li>设备故障预测；</li><li>网络入侵检测；</li><li>医学异常筛查。</li></ul><hr><h4 id="_2-流形学习-manifold-learning" tabindex="-1"><a class="header-anchor" href="#_2-流形学习-manifold-learning"><span><strong>（2）流形学习（Manifold Learning）</strong></span></a></h4><p>核心思想：<br> 高维数据往往“嵌在”低维流形中（例如 3D 螺旋在 2D 平面上可展平）。</p><p><strong>常见方法：</strong></p><table><thead><tr><th>方法</th><th>核心思想</th><th>特点</th></tr></thead><tbody><tr><td>Isomap</td><td>保持全局测地距离</td><td>适合平滑流形</td></tr><tr><td>LLE（局部线性嵌入）</td><td>保持局部邻域线性结构</td><td>保持局部关系</td></tr><tr><td>t-SNE</td><td>保持局部相似性（概率）</td><td>可视化效果最好</td></tr></tbody></table><hr><h4 id="_3-无监督学习的应用案例" tabindex="-1"><a class="header-anchor" href="#_3-无监督学习的应用案例"><span><strong>（3）无监督学习的应用案例</strong></span></a></h4><table><thead><tr><th>场景</th><th>任务</th><th>算法</th></tr></thead><tbody><tr><td>用户分群</td><td>客户细分、精准营销</td><td>K-Means、DBSCAN</td></tr><tr><td>降维可视化</td><td>特征压缩</td><td>PCA、t-SNE</td></tr><tr><td>异常检测</td><td>欺诈识别、设备监控</td><td>Isolation Forest、AutoEncoder</td></tr><tr><td>推荐系统</td><td>用户兴趣建模</td><td>聚类 + 协同过滤</td></tr><tr><td>NLP 预训练</td><td>词向量学习</td><td>Word2Vec、BERT（自监督）</td></tr></tbody></table><hr><h3 id="✅-总结-无监督学习的本质" tabindex="-1"><a class="header-anchor" href="#✅-总结-无监督学习的本质"><span>✅ <strong>总结：无监督学习的本质</strong></span></a></h3><table><thead><tr><th>方法</th><th>类型</th><th>核心目标</th><th>特点</th></tr></thead><tbody><tr><td>K-Means</td><td>划分聚类</td><td>最小化簇内方差</td><td>简单高效</td></tr><tr><td>DBSCAN</td><td>密度聚类</td><td>基于密度可达性</td><td>自动识别噪声</td></tr><tr><td>PCA</td><td>线性降维</td><td>最大方差保留</td><td>可解释性强</td></tr><tr><td>t-SNE</td><td>非线性降维</td><td>保持局部相似度</td><td>可视化强</td></tr><tr><td>AutoEncoder</td><td>深度无监督</td><td>重建输入特征</td><td>表示学习</td></tr><tr><td>Flow / Diffusion</td><td>概率生成</td><td>学习数据分布</td><td>现代生成模型基础</td></tr></tbody></table><hr><blockquote><p>📘 <strong>一句话总结</strong>：<br> 无监督学习的使命是“发现数据中隐藏的秩序”。<br> 它让模型从“标签依赖”走向“结构理解”，是通往表示学习与自监督学习的必经之路。</p></blockquote>',95)])])}const h=r(i,[["render",e]]),p=JSON.parse('{"path":"/posts/ml/2025-11-03-10-ml-book-k-means.html","title":"第10章 无监督学习与聚类（Unsupervised Learning & Clustering）","lang":"zh-CN","frontmatter":{"title":"第10章 无监督学习与聚类（Unsupervised Learning & Clustering）","date":"2025-11-03T00:00:00.000Z","categories":["AI"],"tags":["ai","learn-note"],"published":true,"description":"第10章 无监督学习与聚类 10.1 K-Means 与高维空间的挑战 （1）核心思想 K-Means 是最经典的无监督学习算法之一。它通过 最小化簇内样本的平方误差，将数据划分为 K 个相对紧密的簇。 其优化目标函数为： [ J = \\\\sum_{i=1}^{K} \\\\sum_{x_j \\\\in C_i} ||x_j - \\\\mu_i||^2 ] 其中： (...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"第10章 无监督学习与聚类（Unsupervised Learning & Clustering）\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-11-03T00:00:00.000Z\\",\\"dateModified\\":\\"2025-12-27T05:15:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"老马啸西风\\",\\"url\\":\\"https://houbb.github.io\\"}]}"],["meta",{"property":"og:url","content":"https://houbb.github.io/awesome-ai-coding/posts/ml/2025-11-03-10-ml-book-k-means.html"}],["meta",{"property":"og:site_name","content":"老马啸西风"}],["meta",{"property":"og:title","content":"第10章 无监督学习与聚类（Unsupervised Learning & Clustering）"}],["meta",{"property":"og:description","content":"第10章 无监督学习与聚类 10.1 K-Means 与高维空间的挑战 （1）核心思想 K-Means 是最经典的无监督学习算法之一。它通过 最小化簇内样本的平方误差，将数据划分为 K 个相对紧密的簇。 其优化目标函数为： [ J = \\\\sum_{i=1}^{K} \\\\sum_{x_j \\\\in C_i} ||x_j - \\\\mu_i||^2 ] 其中： (..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-27T05:15:15.000Z"}],["meta",{"property":"article:tag","content":"learn-note"}],["meta",{"property":"article:tag","content":"ai"}],["meta",{"property":"article:published_time","content":"2025-11-03T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-27T05:15:15.000Z"}]]},"git":{"createdTime":1766812269000,"updatedTime":1766812515000,"contributors":[{"name":"bbhou","username":"bbhou","email":"1557740299@qq.com","commits":3,"url":"https://github.com/bbhou"}]},"readingTime":{"minutes":5.35,"words":1604},"filePathRelative":"posts/ml/2025-11-03-10-ml-book-k-means.md","excerpt":"<h2><strong>第10章　无监督学习与聚类</strong></h2>\\n<hr>\\n<h3><strong>10.1 K-Means 与高维空间的挑战</strong></h3>\\n<h4><strong>（1）核心思想</strong></h4>\\n<p>K-Means 是最经典的无监督学习算法之一。它通过 <strong>最小化簇内样本的平方误差</strong>，将数据划分为 K 个相对紧密的簇。</p>\\n<p>其优化目标函数为：<br>\\n[<br>\\nJ = \\\\sum_{i=1}^{K} \\\\sum_{x_j \\\\in C_i} ||x_j - \\\\mu_i||^2<br>\\n]<br>\\n其中：</p>","autoDesc":true}');export{h as comp,p as data};
