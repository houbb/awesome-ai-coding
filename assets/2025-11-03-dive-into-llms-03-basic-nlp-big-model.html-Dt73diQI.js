import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,a,o as e}from"./app-DJMkg_fl.js";const n={};function r(l,s){return e(),t("div",null,[...s[0]||(s[0]=[a(`<h1 id="背景" tabindex="-1"><a class="header-anchor" href="#背景"><span>背景</span></a></h1><p>学习 <a href="https://github.com/Lordog/dive-into-llms" target="_blank" rel="noopener noreferrer">dive-into-llms</a></p><h1 id="chat" tabindex="-1"><a class="header-anchor" href="#chat"><span>chat</span></a></h1><h2 id="对自然语言处理-大模型的一些背景概念有初步认识-什么是预训练、微调、transformer-架构、模型推理、量化、部署等等。" tabindex="-1"><a class="header-anchor" href="#对自然语言处理-大模型的一些背景概念有初步认识-什么是预训练、微调、transformer-架构、模型推理、量化、部署等等。"><span>对自然语言处理／大模型的一些背景概念有初步认识：什么是预训练、微调、Transformer 架构、模型推理、量化、部署等等。</span></a></h2><p>非常棒 👍，你现在已经到了学习大模型最关键的一步 —— 理解 <strong>自然语言处理（NLP）与大模型的核心概念体系</strong>。</p><p>我来用尽可能通俗、系统、工程化的方式，帮你快速建立一个「从 0 到能看懂大模型原理」的完整认知地图👇</p><hr><h2 id="🧩-一、自然语言处理-nlp-是什么" tabindex="-1"><a class="header-anchor" href="#🧩-一、自然语言处理-nlp-是什么"><span>🧩 一、自然语言处理（NLP）是什么？</span></a></h2><p>一句话解释：</p><blockquote><p><strong>NLP 是让计算机理解、生成、分析人类语言的技术。</strong></p></blockquote><p>比如：</p><ul><li><strong>理解类</strong>：情感分析、意图识别、问答、信息抽取</li><li><strong>生成类</strong>：写摘要、翻译、写文章、对话</li><li><strong>推理类</strong>：推断上下文含义、改写句子、总结逻辑</li></ul><p>最早的 NLP 是基于规则的（if-else / 正则表达式），后来是机器学习（SVM、CRF），直到 2017 年 Transformer 出现，才进入了现在的「大模型时代」。</p><hr><h2 id="⚙️-二、预训练-pre-training" tabindex="-1"><a class="header-anchor" href="#⚙️-二、预训练-pre-training"><span>⚙️ 二、预训练（Pre-training）</span></a></h2><p><strong>核心思想：先学习语言规律，再去做任务。</strong></p><p>在深度学习里，训练一个模型从零开始（随机参数）需要巨量数据。但语言数据太多太复杂，于是人们想到：</p><blockquote><p>让模型先“读懂整个世界的语言”，再针对具体任务调整。</p></blockquote><p>所以：</p><ol><li>收集大规模语料（比如 Wikipedia、书籍、网页等）；</li><li>让模型通过“自监督”任务学习语言结构；</li><li>得到一个<strong>通用语言模型（pretrained model）</strong>。</li></ol><p>例子（自监督任务）：</p><ul><li><strong>掩码语言建模（MLM）</strong>：<br> “我今天去[MASK]超市” → 模型预测[MASK]=“了”。</li><li><strong>下句预测（NSP）</strong>：<br> “我今天去超市。→ 我买了牛奶。”是否是相邻句。</li></ul><p>🔸 代表模型：<strong>BERT、GPT、RoBERTa、T5</strong>。</p><p>预训练模型相当于一个「通用语言大脑」。</p><hr><h2 id="🧠-三、微调-fine-tuning" tabindex="-1"><a class="header-anchor" href="#🧠-三、微调-fine-tuning"><span>🧠 三、微调（Fine-tuning）</span></a></h2><p>预训练模型懂语言，但不知道怎么完成具体任务。<br> 于是我们对它进行「微调」：</p><blockquote><p>在你自己的任务数据上，轻微更新模型权重。</p></blockquote><p>比如：</p><ul><li>你要做报警分类模型；</li><li>你拿到报警日志 + 标签；</li><li>用预训练的 BERT，再在这些数据上训练几个 epoch；</li><li>模型学会“报警 → 原因分类”的映射。</li></ul><p>🎯 优点：</p><ul><li>不用从零训练，节省资源；</li><li>保留语言知识，适应任务。</li></ul><p>🔸 现在常见的「ChatGPT」「通义千问」「Claude」等，<br> 其实都是基于「大规模预训练 + 多轮微调（SFT、RLHF）」形成的。</p><hr><h2 id="🧱-四、transformer-架构-大模型的心脏" tabindex="-1"><a class="header-anchor" href="#🧱-四、transformer-架构-大模型的心脏"><span>🧱 四、Transformer 架构（大模型的心脏）</span></a></h2><p>Transformer 是 2017 年 Google 提出的架构论文<br><strong>《Attention Is All You Need》</strong>，<br> 意思是“注意力机制就够了”。</p><h3 id="核心思想" tabindex="-1"><a class="header-anchor" href="#核心思想"><span>核心思想：</span></a></h3><blockquote><p>模型可以根据上下文「注意」到关键部分。</p></blockquote><p>举个例子：</p><blockquote><p>“我昨天去银行钓鱼。”</p></blockquote><p>传统模型不知道“银行”是指“金融机构”还是“河岸”。<br> Transformer 会自动学习“钓鱼”这个上下文，从而判断“bank”是“河岸”。</p><h3 id="架构结构" tabindex="-1"><a class="header-anchor" href="#架构结构"><span>架构结构：</span></a></h3><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>输入 → Embedding → Self-Attention → FeedForward → 输出</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h3 id="模型类型" tabindex="-1"><a class="header-anchor" href="#模型类型"><span>模型类型：</span></a></h3><table><thead><tr><th>类型</th><th>方向</th><th>代表模型</th></tr></thead><tbody><tr><td>Encoder-only</td><td>理解类</td><td>BERT</td></tr><tr><td>Decoder-only</td><td>生成类</td><td>GPT</td></tr><tr><td>Encoder-Decoder</td><td>翻译/摘要类</td><td>T5、BART</td></tr></tbody></table><h3 id="优点" tabindex="-1"><a class="header-anchor" href="#优点"><span>优点：</span></a></h3><p>✅ 并行计算快<br> ✅ 能捕捉远程依赖（长句关系）<br> ✅ 可扩展（堆更多层、参数更多）<br> ✅ 模块化设计便于微调与扩展</p><p>现在几乎所有大模型（GPT、Claude、通义千问、Gemini、Yi、ChatGLM）都基于 Transformer。</p><hr><h2 id="🚀-五、模型推理-inference" tabindex="-1"><a class="header-anchor" href="#🚀-五、模型推理-inference"><span>🚀 五、模型推理（Inference）</span></a></h2><p>“推理”就是 <strong>让训练好的模型开始工作</strong> 的阶段。</p><p>训练是「学规律」；推理是「用规律」。</p><p>流程：</p><ol><li>加载模型权重（通常是几 GB 的 <code>.bin</code> 文件）；</li><li>输入文本（tokenize → 向量）；</li><li>前向计算，生成输出；</li><li>结果解码成文字。</li></ol><p>例如：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> transformers </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> pipeline</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">qa </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> pipeline</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;question-answering&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">model</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;uer/roberta-base-chinese-extractive-qa&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">res </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> qa</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">question</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;磁盘已满会导致什么？&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">context</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;系统无法写入文件，进程挂起&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(res)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>输出：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;score&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0.99</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;answer&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;系统无法写入文件&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><hr><h2 id="⚖️-六、量化-quantization" tabindex="-1"><a class="header-anchor" href="#⚖️-六、量化-quantization"><span>⚖️ 六、量化（Quantization）</span></a></h2><p>量化 = 压缩 + 加速。</p><p>因为大模型太大了（动辄几十亿参数）。</p><p>例如：</p><ul><li>原始模型：每个参数是 32 位浮点数（float32）；</li><li>量化后：每个参数只保留 8 位（int8）或 4 位（int4）。</li></ul><p>🎯 这样可以：</p><ul><li>减少显存占用；</li><li>加快推理速度；</li><li>损失少量精度。</li></ul><p>常用量化技术：</p><table><thead><tr><th>方法</th><th>简介</th></tr></thead><tbody><tr><td>INT8</td><td>最常见、兼顾精度</td></tr><tr><td>INT4</td><td>更极致压缩（用于LLaMA等）</td></tr><tr><td>GPTQ</td><td>专为Transformer定制的量化方案</td></tr><tr><td>AWQ / QLoRA</td><td>结合量化与低秩微调的混合方法</td></tr></tbody></table><hr><h2 id="🧰-七、部署-deployment" tabindex="-1"><a class="header-anchor" href="#🧰-七、部署-deployment"><span>🧰 七、部署（Deployment）</span></a></h2><p>训练完或微调好的模型，要让其他系统能访问，就需要部署。<br> 部署就是「把模型变成一个可访问的服务」。</p><p>典型方式有几种：</p><table><thead><tr><th>类型</th><th>举例</th><th>特点</th></tr></thead><tbody><tr><td>本地推理</td><td>用 <code>transformers</code> 在本机跑</td><td>方便开发调试</td></tr><tr><td>Web 服务</td><td>FastAPI + Uvicorn</td><td>可封装成 REST API</td></tr><tr><td>模型服务框架</td><td>TorchServe、Triton、vLLM、Text-Generation-Inference</td><td>专业推理性能优化</td></tr><tr><td>云端部署</td><td>Hugging Face Spaces、AWS SageMaker</td><td>自动化、弹性伸缩</td></tr></tbody></table><p>💡 实际部署时，还会配合：</p><ul><li>模型并行 / 张量并行</li><li>批量推理（batching）</li><li>缓存 / 流式输出（streaming）</li></ul><hr><h2 id="🧮-八、总结-大模型完整生命周期图" tabindex="-1"><a class="header-anchor" href="#🧮-八、总结-大模型完整生命周期图"><span>🧮 八、总结：大模型完整生命周期图</span></a></h2><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>                ┌──────────────────┐</span></span>
<span class="line"><span>                │ 预训练 (Pretrain) │  —— 从大规模语料学习语言规律</span></span>
<span class="line"><span>                └───────┬──────────┘</span></span>
<span class="line"><span>                        │</span></span>
<span class="line"><span>                        ▼</span></span>
<span class="line"><span>                ┌──────────────────┐</span></span>
<span class="line"><span>                │ 微调 (Fine-tune) │  —— 让模型适配具体任务</span></span>
<span class="line"><span>                └───────┬──────────┘</span></span>
<span class="line"><span>                        │</span></span>
<span class="line"><span>                        ▼</span></span>
<span class="line"><span>                ┌──────────────────┐</span></span>
<span class="line"><span>                │ 推理 (Inference) │  —— 让模型输出结果</span></span>
<span class="line"><span>                └───────┬──────────┘</span></span>
<span class="line"><span>                        │</span></span>
<span class="line"><span>                        ▼</span></span>
<span class="line"><span>                ┌──────────────────┐</span></span>
<span class="line"><span>                │ 量化 / 部署 (Deploy) │ —— 压缩 + 服务化</span></span>
<span class="line"><span>                └──────────────────┘</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><hr><h2 id="📚-九、推荐入门资料-实用-中文-免费" tabindex="-1"><a class="header-anchor" href="#📚-九、推荐入门资料-实用-中文-免费"><span>📚 九、推荐入门资料（实用+中文+免费）</span></a></h2><table><thead><tr><th>主题</th><th>资源</th><th>说明</th></tr></thead><tbody><tr><td>Transformer 基础</td><td><a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer">The Illustrated Transformer（图解 Transformer）</a></td><td>动画图讲得极通俗（建议看英文）</td></tr><tr><td>系统教程</td><td><a href="https://zh.d2l.ai/chapter_attention-mechanisms/index.html" target="_blank" rel="noopener noreferrer">李沐《动手学深度学习》 第9章：Transformer</a></td><td>理论+PyTorch 实践</td></tr><tr><td>实战</td><td><a href="https://huggingface.co/docs/transformers/main/zh/index" target="_blank" rel="noopener noreferrer">Hugging Face Transformers 中文文档</a></td><td>跑模型、部署、微调</td></tr><tr><td>扩展</td><td><a href="https://learn.deeplearning.ai/" target="_blank" rel="noopener noreferrer">LLM Bootcamp（吴恩达）</a></td><td>面向工程师的 LLM 实战课</td></tr></tbody></table><hr><h2 id="🚀-十、学习建议-适合你这种架构师背景" tabindex="-1"><a class="header-anchor" href="#🚀-十、学习建议-适合你这种架构师背景"><span>🚀 十、学习建议（适合你这种架构师背景）</span></a></h2><table><thead><tr><th>阶段</th><th>目标</th><th>推荐实践</th></tr></thead><tbody><tr><td>1️⃣ 第 1 周</td><td>理解 Transformer + 预训练思想</td><td>看图解 Transformer + 跑 BERT 推理</td></tr><tr><td>2️⃣ 第 2 周</td><td>掌握微调流程</td><td>用 Hugging Face 微调中文分类模型</td></tr><tr><td>3️⃣ 第 3 周</td><td>学习推理与量化</td><td>尝试 int8 推理、比较速度差异</td></tr><tr><td>4️⃣ 第 4 周</td><td>学习部署</td><td>用 FastAPI 封装模型服务，暴露接口</td></tr></tbody></table>`,84)])])}const h=i(n,[["render",r]]),o=JSON.parse('{"path":"/posts/learn-llms/2025-11-03-dive-into-llms-03-basic-nlp-big-model.html","title":"dive-into-llms-03-自然语言处理／大模型的一些背景概念","lang":"zh-CN","frontmatter":{"title":"dive-into-llms-03-自然语言处理／大模型的一些背景概念","date":"2025-11-03T00:00:00.000Z","categories":["AI"],"tags":["ai","learn-note"],"published":true,"description":"背景 学习 dive-into-llms chat 对自然语言处理／大模型的一些背景概念有初步认识：什么是预训练、微调、Transformer 架构、模型推理、量化、部署等等。 非常棒 👍，你现在已经到了学习大模型最关键的一步 —— 理解 自然语言处理（NLP）与大模型的核心概念体系。 我来用尽可能通俗、系统、工程化的方式，帮你快速建立一个「从 0 ...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"dive-into-llms-03-自然语言处理／大模型的一些背景概念\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-11-03T00:00:00.000Z\\",\\"dateModified\\":\\"2025-12-27T05:15:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"老马啸西风\\",\\"url\\":\\"https://houbb.github.io\\"}]}"],["meta",{"property":"og:url","content":"https://houbb.github.io/awesome-ai-coding/posts/learn-llms/2025-11-03-dive-into-llms-03-basic-nlp-big-model.html"}],["meta",{"property":"og:site_name","content":"老马啸西风"}],["meta",{"property":"og:title","content":"dive-into-llms-03-自然语言处理／大模型的一些背景概念"}],["meta",{"property":"og:description","content":"背景 学习 dive-into-llms chat 对自然语言处理／大模型的一些背景概念有初步认识：什么是预训练、微调、Transformer 架构、模型推理、量化、部署等等。 非常棒 👍，你现在已经到了学习大模型最关键的一步 —— 理解 自然语言处理（NLP）与大模型的核心概念体系。 我来用尽可能通俗、系统、工程化的方式，帮你快速建立一个「从 0 ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-27T05:15:15.000Z"}],["meta",{"property":"article:tag","content":"learn-note"}],["meta",{"property":"article:tag","content":"ai"}],["meta",{"property":"article:published_time","content":"2025-11-03T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-27T05:15:15.000Z"}]]},"git":{"createdTime":1766812269000,"updatedTime":1766812515000,"contributors":[{"name":"bbhou","username":"bbhou","email":"1557740299@qq.com","commits":3,"url":"https://github.com/bbhou"}]},"readingTime":{"minutes":5.29,"words":1588},"filePathRelative":"posts/learn-llms/2025-11-03-dive-into-llms-03-basic-nlp-big-model.md","excerpt":"\\n<p>学习 <a href=\\"https://github.com/Lordog/dive-into-llms\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">dive-into-llms</a></p>\\n<h1>chat</h1>\\n<h2>对自然语言处理／大模型的一些背景概念有初步认识：什么是预训练、微调、Transformer 架构、模型推理、量化、部署等等。</h2>\\n<p>非常棒 👍，你现在已经到了学习大模型最关键的一步 —— 理解 <strong>自然语言处理（NLP）与大模型的核心概念体系</strong>。</p>\\n<p>我来用尽可能通俗、系统、工程化的方式，帮你快速建立一个「从 0 到能看懂大模型原理」的完整认知地图👇</p>","autoDesc":true}');export{h as comp,o as data};
