import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as i,a as t,b as a,e as p,o as l}from"./app-DYfk_09i.js";const n={};function o(s,r){return l(),i("div",null,[...r[0]||(r[0]=[t('<h1 id="【ai简史】第3章-概率与统计-机器学习的灵魂数学" tabindex="-1"><a class="header-anchor" href="#【ai简史】第3章-概率与统计-机器学习的灵魂数学"><span>【AI简史】第3章 概率与统计：机器学习的灵魂数学</span></a></h1><blockquote><p>你以为机器学习靠的是算力，其实靠的是“算概率”。</p></blockquote><p>很多人一听到“概率与统计”，第一反应就是头疼。<br> 但如果你真想理解机器学习的底层逻辑，这一块必须得啃下来。</p><p>因为无论是推荐算法、语音识别，还是 ChatGPT，<br> 它们本质上都在做一件事——<strong>在不确定性中做决策。</strong></p><p>而能让机器理解“不确定”，只有概率论和统计学。</p><hr><h2 id="一、不确定世界的语言" tabindex="-1"><a class="header-anchor" href="#一、不确定世界的语言"><span>一、不确定世界的语言</span></a></h2><p>数学大体分两种：</p><ul><li>一种描述“确定”的世界，比如代数、几何；</li><li>另一种描述“不确定”的世界——那就是概率与统计。</li></ul><p>生活中充满了不确定：<br> 天气会不会下雨、股价明天涨不涨、下一部电影火不火。</p><p>概率论告诉我们：如何用数字描述这些不确定性；<br> 统计学则告诉我们：如何从数据里发现它们背后的规律。</p><p>这两者结合，就是机器学习的数学灵魂。<br> 说白了，机器学习的目标就是——<strong>用数据去推测未来。</strong></p><hr><h2 id="二、随机变量-世界的骰子" tabindex="-1"><a class="header-anchor" href="#二、随机变量-世界的骰子"><span>二、随机变量：世界的骰子</span></a></h2><p>掷一个骰子，结果可能是 1 到 6。<br> 我们用 X 表示这个结果。</p><p>X 就是一个“随机变量”，它把一个不确定的事件转化为一个数值。</p><p>随机变量有两类：</p><ul><li>离散型，比如骰子点数、硬币正反、商品是否被点击；</li><li>连续型，比如气温、身高、股价。</li></ul><p>而概率分布，就是告诉我们这些值出现的可能性有多大。<br> 对离散变量看概率质量函数（PMF），<br> 对连续变量看概率密度函数（PDF）。</p><p>一句话概括：</p><blockquote><p>概率分布，就是世界的“可能性地图”。</p></blockquote><hr><h2 id="三、期望与方差-平均与不确定" tabindex="-1"><a class="header-anchor" href="#三、期望与方差-平均与不确定"><span>三、期望与方差：平均与不确定</span></a></h2><p>“期望”代表平均结果——长期来看系统的中心。<br> “方差”代表波动——系统有多容易出岔子。</p><p>在机器学习里，期望是模型的预测值，<br> 方差体现模型预测的稳定性。</p><p>你可以这么理解：</p><blockquote><p>期望让我们知道“最可能发生什么”，<br> 方差让我们知道“它有多不稳定”。</p></blockquote><p>模型预测值的平均误差、损失函数的定义、模型泛化能力的衡量，<br> 背后都离不开这两个概念。</p><hr><h2 id="四、极大似然估计-最像真的那个参数" tabindex="-1"><a class="header-anchor" href="#四、极大似然估计-最像真的那个参数"><span>四、极大似然估计：最像真的那个参数</span></a></h2><p>假设你掷了 10 次硬币，出现 8 次正面。<br> 你猜这枚硬币的正面概率是多少？</p><p>答案是 0.8——<br> 这是一个典型的“极大似然估计”（Maximum Likelihood Estimation，简称 MLE）。</p><p>它的思想是：</p><blockquote><p>给定观测数据，找到最可能产生这些数据的参数。</p></blockquote><p>比如逻辑回归、隐马尔可夫模型、GMM、高斯混合分布，<br> 这些模型训练时，都是在背后做这件事——<br> 寻找最可能的那一组参数。</p><p>可以说，MLE 是所有概率模型的共同语言。</p><hr><h2 id="五、贝叶斯推断-用信念更新信念" tabindex="-1"><a class="header-anchor" href="#五、贝叶斯推断-用信念更新信念"><span>五、贝叶斯推断：用信念更新信念</span></a></h2><p>贝叶斯推断比极大似然更“哲学”一点。</p><p>它认为，我们永远不是从零开始。<br> 每一次新的观察，都是在修正我们原有的信念。</p><p>用公式表达就是：</p><p>P(参数 | 数据) = [P(数据 | 参数) × P(参数)] / P(数据)</p><p>别被符号吓到。它的意思很简单：</p><blockquote><p>我有一个先验信念（Prior），<br> 看到新数据（Likelihood）后，<br> 更新成新的信念（Posterior）。</p></blockquote><p>举个例子：<br> 你原本觉得硬币是公平的（p=0.5），<br> 但掷了十次后出现八次正面，<br> 你自然会觉得它“可能稍微偏正面一点”。</p><p>这就是贝叶斯思维。<br> 不是推翻旧信念，而是根据证据去更新它。</p><p>现代机器学习里，很多方法都带着这种思维：<br> 朴素贝叶斯分类器、贝叶斯网络、高斯过程，<br> 甚至大语言模型的“先验知识”也有贝叶斯的影子。</p><hr><h2 id="六、朴素贝叶斯-简单的高分选手" tabindex="-1"><a class="header-anchor" href="#六、朴素贝叶斯-简单的高分选手"><span>六、朴素贝叶斯：简单的高分选手</span></a></h2><p>朴素贝叶斯是贝叶斯思想的一个简化版。<br> 它假设输入特征之间相互独立——虽然不太现实，但计算效率极高。</p><p>算法逻辑是这样的：</p><p>P(y|x₁, x₂, …, xₙ) ∝ P(y) × Π P(xᵢ|y)</p><p>通俗地讲：<br> 一个邮件是不是垃圾邮件，<br> 取决于它是否包含“中奖”、“免费”、“限时”等词语，<br> 以及这些词在垃圾邮件中出现的概率。</p><p>训练时，我们统计这些概率；<br> 预测时，我们计算每个类别的概率，选最大的那个。</p><p>它结构简单，但在文本分类上表现出奇效。<br> 垃圾邮件过滤、情感分析、新闻分类——它都能打。</p><p>有人说：</p><blockquote><p>“朴素贝叶斯是那个看起来平平无奇，却次次考高分的学生。”</p></blockquote><hr><h2 id="七、信息论-用数字衡量-未知" tabindex="-1"><a class="header-anchor" href="#七、信息论-用数字衡量-未知"><span>七、信息论：用数字衡量“未知”</span></a></h2><p>香农的信息论，让我们第一次能用数学量化“不确定性”。</p><p>熵（Entropy）描述系统的混乱程度：</p><p>H(X) = - Σ P(x) log P(x)</p><p>一个永远正面的硬币，熵是 0；<br> 一个完全随机的硬币，熵是 1。<br> 熵越高，系统越不可预测。</p><p>KL 散度（相对熵）衡量两个分布的差距，<br> 比如模型预测分布 Q(x) 和真实分布 P(x) 的差异。</p><p>交叉熵（Cross Entropy）是实际中最常用的形式，<br> 是深度学习里分类任务的标准损失函数。</p><p>一句话总结：</p><blockquote><p>熵衡量不确定，KL 衡量差距，交叉熵用来优化。</p></blockquote><hr><h2 id="八、为什么要懂概率" tabindex="-1"><a class="header-anchor" href="#八、为什么要懂概率"><span>八、为什么要懂概率？</span></a></h2><p>因为这不仅是理解机器学习的钥匙，<br> 也是理解世界的一种方式。</p><p>当模型说“猫的概率是 0.8”时，<br> 它并不是在说“这一定是猫”，<br> 而是在说：“在我所见的世界里，最有可能是猫。”</p><p>这其实跟人类判断世界的方式一模一样。<br> 我们也从不追求“绝对正确”，<br> 我们只是在不断修正、不断接近真相。</p><p>概率论教会机器理性，<br> 统计学教会机器学习，<br> 而理解它们，<br> 就是理解智能本身。</p><hr><p><strong>结语</strong></p><p>当你真正学懂概率，会发现一个奇妙的变化：<br> 世界从“随机与混乱”，<br> 变成了“有迹可循的可能性空间”。</p><p>这就是机器学习的起点，<br> 也是人类理解智能的开始。</p><hr><p>是否希望我帮你补上适合发布的<strong>标题备选 + 封面文案 + 公众号摘要（引导点击的简介）</strong>？<br> 我可以直接给出三套风格（思考型 / 科普型 / 干货型）供你选择。</p><hr><h1 id="第3章-概率与统计" tabindex="-1"><a class="header-anchor" href="#第3章-概率与统计"><span>第3章　概率与统计</span></a></h1><p>这一章可以说是机器学习的数学“灵魂”章节——<strong>概率与统计</strong>是理解一切模型（从朴素贝叶斯到深度神经网络）的底层逻辑。</p><h2 id="🌟-引言" tabindex="-1"><a class="header-anchor" href="#🌟-引言"><span>🌟 引言</span></a></h2><p>机器学习的核心任务其实就是“在不确定性中做决策”。</p><p>而<strong>概率论</strong>提供了处理不确定性的语言，<strong>统计学</strong>提供了从数据中估计规律的方法。</p><p>如果说：</p><ul><li>代数 → 是确定世界的数学；</li><li>概率与统计 → 就是“不确定世界的数学”。</li></ul><hr><h2 id="_3-1-随机变量与分布" tabindex="-1"><a class="header-anchor" href="#_3-1-随机变量与分布"><span>3.1 随机变量与分布</span></a></h2><h3 id="✅-随机变量-random-variable" tabindex="-1"><a class="header-anchor" href="#✅-随机变量-random-variable"><span>✅ 随机变量（Random Variable）</span></a></h3>',90),a("ul",null,[a("li",null,[a("p",null,[a("strong",null,"定义"),p("：随机变量是一个用数字表示随机事件结果的函数。"),a("br"),p(" 比如：")]),a("ul",null,[a("li",{"1,2,3,4,5,6":""},"掷骰子 → 可能结果"),a("li",null,"把“点数”定义为随机变量 X，那 X 就是一个离散随机变量。")])])],-1),t('<h3 id="🧩-两大类" tabindex="-1"><a class="header-anchor" href="#🧩-两大类"><span>🧩 两大类：</span></a></h3><ol><li><p><strong>离散型随机变量（Discrete）</strong></p><ul><li>取值是有限或可数的</li><li>如骰子点数、硬币正反面</li><li>常用分布：伯努利分布、二项分布、泊松分布</li></ul></li><li><p><strong>连续型随机变量（Continuous）</strong></p><ul><li>取值是连续的（可取任意实数）</li><li>如人的身高、温度</li><li>常用分布：正态分布、均匀分布、指数分布</li></ul></li></ol><h3 id="📊-概率分布-probability-distribution" tabindex="-1"><a class="header-anchor" href="#📊-概率分布-probability-distribution"><span>📊 概率分布（Probability Distribution）</span></a></h3><p>概率分布定义了随机变量的取值“可能性”：</p><ul><li>对离散变量 → 概率质量函数（PMF）<br> ( P(X=x_i) )</li><li>对连续变量 → 概率密度函数（PDF）<br> ( f(x) )，且 ( P(a \\le X \\le b) = \\int_a^b f(x) dx )</li></ul><h3 id="🎯-期望与方差" tabindex="-1"><a class="header-anchor" href="#🎯-期望与方差"><span>🎯 期望与方差</span></a></h3><ul><li><strong>期望</strong>：平均值（模型预测的平均输出）<br> [<br> E[X] = \\sum_i x_i P(x_i)<br> ]</li><li><strong>方差</strong>：不确定性的量化<br> [<br> Var(X) = E[(X - E[X])^2]<br> ]</li></ul><p>这些是机器学习里「损失函数」与「不确定性」的数学基石。</p><hr><h2 id="_3-2-极大似然估计-mle-与贝叶斯推断" tabindex="-1"><a class="header-anchor" href="#_3-2-极大似然估计-mle-与贝叶斯推断"><span>3.2 极大似然估计（MLE）与贝叶斯推断</span></a></h2><h3 id="🎯-极大似然估计-maximum-likelihood-estimation-mle" tabindex="-1"><a class="header-anchor" href="#🎯-极大似然估计-maximum-likelihood-estimation-mle"><span>🎯 极大似然估计（Maximum Likelihood Estimation, MLE）</span></a></h3><p><strong>目标</strong>：在已知数据的情况下，找到最可能生成这些数据的模型参数。</p><p>假设我们有样本数据 ( D = {x_1, x_2, ..., x_n} )，<br> 模型的参数为 ( \\theta )，则似然函数为：</p><p>[<br> L(\\theta) = P(D|\\theta) = \\prod_i P(x_i|\\theta)<br> ]</p><p>取对数方便计算：</p><p>[<br> \\hat{\\theta} = \\arg\\max_\\theta \\log L(\\theta)<br> ]</p><p>🧠 举个例子：</p><ul><li>掷硬币 n 次，结果正面次数 k</li><li>假设正面概率为 ( p )，似然函数：<br> [<br> L(p) = p^k (1-p)^{n-k}<br> ]</li><li>最大化后得到：<br> [<br> \\hat{p} = \\frac{k}{n}<br> ]<br> 这就是最直观的「极大似然估计」。</li></ul><hr><h3 id="🧮-贝叶斯推断-bayesian-inference" tabindex="-1"><a class="header-anchor" href="#🧮-贝叶斯推断-bayesian-inference"><span>🧮 贝叶斯推断（Bayesian Inference）</span></a></h3><p>贝叶斯思想强调「先验知识 + 数据更新」：<br> [<br> P(\\theta|D) = \\frac{P(D|\\theta) P(\\theta)}{P(D)}<br> ]</p><ul><li><strong>先验（Prior）</strong>：在看到数据前对参数的信念。</li><li><strong>似然（Likelihood）</strong>：数据在参数下出现的可能性。</li><li><strong>后验（Posterior）</strong>：看到数据后的新信念。</li></ul><p>📘 举例：<br> 如果你认为硬币可能是公平的（先验 ( p=0.5 )），<br> 但你掷了10次出现8次正面，贝叶斯更新后你会认为“可能稍偏正面”。</p><p>👉 贝叶斯方法在现代机器学习中非常重要：</p><ul><li>朴素贝叶斯分类器</li><li>贝叶斯网络</li><li>高斯过程（Gaussian Process）</li><li>LLM 先验知识建模</li></ul><hr><h2 id="_3-3-条件概率与朴素贝叶斯模型" tabindex="-1"><a class="header-anchor" href="#_3-3-条件概率与朴素贝叶斯模型"><span>3.3 条件概率与朴素贝叶斯模型</span></a></h2><h3 id="🧩-条件概率-conditional-probability" tabindex="-1"><a class="header-anchor" href="#🧩-条件概率-conditional-probability"><span>🧩 条件概率（Conditional Probability）</span></a></h3><p>[<br> P(A|B) = \\frac{P(A,B)}{P(B)}<br> ]</p><p>理解为“在 B 发生的前提下，A 发生的概率”。</p><p>比如：</p><ul><li>事件 A：邮件是垃圾邮件</li><li>事件 B：邮件中出现“中奖”一词<br> 则 ( P(A|B) ) 表示：出现“中奖”的邮件是垃圾邮件的概率。</li></ul><hr><h3 id="📘-朴素贝叶斯分类器-naive-bayes" tabindex="-1"><a class="header-anchor" href="#📘-朴素贝叶斯分类器-naive-bayes"><span>📘 朴素贝叶斯分类器（Naive Bayes）</span></a></h3><p>假设输入特征之间相互独立（朴素假设）：</p><p>[<br> P(y|x_1, ..., x_n) \\propto P(y) \\prod_i P(x_i|y)<br> ]</p><p>算法流程：</p><ol><li>从训练数据估计先验 ( P(y) )</li><li>估计条件概率 ( P(x_i|y) )</li><li>对新样本，计算各类的后验概率并选取最大者。</li></ol><p>📊 应用场景：</p><ul><li>垃圾邮件分类</li><li>文本情感分析</li><li>新闻主题分类</li></ul><p>🧠 尽管“朴素”，但在高维稀疏数据（如文本词袋模型）上效果惊人好。</p><hr><h2 id="_3-4-信息论基础-熵、kl散度、交叉熵" tabindex="-1"><a class="header-anchor" href="#_3-4-信息论基础-熵、kl散度、交叉熵"><span>3.4 信息论基础：熵、KL散度、交叉熵</span></a></h2><h3 id="🔹-熵-entropy" tabindex="-1"><a class="header-anchor" href="#🔹-熵-entropy"><span>🔹 熵（Entropy）</span></a></h3><p>衡量不确定性的数学量：</p><p>[<br> H(X) = - \\sum_x P(x) \\log P(x)<br> ]</p><ul><li>熵越大 → 不确定性越高</li><li>熵越小 → 越有序</li></ul><p>📘 举例：</p><ul><li>公平硬币 ( H = 1 )</li><li>总是正面的硬币 ( H = 0 )</li></ul><hr><h3 id="🔹-相对熵-kl-散度" tabindex="-1"><a class="header-anchor" href="#🔹-相对熵-kl-散度"><span>🔹 相对熵（KL 散度）</span></a></h3><p>衡量两个分布的“差距”：<br> [<br> D_{KL}(P||Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}<br> ]</p><p>在机器学习中：</p><ul><li>衡量模型分布 ( Q ) 与真实分布 ( P ) 的差距。</li><li>是许多损失函数（例如交叉熵）的理论来源。</li></ul><hr><h3 id="🔹-交叉熵-cross-entropy" tabindex="-1"><a class="header-anchor" href="#🔹-交叉熵-cross-entropy"><span>🔹 交叉熵（Cross Entropy）</span></a></h3><p>[<br> H(P,Q) = - \\sum_x P(x) \\log Q(x)<br> ]</p><p>当 ( P ) 是真实分布、( Q ) 是模型预测分布时，<br> 最小化交叉熵 ≈ 让模型预测尽可能接近真实。</p><p>📘 应用：</p><ul><li>分类任务中的损失函数（Softmax + CrossEntropyLoss）</li><li>信息压缩与语言模型的困惑度（Perplexity）计算</li></ul><hr><h2 id="🌍-小结" tabindex="-1"><a class="header-anchor" href="#🌍-小结"><span>🌍 小结</span></a></h2><table><thead><tr><th>概念</th><th>核心作用</th><th>在机器学习中的体现</th></tr></thead><tbody><tr><td>随机变量</td><td>建模不确定性</td><td>特征、标签的概率表达</td></tr><tr><td>MLE</td><td>参数估计</td><td>逻辑回归、GMM</td></tr><tr><td>贝叶斯推断</td><td>融合先验与数据</td><td>朴素贝叶斯、贝叶斯网络</td></tr><tr><td>条件概率</td><td>推断关系</td><td>分类与推荐</td></tr><tr><td>熵 / KL 散度 / 交叉熵</td><td>信息量与分布差异</td><td>损失函数、模型评估</td></tr></tbody></table>',63)])])}const d=e(n,[["render",o]]),c=JSON.parse('{"path":"/posts/ml/2025-11-03-03-ml-book-rate-and-stat.html","title":"第3章　概率与统计","lang":"zh-CN","frontmatter":{"title":"第3章　概率与统计","date":"2025-11-03T00:00:00.000Z","categories":["AI"],"tags":["ai","learn-note"],"published":true,"description":"【AI简史】第3章 概率与统计：机器学习的灵魂数学 你以为机器学习靠的是算力，其实靠的是“算概率”。 很多人一听到“概率与统计”，第一反应就是头疼。 但如果你真想理解机器学习的底层逻辑，这一块必须得啃下来。 因为无论是推荐算法、语音识别，还是 ChatGPT， 它们本质上都在做一件事——在不确定性中做决策。 而能让机器理解“不确定”，只有概率论和统计学...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"第3章　概率与统计\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-11-03T00:00:00.000Z\\",\\"dateModified\\":\\"2025-12-27T05:15:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"老马啸西风\\",\\"url\\":\\"https://houbb.github.io\\"}]}"],["meta",{"property":"og:url","content":"https://houbb.github.io/blog-thinking/posts/ml/2025-11-03-03-ml-book-rate-and-stat.html"}],["meta",{"property":"og:site_name","content":"老马啸西风"}],["meta",{"property":"og:title","content":"第3章　概率与统计"}],["meta",{"property":"og:description","content":"【AI简史】第3章 概率与统计：机器学习的灵魂数学 你以为机器学习靠的是算力，其实靠的是“算概率”。 很多人一听到“概率与统计”，第一反应就是头疼。 但如果你真想理解机器学习的底层逻辑，这一块必须得啃下来。 因为无论是推荐算法、语音识别，还是 ChatGPT， 它们本质上都在做一件事——在不确定性中做决策。 而能让机器理解“不确定”，只有概率论和统计学..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-27T05:15:15.000Z"}],["meta",{"property":"article:tag","content":"learn-note"}],["meta",{"property":"article:tag","content":"ai"}],["meta",{"property":"article:published_time","content":"2025-11-03T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-27T05:15:15.000Z"}]]},"git":{"createdTime":1766812269000,"updatedTime":1766812515000,"contributors":[{"name":"bbhou","username":"bbhou","email":"1557740299@qq.com","commits":3,"url":"https://github.com/bbhou"}]},"readingTime":{"minutes":9.97,"words":2990},"filePathRelative":"posts/ml/2025-11-03-03-ml-book-rate-and-stat.md","excerpt":"\\n<blockquote>\\n<p>你以为机器学习靠的是算力，其实靠的是“算概率”。</p>\\n</blockquote>\\n<p>很多人一听到“概率与统计”，第一反应就是头疼。<br>\\n但如果你真想理解机器学习的底层逻辑，这一块必须得啃下来。</p>\\n<p>因为无论是推荐算法、语音识别，还是 ChatGPT，<br>\\n它们本质上都在做一件事——<strong>在不确定性中做决策。</strong></p>\\n<p>而能让机器理解“不确定”，只有概率论和统计学。</p>\\n<hr>\\n<h2>一、不确定世界的语言</h2>\\n<p>数学大体分两种：</p>\\n<ul>\\n<li>一种描述“确定”的世界，比如代数、几何；</li>\\n<li>另一种描述“不确定”的世界——那就是概率与统计。</li>\\n</ul>","autoDesc":true}');export{d as comp,c as data};
