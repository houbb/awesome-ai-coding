import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as r,a,o as n}from"./app-CxH-55KZ.js";const i={};function o(s,t){return n(),r("div",null,[...t[0]||(t[0]=[a('<h1 id="第13章-nlp-领域的机器学习实践" tabindex="-1"><a class="header-anchor" href="#第13章-nlp-领域的机器学习实践"><span>第13章　NLP 领域的机器学习实践</span></a></h1><p>第 13 章「<strong>NLP 领域的机器学习实践</strong>」是整个机器学习体系中最具代表性的实战篇章之一。</p><p>自然语言处理（Natural Language Processing, NLP）是机器学习在真实世界中最早、也最广泛的落地场景之一。</p><p>本章将从数据预处理、表示方法到经典任务与模型，系统地讲解传统机器学习如何“理解文字”。</p><h1 id="第13章-nlp-领域的机器学习实践-1" tabindex="-1"><a class="header-anchor" href="#第13章-nlp-领域的机器学习实践-1"><span>第13章　NLP 领域的机器学习实践</span></a></h1><p>语言是人类智慧的外化，而 NLP 就是让机器理解、生成、归纳语言规律的科学。</p><p>虽然如今的 NLP 已进入深度学习与大模型时代，但传统机器学习方法依然是 NLP 的理论基石与工业入门方案。</p><hr><h2 id="_13-1-文本预处理与向量化-tf-idf、word2vec" tabindex="-1"><a class="header-anchor" href="#_13-1-文本预处理与向量化-tf-idf、word2vec"><span><strong>13.1 文本预处理与向量化（TF-IDF、Word2Vec）</strong></span></a></h2><p>机器学习无法直接理解文字，它只能理解「数字」。<br> 因此，文本预处理与向量化是所有 NLP 任务的第一步。</p><hr><h3 id="🧹-1-文本预处理-text-preprocessing" tabindex="-1"><a class="header-anchor" href="#🧹-1-文本预处理-text-preprocessing"><span>🧹 1. 文本预处理（Text Preprocessing）</span></a></h3><p><strong>目标：</strong> 清洗、规范化原始文本，让后续算法更好地建模。</p><p><strong>常见步骤：</strong></p><ol><li><p><strong>分词（Tokenization）</strong></p><ul><li>英文：按空格、标点切分。</li><li>中文：需使用分词器，如 <code>jieba</code>、<code>HanLP</code>、<code>THULAC</code>。</li></ul></li><li><p><strong>大小写统一与停用词去除</strong></p><ul><li>停用词（如 “的”、“and”、“the”）对语义贡献小。</li></ul></li><li><p><strong>词干提取 / 词形还原（Stemming / Lemmatization）</strong></p><ul><li>把 “running”、“runs” 统一成 “run”。</li></ul></li><li><p><strong>符号与噪声去除</strong></p><ul><li>表情、网址、数字、HTML 标签等。</li></ul></li><li><p><strong>特征缩放与过滤</strong></p><ul><li>去除极少出现的词，控制词表规模。</li></ul></li></ol><p>💡 <em>预处理的目标不是“纯净”，而是“高效”。</em></p><hr><h3 id="📊-2-词袋模型-bag-of-words-bow" tabindex="-1"><a class="header-anchor" href="#📊-2-词袋模型-bag-of-words-bow"><span>📊 2. 词袋模型（Bag of Words, BoW）</span></a></h3><p>最基础的文本向量化方式：</p><ul><li>不考虑词序；</li><li>统计每个词在文档中出现的次数；</li><li>用词频向量表示文本。</li></ul><p>示例：</p><blockquote><p>“I love NLP” → [1,1,1,0,0,...]</p></blockquote><p><strong>优点：</strong> 简单直观；<br><strong>缺点：</strong> 忽略语义与上下文，维度高且稀疏。</p><hr><h3 id="📈-3-tf-idf-term-frequency-inverse-document-frequency" tabindex="-1"><a class="header-anchor" href="#📈-3-tf-idf-term-frequency-inverse-document-frequency"><span>📈 3. TF-IDF（Term Frequency - Inverse Document Frequency）</span></a></h3><p><strong>核心思想：</strong></p><ul><li>词频高说明词重要；</li><li>但如果在所有文档都常见（如“的”），说明没区分度。</li></ul><p>公式：<br> [<br> \\text{TF-IDF}(t, d) = TF(t,d) \\times \\log\\frac{N}{DF(t)}<br> ]</p><ul><li>TF：词频</li><li>DF：包含该词的文档数</li><li>N：总文档数</li></ul><p>结果是一个“稀疏但信息量高”的特征向量。</p><hr><h3 id="🧠-4-word2vec-与词嵌入-word-embedding" tabindex="-1"><a class="header-anchor" href="#🧠-4-word2vec-与词嵌入-word-embedding"><span>🧠 4. Word2Vec 与词嵌入（Word Embedding）</span></a></h3><p><strong>革命性思想：</strong><br> 让每个词对应一个低维稠密向量，使语义相似的词在空间中距离更近。</p><p>两种模型：</p><ul><li><strong>CBOW（Continuous Bag of Words）</strong>：通过上下文预测中心词；</li><li><strong>Skip-gram</strong>：通过中心词预测上下文。</li></ul><p>训练目标：最大化词语共现的概率。</p><p>示例结果：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>vector(&quot;king&quot;) - vector(&quot;man&quot;) + vector(&quot;woman&quot;) ≈ vector(&quot;queen&quot;)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>💡 <em>Word2Vec 让机器开始“理解语义”，是通向深度 NLP 的里程碑。</em></p><hr><h2 id="_13-2-文本分类与情感分析" tabindex="-1"><a class="header-anchor" href="#_13-2-文本分类与情感分析"><span><strong>13.2 文本分类与情感分析</strong></span></a></h2><h3 id="🏷️-1-文本分类-text-classification" tabindex="-1"><a class="header-anchor" href="#🏷️-1-文本分类-text-classification"><span>🏷️ 1. 文本分类（Text Classification）</span></a></h3><p><strong>任务目标：</strong><br> 给定一段文本，判断它属于哪个类别。<br> （如新闻分类、垃圾邮件检测、产品评论分类等）</p><p><strong>常见流程：</strong></p><ol><li>文本预处理</li><li>特征提取（BoW、TF-IDF、Word2Vec 平均）</li><li>模型训练（SVM、逻辑回归、朴素贝叶斯等）</li><li>模型评估（Accuracy、F1、AUC）</li></ol><p><strong>经典模型：</strong></p><table><thead><tr><th>模型</th><th>特点</th></tr></thead><tbody><tr><td>朴素贝叶斯 (Naive Bayes)</td><td>快速、可解释、效果稳健</td></tr><tr><td>逻辑回归 (Logistic Regression)</td><td>线性分类基准模型</td></tr><tr><td>SVM</td><td>适合高维稀疏数据</td></tr><tr><td>随机森林 / XGBoost</td><td>非线性建模能力强</td></tr></tbody></table><hr><h3 id="💬-2-情感分析-sentiment-analysis" tabindex="-1"><a class="header-anchor" href="#💬-2-情感分析-sentiment-analysis"><span>💬 2. 情感分析（Sentiment Analysis）</span></a></h3><p><strong>目标：</strong> 判断一段文本的情感倾向（正面、中性、负面）。</p><p><strong>典型应用：</strong></p><ul><li>电商评论好评率分析</li><li>社交媒体舆情监测</li><li>用户满意度建模</li></ul><p><strong>方法演进：</strong></p><ol><li><strong>基于词典</strong>：正负面情感词表 + 规则</li><li><strong>基于机器学习</strong>：TF-IDF + 分类器（SVM / LR）</li><li><strong>深度学习 / 预训练模型</strong>：RNN / BERT 语义建模</li></ol><p>💡 <em>传统机器学习在情感分析中仍广泛使用，尤其在小样本、可解释场景中。</em></p><hr><h2 id="_13-3-主题模型-lda、lsa" tabindex="-1"><a class="header-anchor" href="#_13-3-主题模型-lda、lsa"><span><strong>13.3 主题模型（LDA、LSA）</strong></span></a></h2><h3 id="🧩-1-主题模型是什么" tabindex="-1"><a class="header-anchor" href="#🧩-1-主题模型是什么"><span>🧩 1. 主题模型是什么？</span></a></h3><p><strong>目标：</strong> 从大量文档中自动发现“潜在主题”。<br> 比如一堆新闻中自动聚类出「体育」、「财经」、「科技」等话题。</p><hr><h3 id="🧮-2-lsa-latent-semantic-analysis" tabindex="-1"><a class="header-anchor" href="#🧮-2-lsa-latent-semantic-analysis"><span>🧮 2. LSA（Latent Semantic Analysis）</span></a></h3><p>基于 <strong>SVD（奇异值分解）</strong> 对文档-词矩阵进行降维，<br> 提取出潜在语义空间。</p><p>[<br> A_{m \\times n} \\approx U_{m \\times k} \\Sigma_{k \\times k} V^T_{k \\times n}<br> ]</p><ul><li>每篇文档、每个词都被表示在一个 k 维语义空间中；</li><li>语义相似的文档、词会在空间中靠近。</li></ul><p><strong>缺点：</strong> 解释性弱、易受噪声影响。</p><hr><h3 id="🧠-3-lda-latent-dirichlet-allocation" tabindex="-1"><a class="header-anchor" href="#🧠-3-lda-latent-dirichlet-allocation"><span>🧠 3. LDA（Latent Dirichlet Allocation）</span></a></h3><p>一种基于概率图模型的主题模型。<br> 假设：</p><ul><li>每篇文档由若干主题混合而成；</li><li>每个主题又由若干词构成。</li></ul><p>LDA 用贝叶斯推断求出文档→主题、主题→词的分布。</p><p><strong>输出结果：</strong></p><ul><li>每篇文档的主题比例；</li><li>每个主题的关键词。</li></ul><p><strong>应用场景：</strong></p><ul><li>新闻聚类与推荐；</li><li>学术论文主题发现；</li><li>舆情话题演变分析。</li></ul><hr><h2 id="_13-4-信息抽取与关键词提取" tabindex="-1"><a class="header-anchor" href="#_13-4-信息抽取与关键词提取"><span><strong>13.4 信息抽取与关键词提取</strong></span></a></h2><h3 id="🧾-1-信息抽取-information-extraction-ie" tabindex="-1"><a class="header-anchor" href="#🧾-1-信息抽取-information-extraction-ie"><span>🧾 1. 信息抽取（Information Extraction, IE）</span></a></h3><p><strong>目标：</strong> 从非结构化文本中提取结构化信息。<br> 如：</p><ul><li>实体识别（人名、地名、机构名）</li><li>关系抽取（“马斯克—创办—特斯拉”）</li><li>事件抽取（“发布新产品”、“签署协议”）</li></ul><p><strong>常见方法：</strong></p><table><thead><tr><th>方法</th><th>描述</th></tr></thead><tbody><tr><td>基于规则</td><td>关键词 + 正则表达式匹配</td></tr><tr><td>条件随机场（CRF）</td><td>序列标注的经典算法</td></tr><tr><td>SVM / 最大熵模型</td><td>逐词分类</td></tr><tr><td>深度模型</td><td>BiLSTM-CRF, BERT-NER</td></tr></tbody></table><p>💡 <em>信息抽取是 NLP 从“阅读”走向“理解”的关键一步。</em></p><hr><h3 id="🔑-2-关键词提取-keyword-extraction" tabindex="-1"><a class="header-anchor" href="#🔑-2-关键词提取-keyword-extraction"><span>🔑 2. 关键词提取（Keyword Extraction）</span></a></h3><p><strong>目标：</strong> 自动识别一篇文档中最能代表主题的词语。</p><p><strong>常见算法：</strong></p><table><thead><tr><th>算法</th><th>核心思想</th></tr></thead><tbody><tr><td>TF-IDF</td><td>高频且区分度高的词</td></tr><tr><td>TextRank</td><td>基于词共现的图排序算法（类似 PageRank）</td></tr><tr><td>RAKE</td><td>关键词候选短语的共现统计</td></tr><tr><td>YAKE</td><td>无需语料统计的单文档算法</td></tr></tbody></table><p><strong>应用：</strong></p><ul><li>搜索引擎摘要生成</li><li>文档索引与聚类</li><li>舆情与内容推荐</li></ul><hr><h2 id="✅-本章小结" tabindex="-1"><a class="header-anchor" href="#✅-本章小结"><span>✅ 本章小结</span></a></h2><table><thead><tr><th>模块</th><th>核心目标</th><th>典型算法</th><th>关键词</th></tr></thead><tbody><tr><td>文本预处理与向量化</td><td>将语言转为数字</td><td>TF-IDF、Word2Vec</td><td>表示学习</td></tr><tr><td>文本分类与情感分析</td><td>识别类别或情感倾向</td><td>Naive Bayes、SVM</td><td>监督学习</td></tr><tr><td>主题模型</td><td>自动发现隐藏主题</td><td>LDA、LSA</td><td>无监督学习</td></tr><tr><td>信息抽取</td><td>把文本变成结构化数据</td><td>CRF、TextRank</td><td>NLP 落地实践</td></tr></tbody></table>',92)])])}const p=e(i,[["render",o]]),h=JSON.parse('{"path":"/posts/ml/2025-11-03-13-ml-book-nlp-inaction.html","title":"第13章　NLP 领域的机器学习实践","lang":"zh-CN","frontmatter":{"title":"第13章　NLP 领域的机器学习实践","date":"2025-11-03T00:00:00.000Z","categories":["AI"],"tags":["ai","learn-note"],"published":true,"description":"第13章 NLP 领域的机器学习实践 第 13 章「NLP 领域的机器学习实践」是整个机器学习体系中最具代表性的实战篇章之一。 自然语言处理（Natural Language Processing, NLP）是机器学习在真实世界中最早、也最广泛的落地场景之一。 本章将从数据预处理、表示方法到经典任务与模型，系统地讲解传统机器学习如何“理解文字”。 第1...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"第13章　NLP 领域的机器学习实践\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-11-03T00:00:00.000Z\\",\\"dateModified\\":\\"2025-12-27T05:15:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"老马啸西风\\",\\"url\\":\\"https://houbb.github.io\\"}]}"],["meta",{"property":"og:url","content":"https://houbb.github.io/awesome-ai-coding/posts/ml/2025-11-03-13-ml-book-nlp-inaction.html"}],["meta",{"property":"og:site_name","content":"老马啸西风"}],["meta",{"property":"og:title","content":"第13章　NLP 领域的机器学习实践"}],["meta",{"property":"og:description","content":"第13章 NLP 领域的机器学习实践 第 13 章「NLP 领域的机器学习实践」是整个机器学习体系中最具代表性的实战篇章之一。 自然语言处理（Natural Language Processing, NLP）是机器学习在真实世界中最早、也最广泛的落地场景之一。 本章将从数据预处理、表示方法到经典任务与模型，系统地讲解传统机器学习如何“理解文字”。 第1..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-27T05:15:15.000Z"}],["meta",{"property":"article:tag","content":"learn-note"}],["meta",{"property":"article:tag","content":"ai"}],["meta",{"property":"article:published_time","content":"2025-11-03T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-27T05:15:15.000Z"}]]},"git":{"createdTime":1766812269000,"updatedTime":1766812515000,"contributors":[{"name":"bbhou","username":"bbhou","email":"1557740299@qq.com","commits":3,"url":"https://github.com/bbhou"}]},"readingTime":{"minutes":5.51,"words":1654},"filePathRelative":"posts/ml/2025-11-03-13-ml-book-nlp-inaction.md","excerpt":"\\n<p>第 13 章「<strong>NLP 领域的机器学习实践</strong>」是整个机器学习体系中最具代表性的实战篇章之一。</p>\\n<p>自然语言处理（Natural Language Processing, NLP）是机器学习在真实世界中最早、也最广泛的落地场景之一。</p>\\n<p>本章将从数据预处理、表示方法到经典任务与模型，系统地讲解传统机器学习如何“理解文字”。</p>\\n<h1>第13章　NLP 领域的机器学习实践</h1>\\n<p>语言是人类智慧的外化，而 NLP 就是让机器理解、生成、归纳语言规律的科学。</p>\\n<p>虽然如今的 NLP 已进入深度学习与大模型时代，但传统机器学习方法依然是 NLP 的理论基石与工业入门方案。</p>","autoDesc":true}');export{p as comp,h as data};
