import{_ as d}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as r,a as e,o as a}from"./app-Br9pe4tt.js";const n={};function o(h,t){return a(),r("div",null,[...t[0]||(t[0]=[e('<h1 id="chat" tabindex="-1"><a class="header-anchor" href="#chat"><span>chat</span></a></h1><h2 id="ai-发展历史" tabindex="-1"><a class="header-anchor" href="#ai-发展历史"><span>AI 发展历史</span></a></h2><p>理解 <strong>机器学习 → 深度学习 → 大语言模型（LLM）</strong> 的发展历史，能帮你从“全景视角”看清整个 AI 领域的演化逻辑。</p><p>下面我会系统梳理从 1950s 到 2025 年的技术脉络，包括关键人物、重要论文、标志性模型和时代转折点。</p><p>我们可以把整个历程分成 <strong>七个时代</strong>。</p><hr><h1 id="🧭-一、ai-全景时间线-1950s–2025" tabindex="-1"><a class="header-anchor" href="#🧭-一、ai-全景时间线-1950s–2025"><span>🧭 一、AI 全景时间线（1950s–2025）</span></a></h1><table><thead><tr><th>时代</th><th>时间</th><th>代表阶段</th><th>核心特征</th></tr></thead><tbody><tr><td>🌱 1. 萌芽期</td><td>1950–1980</td><td>早期人工智能 / 符号主义</td><td>基于逻辑和规则的“推理式 AI”</td></tr><tr><td>🧩 2. 统计学习期</td><td>1980–2000</td><td>传统机器学习崛起</td><td>统计方法 + 特征工程</td></tr><tr><td>🔥 3. 深度学习复兴期</td><td>2006–2012</td><td>神经网络复兴</td><td>多层神经网络训练突破</td></tr><tr><td>🚀 4. 深度学习爆发期</td><td>2012–2018</td><td>图像识别 / 语音 / NLP 爆发</td><td>CNN、RNN、LSTM、Seq2Seq、Attention</td></tr><tr><td>🧠 5. Transformer 时代</td><td>2017–2020</td><td>“Attention is All You Need”</td><td>序列建模范式变革</td></tr><tr><td>🌍 6. LLM 时代</td><td>2020–2023</td><td>GPT、BERT、T5</td><td>预训练 + 指令微调</td></tr><tr><td>🤖 7. Agent &amp; Multimodal 时代</td><td>2023–2025+</td><td>GPT-4、Gemini、Claude、Mistral</td><td>智能体、多模态、推理与工具使用</td></tr></tbody></table><hr><h2 id="🌱-第一阶段-人工智能萌芽期-1950s–1980" tabindex="-1"><a class="header-anchor" href="#🌱-第一阶段-人工智能萌芽期-1950s–1980"><span>🌱 第一阶段：人工智能萌芽期（1950s–1980）</span></a></h2><h3 id="🧩-核心思想" tabindex="-1"><a class="header-anchor" href="#🧩-核心思想"><span>🧩 核心思想</span></a></h3><ul><li><strong>符号主义（Symbolic AI）</strong>：人类知识 → 逻辑规则 → 推理机。</li><li>AI = 知识 + 规则 + 演绎推理。</li></ul><h3 id="🕰️-关键事件" tabindex="-1"><a class="header-anchor" href="#🕰️-关键事件"><span>🕰️ 关键事件</span></a></h3><table><thead><tr><th>年份</th><th>事件</th></tr></thead><tbody><tr><td>1950</td><td>Alan Turing 提出《Computing Machinery and Intelligence》，提出“图灵测试”。</td></tr><tr><td>1956</td><td>达特茅斯会议（Dartmouth Conference），AI 正式成为一门学科。</td></tr><tr><td>1960s</td><td>出现第一个自然语言系统 ELIZA。</td></tr><tr><td>1970s</td><td>基于规则的专家系统兴起（如 MYCIN、DENDRAL）。</td></tr></tbody></table><h3 id="💡-局限" tabindex="-1"><a class="header-anchor" href="#💡-局限"><span>💡 局限</span></a></h3><ul><li>依赖人工编写规则；</li><li>无法应对复杂、模糊、海量数据；</li><li>被称为“<strong>第一次 AI 寒冬</strong>”（1974–1980）。</li></ul><hr><h2 id="🧮-第二阶段-统计机器学习时代-1980–2000" tabindex="-1"><a class="header-anchor" href="#🧮-第二阶段-统计机器学习时代-1980–2000"><span>🧮 第二阶段：统计机器学习时代（1980–2000）</span></a></h2><h3 id="🧠-核心转变" tabindex="-1"><a class="header-anchor" href="#🧠-核心转变"><span>🧠 核心转变</span></a></h3><p>AI 从“规则推理” → “数据驱动”</p><blockquote><p>机器学习开始用数学模型、统计概率去自动学习规律。</p></blockquote><h3 id="🏗️-代表算法与里程碑" tabindex="-1"><a class="header-anchor" href="#🏗️-代表算法与里程碑"><span>🏗️ 代表算法与里程碑</span></a></h3><table><thead><tr><th>年份</th><th>算法 / 模型</th><th>贡献</th></tr></thead><tbody><tr><td>1980</td><td>感知机（Perceptron）重新被研究</td><td>早期神经网络雏形</td></tr><tr><td>1986</td><td>反向传播（Backpropagation）算法提出</td><td>多层网络可训练</td></tr><tr><td>1990</td><td>SVM（支持向量机）</td><td>强大的分类器，泛化能力出色</td></tr><tr><td>1995</td><td>随机森林（Random Forest）</td><td>集成学习的代表作</td></tr><tr><td>1998</td><td>LeNet-5（Yann LeCun）</td><td>手写数字识别奠基之作</td></tr><tr><td>1999</td><td>Boosting 提出（AdaBoost）</td><td>弱分类器组合成强分类器</td></tr></tbody></table><h3 id="⚙️-技术特征" tabindex="-1"><a class="header-anchor" href="#⚙️-技术特征"><span>⚙️ 技术特征</span></a></h3><ul><li>核心算法：SVM、KNN、Naive Bayes、决策树、随机森林、XGBoost。</li><li>特征工程是关键（人工提取特征）。</li><li>工具：Scikit-learn、Weka、MATLAB。</li></ul><h3 id="📊-应用典型" tabindex="-1"><a class="header-anchor" href="#📊-应用典型"><span>📊 应用典型</span></a></h3><ul><li>垃圾邮件分类（Naive Bayes）</li><li>信贷评分模型（Logistic Regression）</li><li>医学诊断</li><li>推荐系统雏形（协同过滤）</li></ul><hr><h2 id="🔥-第三阶段-深度学习复兴-2006–2012" tabindex="-1"><a class="header-anchor" href="#🔥-第三阶段-深度学习复兴-2006–2012"><span>🔥 第三阶段：深度学习复兴（2006–2012）</span></a></h2><h3 id="🧩-背景" tabindex="-1"><a class="header-anchor" href="#🧩-背景"><span>🧩 背景</span></a></h3><ul><li>神经网络在 1990s 被弃用（难训练、过拟合、计算力不足）。</li><li>2006 年 Geoffrey Hinton 提出“深度置信网络（DBN）”，用 <strong>逐层预训练</strong> 解决梯度消失问题。</li></ul><h3 id="🧠-关键事件" tabindex="-1"><a class="header-anchor" href="#🧠-关键事件"><span>🧠 关键事件</span></a></h3><table><thead><tr><th>年份</th><th>事件</th></tr></thead><tbody><tr><td>2006</td><td>Hinton 发表《Reducing the Dimensionality of Data with Neural Networks》，引爆深度学习。</td></tr><tr><td>2009</td><td>Google 开始研究大规模神经网络。</td></tr><tr><td>2010</td><td>GPU 计算普及，使得神经网络能大规模训练。</td></tr><tr><td>2011</td><td>IBM Watson 赢得《Jeopardy!》电视问答赛。</td></tr></tbody></table><h3 id="🔑-技术特点" tabindex="-1"><a class="header-anchor" href="#🔑-技术特点"><span>🔑 技术特点</span></a></h3><ul><li>层数更深的神经网络；</li><li>非线性激活函数（ReLU）；</li><li>Dropout、BatchNorm 等防过拟合技术；</li><li>数据集开始爆炸增长（ImageNet、MNIST）。</li></ul><hr><h2 id="🚀-第四阶段-深度学习爆发期-2012–2018" tabindex="-1"><a class="header-anchor" href="#🚀-第四阶段-深度学习爆发期-2012–2018"><span>🚀 第四阶段：深度学习爆发期（2012–2018）</span></a></h2><h3 id="⚡-关键事件与模型" tabindex="-1"><a class="header-anchor" href="#⚡-关键事件与模型"><span>⚡ 关键事件与模型</span></a></h3><table><thead><tr><th>年份</th><th>模型 / 论文</th><th>里程碑</th></tr></thead><tbody><tr><td>2012</td><td><strong>AlexNet</strong>（ImageNet 冠军）</td><td>GPU + CNN 引爆计算机视觉革命</td></tr><tr><td>2013</td><td><strong>Word2Vec</strong>（Google）</td><td>分布式词向量改变 NLP 方向</td></tr><tr><td>2014</td><td><strong>GAN</strong>（Goodfellow）</td><td>生成对抗网络开创生成式建模</td></tr><tr><td>2014</td><td><strong>Seq2Seq</strong>（Google）</td><td>奠定机器翻译和 NLP 序列模型基础</td></tr><tr><td>2015</td><td><strong>ResNet</strong>（微软）</td><td>深度 CNN 结构突破（152 层）</td></tr><tr><td>2016</td><td><strong>AlphaGo</strong> 战胜李世石</td><td>深度强化学习的里程碑</td></tr><tr><td>2017</td><td><strong>Attention is All You Need</strong></td><td>Transformer 横空出世，NLP 进入新时代</td></tr></tbody></table><h3 id="🧠-技术体系" tabindex="-1"><a class="header-anchor" href="#🧠-技术体系"><span>🧠 技术体系</span></a></h3><ul><li><strong>CNN（卷积神经网络）</strong> → 图像识别、目标检测；</li><li><strong>RNN/LSTM/GRU</strong> → 序列任务（语音、文本）；</li><li><strong>Seq2Seq + Attention</strong> → 机器翻译；</li><li><strong>GAN</strong> → 生成图像、风格迁移；</li><li><strong>Deep Reinforcement Learning</strong> → 决策、游戏、机器人。</li></ul><hr><h2 id="🧠-第五阶段-transformer-时代-2017–2020" tabindex="-1"><a class="header-anchor" href="#🧠-第五阶段-transformer-时代-2017–2020"><span>🧠 第五阶段：Transformer 时代（2017–2020）</span></a></h2><h3 id="🧩-标志事件" tabindex="-1"><a class="header-anchor" href="#🧩-标志事件"><span>🧩 标志事件</span></a></h3><blockquote><p>2017 年，《Attention is All You Need》提出 Transformer，取代循环结构（RNN/LSTM）。</p></blockquote><h3 id="🏗️-代表模型" tabindex="-1"><a class="header-anchor" href="#🏗️-代表模型"><span>🏗️ 代表模型</span></a></h3><table><thead><tr><th>年份</th><th>模型</th><th>贡献</th></tr></thead><tbody><tr><td>2018</td><td>BERT（Google）</td><td>双向 Transformer，开启预训练 + 微调范式</td></tr><tr><td>2019</td><td>GPT-2（OpenAI）</td><td>大规模自回归语言模型</td></tr><tr><td>2019</td><td>XLNet、RoBERTa、ALBERT</td><td>提升 BERT 效果</td></tr><tr><td>2020</td><td>T5（Google）</td><td>“一切任务皆文本”的理念</td></tr></tbody></table><h3 id="💡-技术革命" tabindex="-1"><a class="header-anchor" href="#💡-技术革命"><span>💡 技术革命</span></a></h3><ul><li>从“任务特定训练” → “预训练 + 微调”；</li><li>从“统计特征” → “语义理解”；</li><li>大规模语料 + 迁移学习 → 泛化性能极强。</li></ul><hr><h2 id="🌍-第六阶段-大语言模型-llm-时代-2020–2023" tabindex="-1"><a class="header-anchor" href="#🌍-第六阶段-大语言模型-llm-时代-2020–2023"><span>🌍 第六阶段：大语言模型（LLM）时代（2020–2023）</span></a></h2><h3 id="⚙️-关键技术突破" tabindex="-1"><a class="header-anchor" href="#⚙️-关键技术突破"><span>⚙️ 关键技术突破</span></a></h3><ol><li><strong>自监督学习（Self-Supervised Learning）</strong><br> 模型从海量文本中“自己学语言”；</li><li><strong>In-context Learning（上下文学习）</strong><br> 模型可在对话中“即学即用”；</li><li><strong>指令微调（Instruction Tuning）</strong><br> 模型能听懂人类指令；</li><li><strong>RLHF（人类反馈强化学习）</strong><br> 通过人类偏好让模型更“对齐人类价值”。</li></ol><h3 id="🚀-关键模型时间线" tabindex="-1"><a class="header-anchor" href="#🚀-关键模型时间线"><span>🚀 关键模型时间线</span></a></h3><table><thead><tr><th>年份</th><th>模型</th><th>开发方</th><th>特点</th></tr></thead><tbody><tr><td>2020</td><td>GPT-3</td><td>OpenAI</td><td>175B 参数，强泛化能力</td></tr><tr><td>2021</td><td>Codex</td><td>OpenAI</td><td>代码生成能力（GitHub Copilot）</td></tr><tr><td>2022</td><td>ChatGPT</td><td>OpenAI</td><td>RLHF 对齐 + 对话式体验</td></tr><tr><td>2022</td><td>PaLM</td><td>Google</td><td>大规模多任务语言模型</td></tr><tr><td>2023</td><td>GPT-4</td><td>OpenAI</td><td>多模态理解、推理增强</td></tr><tr><td>2023</td><td>Claude / Gemini / Mistral</td><td>Anthropic / Google / 法国团队</td><td>开放生态繁荣</td></tr></tbody></table><hr><h2 id="🤖-第七阶段-agent-多模态智能-2023–2025" tabindex="-1"><a class="header-anchor" href="#🤖-第七阶段-agent-多模态智能-2023–2025"><span>🤖 第七阶段：Agent + 多模态智能（2023–2025）</span></a></h2><h3 id="🧩-特征" tabindex="-1"><a class="header-anchor" href="#🧩-特征"><span>🧩 特征</span></a></h3><blockquote><p>LLM 不再只是语言模型，而是成为“通用智能体”的核心。</p></blockquote><h3 id="🔑-技术方向" tabindex="-1"><a class="header-anchor" href="#🔑-技术方向"><span>🔑 技术方向</span></a></h3><table><thead><tr><th>方向</th><th>描述</th></tr></thead><tbody><tr><td><strong>AI Agent</strong></td><td>模型具备长期记忆、目标规划、调用工具能力（如 AutoGPT、LangChain、OpenDevin）</td></tr><tr><td><strong>多模态（Multimodal）</strong></td><td>文本 + 图像 + 音频 + 视频（GPT-4V、Gemini 1.5）</td></tr><tr><td><strong>推理与工具调用（Tool Use）</strong></td><td>模型能调用 API、执行代码、检索知识</td></tr><tr><td><strong>模型压缩 / 私有部署</strong></td><td>LLaMA、Mistral、Qwen、Yi 系列推动开源浪潮</td></tr><tr><td><strong>可解释与安全对齐（Alignment &amp; Safety）</strong></td><td>Anthropic、OpenAI 专注模型价值对齐</td></tr></tbody></table><hr><h2 id="🧭-总结-ai-三次浪潮" tabindex="-1"><a class="header-anchor" href="#🧭-总结-ai-三次浪潮"><span>🧭 总结：AI 三次浪潮</span></a></h2><table><thead><tr><th>阶段</th><th>时间</th><th>核心驱动力</th><th>代表人物 / 模型</th></tr></thead><tbody><tr><td><strong>第一次浪潮：符号主义 AI</strong></td><td>1950–1980</td><td>规则与逻辑</td><td>Turing、Minsky、MYCIN</td></tr><tr><td><strong>第二次浪潮：机器学习</strong></td><td>1980–2010</td><td>数据与统计</td><td>Vapnik、Cortes（SVM）、Breiman（RF）</td></tr><tr><td><strong>第三次浪潮：深度学习 &amp; LLM</strong></td><td>2010–现在</td><td>计算力 + 大数据 + Transformer</td><td>Hinton、LeCun、Bengio、OpenAI、Google DeepMind</td></tr></tbody></table><hr><h2 id="🧩-延伸阅读建议" tabindex="-1"><a class="header-anchor" href="#🧩-延伸阅读建议"><span>🧩 延伸阅读建议</span></a></h2><table><thead><tr><th>主题</th><th>推荐资源</th></tr></thead><tbody><tr><td>机器学习历史</td><td>《The Master Algorithm》 by Pedro Domingos</td></tr><tr><td>深度学习革命</td><td>《Deep Learning》 by Ian Goodfellow</td></tr><tr><td>Transformer/LLM 演化</td><td>《Attention Is All You Need》、OpenAI Blog、Google Research Papers</td></tr><tr><td>AI 时代思考</td><td>《Architects of Intelligence》（Martin Ford）</td></tr></tbody></table>',67)])])}const l=d(n,[["render",o]]),p=JSON.parse('{"path":"/posts/learn-llms/2025-11-03-dive-into-llms-100-ml-history.html","title":"dive-into-llms-104-给出从机器学习、深度学习、LLM 从开始到现在的整个发展历史","lang":"zh-CN","frontmatter":{"title":"dive-into-llms-104-给出从机器学习、深度学习、LLM 从开始到现在的整个发展历史","date":"2025-11-03T00:00:00.000Z","categories":["AI"],"tags":["ai","learn-note"],"published":true,"description":"chat AI 发展历史 理解 机器学习 → 深度学习 → 大语言模型（LLM） 的发展历史，能帮你从“全景视角”看清整个 AI 领域的演化逻辑。 下面我会系统梳理从 1950s 到 2025 年的技术脉络，包括关键人物、重要论文、标志性模型和时代转折点。 我们可以把整个历程分成 七个时代。 🧭 一、AI 全景时间线（1950s–2025） 🌱 第...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"dive-into-llms-104-给出从机器学习、深度学习、LLM 从开始到现在的整个发展历史\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-11-03T00:00:00.000Z\\",\\"dateModified\\":\\"2025-12-27T05:15:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"老马啸西风\\",\\"url\\":\\"https://houbb.github.io\\"}]}"],["meta",{"property":"og:url","content":"https://houbb.github.io/awesome-ai-coding/posts/learn-llms/2025-11-03-dive-into-llms-100-ml-history.html"}],["meta",{"property":"og:site_name","content":"老马啸西风"}],["meta",{"property":"og:title","content":"dive-into-llms-104-给出从机器学习、深度学习、LLM 从开始到现在的整个发展历史"}],["meta",{"property":"og:description","content":"chat AI 发展历史 理解 机器学习 → 深度学习 → 大语言模型（LLM） 的发展历史，能帮你从“全景视角”看清整个 AI 领域的演化逻辑。 下面我会系统梳理从 1950s 到 2025 年的技术脉络，包括关键人物、重要论文、标志性模型和时代转折点。 我们可以把整个历程分成 七个时代。 🧭 一、AI 全景时间线（1950s–2025） 🌱 第..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-27T05:15:15.000Z"}],["meta",{"property":"article:tag","content":"learn-note"}],["meta",{"property":"article:tag","content":"ai"}],["meta",{"property":"article:published_time","content":"2025-11-03T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-27T05:15:15.000Z"}]]},"git":{"createdTime":1766812269000,"updatedTime":1766812515000,"contributors":[{"name":"bbhou","username":"bbhou","email":"1557740299@qq.com","commits":3,"url":"https://github.com/bbhou"}]},"readingTime":{"minutes":5.68,"words":1703},"filePathRelative":"posts/learn-llms/2025-11-03-dive-into-llms-100-ml-history.md","excerpt":"\\n<h2>AI 发展历史</h2>\\n<p>理解 <strong>机器学习 → 深度学习 → 大语言模型（LLM）</strong> 的发展历史，能帮你从“全景视角”看清整个 AI 领域的演化逻辑。</p>\\n<p>下面我会系统梳理从 1950s 到 2025 年的技术脉络，包括关键人物、重要论文、标志性模型和时代转折点。</p>\\n<p>我们可以把整个历程分成 <strong>七个时代</strong>。</p>\\n<hr>\\n<h1>🧭 一、AI 全景时间线（1950s–2025）</h1>\\n<table>\\n<thead>\\n<tr>\\n<th>时代</th>\\n<th>时间</th>\\n<th>代表阶段</th>\\n<th>核心特征</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>🌱 1. 萌芽期</td>\\n<td>1950–1980</td>\\n<td>早期人工智能 / 符号主义</td>\\n<td>基于逻辑和规则的“推理式 AI”</td>\\n</tr>\\n<tr>\\n<td>🧩 2. 统计学习期</td>\\n<td>1980–2000</td>\\n<td>传统机器学习崛起</td>\\n<td>统计方法 + 特征工程</td>\\n</tr>\\n<tr>\\n<td>🔥 3. 深度学习复兴期</td>\\n<td>2006–2012</td>\\n<td>神经网络复兴</td>\\n<td>多层神经网络训练突破</td>\\n</tr>\\n<tr>\\n<td>🚀 4. 深度学习爆发期</td>\\n<td>2012–2018</td>\\n<td>图像识别 / 语音 / NLP 爆发</td>\\n<td>CNN、RNN、LSTM、Seq2Seq、Attention</td>\\n</tr>\\n<tr>\\n<td>🧠 5. Transformer 时代</td>\\n<td>2017–2020</td>\\n<td>“Attention is All You Need”</td>\\n<td>序列建模范式变革</td>\\n</tr>\\n<tr>\\n<td>🌍 6. LLM 时代</td>\\n<td>2020–2023</td>\\n<td>GPT、BERT、T5</td>\\n<td>预训练 + 指令微调</td>\\n</tr>\\n<tr>\\n<td>🤖 7. Agent &amp; Multimodal 时代</td>\\n<td>2023–2025+</td>\\n<td>GPT-4、Gemini、Claude、Mistral</td>\\n<td>智能体、多模态、推理与工具使用</td>\\n</tr>\\n</tbody>\\n</table>","autoDesc":true}');export{l as comp,p as data};
