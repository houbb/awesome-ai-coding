import{_ as s}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as n,a,o as t}from"./app-BiyrO2g_.js";const i={};function r(l,e){return t(),n("div",null,[...e[0]||(e[0]=[a(`<h1 id="ai编程有一个问题-如何减少上下文。提升-ai-专注力" tabindex="-1"><a class="header-anchor" href="#ai编程有一个问题-如何减少上下文。提升-ai-专注力"><span>AI编程有一个问题，如何减少上下文。提升 ai 专注力？</span></a></h1><p>这是 <strong>AI 编程实践里一个“核心但常被低估”的问题</strong>。</p><p>本质上你问的不是“上下文怎么少”，而是：</p><blockquote><p><strong>如何把“对当前任务真正有用的信息密度”拉到极高，同时把无关噪音降到极低，从而让模型在有限注意力预算内保持专注。</strong></p></blockquote><p>下面我从<strong>原理 → 方法 → 可操作机制 → 你当前实践的进阶建议</strong>四个层面系统回答。</p><hr><h2 id="一、先说清本质-为什么-上下文多-反而让-ai-变笨" tabindex="-1"><a class="header-anchor" href="#一、先说清本质-为什么-上下文多-反而让-ai-变笨"><span>一、先说清本质：为什么“上下文多”反而让 AI 变笨</span></a></h2><p>LLM 的注意力并不是“越多越好”，而是：</p><ul><li>有 <strong>固定的注意力预算</strong></li><li>上下文越长 → <strong>每一条信息分到的权重越低</strong></li><li>模型会自动做“概率性取舍”，而不是“逻辑筛选”</li></ul><p>因此当你给 AI：</p><ul><li>README</li><li><a href="http://DESIGN.md" target="_blank" rel="noopener noreferrer">DESIGN.md</a></li><li>历史代码</li><li>一堆无关但“看起来相关”的文件</li></ul><p>模型实际上会进入一种状态：</p><blockquote><p><strong>“我知道很多，但我不知道现在最重要的是什么”</strong></p></blockquote><p>结果就是你看到的：</p><ul><li>实现发散</li><li>忽略关键约束</li><li>自作主张补逻辑</li></ul><hr><h2 id="二、减少上下文-删文件-而是「上下文分层-权重化」" tabindex="-1"><a class="header-anchor" href="#二、减少上下文-删文件-而是「上下文分层-权重化」"><span>二、减少上下文 ≠ 删文件，而是「上下文分层 + 权重化」</span></a></h2><p>一个非常关键的认知转变：</p><blockquote><p>❌ 少给上下文<br> ✅ <strong>让上下文有“层级、角色和优先级”</strong></p></blockquote><h3 id="_1️⃣-把上下文分成-4-层-强烈推荐" tabindex="-1"><a class="header-anchor" href="#_1️⃣-把上下文分成-4-层-强烈推荐"><span>1️⃣ 把上下文分成 4 层（强烈推荐）</span></a></h3><table><thead><tr><th>层级</th><th>作用</th><th>是否常驻</th></tr></thead><tbody><tr><td><strong>L0：执行指令层</strong></td><td>当前任务“必须遵守的规则”</td><td>✅ 必须</td></tr><tr><td><strong>L1：任务输入层</strong></td><td>当前要做的事情</td><td>✅ 必须</td></tr><tr><td><strong>L2：参考理解层</strong></td><td>帮助理解，但不直接执行</td><td>⚠️ 按需</td></tr><tr><td><strong>L3：历史背景层</strong></td><td>仅用于回溯</td><td>❌ 默认不加载</td></tr></tbody></table><p><strong>绝大多数 AI 编程失败，是 L2 / L3 抢了 L0 / L1 的注意力。</strong></p><hr><h2 id="三、实战方法-如何让-ai-只看该看的" tabindex="-1"><a class="header-anchor" href="#三、实战方法-如何让-ai-只看该看的"><span>三、实战方法：如何让 AI“只看该看的”</span></a></h2><h3 id="方法一-用「显式上下文裁剪指令」" tabindex="-1"><a class="header-anchor" href="#方法一-用「显式上下文裁剪指令」"><span>方法一：用「显式上下文裁剪指令」</span></a></h3><p>在 system / prompt 开头直接声明：</p><div class="language-text line-numbers-mode" data-highlighter="shiki" data-ext="text" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-text"><span class="line"><span>上下文注意力规则：</span></span>
<span class="line"><span>1. 仅将【L0 执行约束】与【L1 当前任务】作为强约束</span></span>
<span class="line"><span>2. README / DESIGN / 历史代码仅用于理解，不得引入未明确声明的新行为</span></span>
<span class="line"><span>3. 若信息冲突，以 L0 &gt; L1 &gt; L2 的优先级处理</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>这不是废话</strong>，这是在“教模型如何分配注意力权重”。</p><hr><h3 id="方法二-把-上下文-变成-索引-而不是全文" tabindex="-1"><a class="header-anchor" href="#方法二-把-上下文-变成-索引-而不是全文"><span>方法二：把“上下文”变成“索引”，而不是全文</span></a></h3><p>❌ 常见错误：</p><blockquote><p>把 <a href="http://DESIGN.md" target="_blank" rel="noopener noreferrer">DESIGN.md</a> 整个贴进去</p></blockquote><p>✅ 正确做法：</p><div class="language-text line-numbers-mode" data-highlighter="shiki" data-ext="text" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-text"><span class="line"><span>可参考文档索引：</span></span>
<span class="line"><span>- DESIGN.md</span></span>
<span class="line"><span>  - 2.3 数据流（仅用于理解字段含义）</span></span>
<span class="line"><span>  - 4.1 错误处理原则（仅在涉及异常时使用）</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>👉 <strong>你是在告诉 AI：哪些信息“可能被用到”，而不是“全部要用”。</strong></p><hr><h3 id="方法三-使用「上下文冻结-context-freeze-」机制" tabindex="-1"><a class="header-anchor" href="#方法三-使用「上下文冻结-context-freeze-」机制"><span>方法三：使用「上下文冻结（Context Freeze）」机制</span></a></h3><p>在任务中段明确声明：</p><div class="language-text line-numbers-mode" data-highlighter="shiki" data-ext="text" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-text"><span class="line"><span>从现在开始：</span></span>
<span class="line"><span>- 你只能基于已确认的设计和当前代码修改</span></span>
<span class="line"><span>- 不允许引入新的概念、字段、流程</span></span>
<span class="line"><span>- 若发现信息不足，必须停下来提出问题</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这一步对 <strong>防止中后段发散</strong> 极其有效。</p><hr><h3 id="方法四-拆-一个任务-为-多个注意力窗口" tabindex="-1"><a class="header-anchor" href="#方法四-拆-一个任务-为-多个注意力窗口"><span>方法四：拆“一个任务”为“多个注意力窗口”</span></a></h3><p>与其一次性让 AI：</p><blockquote><p>理解系统 + 设计方案 + 写代码</p></blockquote><p>不如强制它分阶段：</p><ol><li><p><strong>理解阶段</strong></p><ul><li>输出：<code>我对系统的理解清单</code></li></ul></li><li><p><strong>澄清阶段</strong></p><ul><li>输出：<code>不确定点 / 歧义点</code></li></ul></li><li><p><strong>执行阶段</strong></p><ul><li>输入：<code>确认后的最小执行上下文</code></li></ul></li><li><p><strong>校验阶段</strong></p><ul><li>对照清单逐条检查</li></ul></li></ol><p>这本质是在<strong>人为模拟“多次注意力聚焦”</strong>。</p><hr><h2 id="四、你当前实践的-进阶优化建议-结合你之前的做法" tabindex="-1"><a class="header-anchor" href="#四、你当前实践的-进阶优化建议-结合你之前的做法"><span>四、你当前实践的“进阶优化建议”（结合你之前的做法）</span></a></h2><p>你现在的体系里已经有非常好的基础：</p><ul><li><code>GEMINI.md</code> → <strong>全局约束</strong></li><li><code>PROJECT_PROFILE.md</code> → <strong>系统认知</strong></li><li><code>DESIGN.md（AI 可执行版）</code> → <strong>结构化任务输入</strong></li></ul><p>问题只剩一个：</p><blockquote><p><strong>这些文件现在对 AI 来说“权重是相同的”</strong></p></blockquote><h3 id="建议你再加-2-个-专门用来降噪-的文件" tabindex="-1"><a class="header-anchor" href="#建议你再加-2-个-专门用来降噪-的文件"><span>建议你再加 2 个“专门用来降噪”的文件</span></a></h3><hr><h3 id="_1️⃣-ai-execution-context-md-最小执行上下文" tabindex="-1"><a class="header-anchor" href="#_1️⃣-ai-execution-context-md-最小执行上下文"><span>1️⃣ <code>AI_EXECUTION_CONTEXT.md</code>（最小执行上下文）</span></a></h3><p>只允许包含：</p><ul><li>本次要改的模块</li><li>明确允许修改的文件列表</li><li>禁止触碰的边界</li><li>成功 / 失败判定标准</li></ul><blockquote><p><strong>这是每一次执行真正要喂给模型的“主食”</strong></p></blockquote><p>其他文档都变成“参考链接”。</p><hr><h3 id="_2️⃣-ai-attention-rules-md-注意力分配规则" tabindex="-1"><a class="header-anchor" href="#_2️⃣-ai-attention-rules-md-注意力分配规则"><span>2️⃣ <code>AI_ATTENTION_RULES.md</code>（注意力分配规则）</span></a></h3><p>示例核心内容：</p><div class="language-text line-numbers-mode" data-highlighter="shiki" data-ext="text" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-text"><span class="line"><span>注意力优先级：</span></span>
<span class="line"><span>P0：本文件 + 当前指令</span></span>
<span class="line"><span>P1：明确标注为【强约束】的设计</span></span>
<span class="line"><span>P2：历史设计，仅供理解</span></span>
<span class="line"><span>P3：历史代码，仅在无歧义时参考</span></span>
<span class="line"><span></span></span>
<span class="line"><span>禁止行为：</span></span>
<span class="line"><span>- 基于历史代码推断新需求</span></span>
<span class="line"><span>- 为“看起来合理”而补充逻辑</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>这是在显式“管控模型的自由度”。</strong></p><hr><h2 id="五、一句话总结-给你一个可复用原则" tabindex="-1"><a class="header-anchor" href="#五、一句话总结-给你一个可复用原则"><span>五、一句话总结（给你一个可复用原则）</span></a></h2><blockquote><p><strong>AI 编程不是“喂更多上下文”，而是“设计注意力分配机制”。</strong><br> 真正高质量的 AI 编程系统，核心能力不是生成代码，而是：</p><p><strong>让模型在任何时刻都清楚：</strong></p><ul><li>我现在在干什么</li><li>哪些信息最重要</li><li>哪些我不能擅自决定</li></ul></blockquote><h1 id="参考资料" tabindex="-1"><a class="header-anchor" href="#参考资料"><span>参考资料</span></a></h1>`,69)])])}const d=s(i,[["render",r]]),c=JSON.parse('{"path":"/posts/sdd/2025-11-20-ai-sdd-32-reduce-context.html","title":"AI SDD 开发规范-32-AI编程有一个问题，如何减少上下文。提升 ai 专注力？","lang":"zh-CN","frontmatter":{"title":"AI SDD 开发规范-32-AI编程有一个问题，如何减少上下文。提升 ai 专注力？","tags":["ai","sdd","sh"],"published":true,"description":"AI编程有一个问题，如何减少上下文。提升 ai 专注力？ 这是 AI 编程实践里一个“核心但常被低估”的问题。 本质上你问的不是“上下文怎么少”，而是： 如何把“对当前任务真正有用的信息密度”拉到极高，同时把无关噪音降到极低，从而让模型在有限注意力预算内保持专注。 下面我从原理 → 方法 → 可操作机制 → 你当前实践的进阶建议四个层面系统回答。 一、...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"AI SDD 开发规范-32-AI编程有一个问题，如何减少上下文。提升 ai 专注力？\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-12-27T07:18:50.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"老马啸西风\\",\\"url\\":\\"https://houbb.github.io\\"}]}"],["meta",{"property":"og:url","content":"https://houbb.github.io/awesome-ai-coding/posts/sdd/2025-11-20-ai-sdd-32-reduce-context.html"}],["meta",{"property":"og:site_name","content":"老马啸西风"}],["meta",{"property":"og:title","content":"AI SDD 开发规范-32-AI编程有一个问题，如何减少上下文。提升 ai 专注力？"}],["meta",{"property":"og:description","content":"AI编程有一个问题，如何减少上下文。提升 ai 专注力？ 这是 AI 编程实践里一个“核心但常被低估”的问题。 本质上你问的不是“上下文怎么少”，而是： 如何把“对当前任务真正有用的信息密度”拉到极高，同时把无关噪音降到极低，从而让模型在有限注意力预算内保持专注。 下面我从原理 → 方法 → 可操作机制 → 你当前实践的进阶建议四个层面系统回答。 一、..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-27T07:18:50.000Z"}],["meta",{"property":"article:tag","content":"sh"}],["meta",{"property":"article:tag","content":"sdd"}],["meta",{"property":"article:tag","content":"ai"}],["meta",{"property":"article:modified_time","content":"2025-12-27T07:18:50.000Z"}]]},"git":{"createdTime":1766819930000,"updatedTime":1766819930000,"contributors":[{"name":"bbhou","username":"bbhou","email":"1557740299@qq.com","commits":1,"url":"https://github.com/bbhou"}]},"readingTime":{"minutes":4.32,"words":1295},"filePathRelative":"posts/sdd/2025-11-20-ai-sdd-32-reduce-context.md","excerpt":"\\n<p>这是 <strong>AI 编程实践里一个“核心但常被低估”的问题</strong>。</p>\\n<p>本质上你问的不是“上下文怎么少”，而是：</p>\\n<blockquote>\\n<p><strong>如何把“对当前任务真正有用的信息密度”拉到极高，同时把无关噪音降到极低，从而让模型在有限注意力预算内保持专注。</strong></p>\\n</blockquote>\\n<p>下面我从<strong>原理 → 方法 → 可操作机制 → 你当前实践的进阶建议</strong>四个层面系统回答。</p>\\n<hr>\\n<h2>一、先说清本质：为什么“上下文多”反而让 AI 变笨</h2>\\n<p>LLM 的注意力并不是“越多越好”，而是：</p>","autoDesc":true}');export{d as comp,c as data};
