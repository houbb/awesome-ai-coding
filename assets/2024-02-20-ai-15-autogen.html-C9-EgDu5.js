import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as i,a as s,o as n}from"./app-Dvf0wTTF.js";const t={};function r(o,a){return n(),i("div",null,[...a[0]||(a[0]=[s(`<h2 id="autogen简介" tabindex="-1"><a class="header-anchor" href="#autogen简介"><span>AutoGen简介</span></a></h2><p>AutoGen 是一个框架，它通过多个可以相互对话的代理实现 LLM（Large Language Model）应用的开发。AutoGen 代理是可定制、可对话的，并且可以无缝地允许人类参与。它们可以以使用 LLM、人类输入和工具的各种模式运行。</p><figure><img src="https://github.com/microsoft/autogen/blob/main/website/static/img/autogen_agentchat.png" alt="AutoGen概览" tabindex="0" loading="lazy"><figcaption>AutoGen概览</figcaption></figure><ul><li>AutoGen 通过 <a href="https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat" target="_blank" rel="noopener noreferrer">多代理对话</a> 实现了下一代 LLM 应用的构建，减少了工作量。它简化了复杂 LLM 工作流的编排、自动化和优化。它最大程度地提高了 LLM 模型的性能，并克服了它们的弱点。</li><li>它支持<a href="https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#supporting-diverse-conversation-patterns" target="_blank" rel="noopener noreferrer">多样化的对话模式</a>用于复杂的工作流程。通过可定制和可对话的代理，开发人员可以使用 AutoGen 构建涵盖对话自主性、代理数量和代理对话拓扑的各种对话模式。</li><li>它提供了一系列具有不同复杂性的工作系统。这些系统涵盖了<a href="https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#diverse-applications-implemented-with-autogen" target="_blank" rel="noopener noreferrer">各种应用领域</a>的广泛应用和复杂性。这表明 AutoGen 如何轻松支持多样化的对话模式。</li><li>AutoGen 提供了<a href="https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification" target="_blank" rel="noopener noreferrer">增强的 LLM 推断</a>。它提供了诸如 API 统一化、缓存和高级用法模式（如错误处理、多配置推断、上下文编程等）等实用程序。</li></ul><p>AutoGen 基于微软、宾州州立大学和华盛顿大学的合作<a href="https://microsoft.github.io/autogen/docs/Research" target="_blank" rel="noopener noreferrer">研究</a>。</p><h2 id="路线图" tabindex="-1"><a class="header-anchor" href="#路线图"><span>路线图</span></a></h2><p>要了解我们正在进行的工作以及计划进行的工作，请查看我们的<a href="https://aka.ms/autogen-roadmap" target="_blank" rel="noopener noreferrer">路线图问题</a>。</p><h2 id="快速开始" tabindex="-1"><a class="header-anchor" href="#快速开始"><span>快速开始</span></a></h2><p>开始玩耍的最简单方法是：</p><ol><li><p>单击以下链接使用 GitHub Codespace</p><figure><a href="https://codespaces.new/microsoft/autogen?quickstart=1" target="_blank" rel="noopener noreferrer"><img src="https://github.com/codespaces/badge.svg" alt="Open in GitHub Codespaces" tabindex="0" loading="lazy"></a><figcaption>Open in GitHub Codespaces</figcaption></figure></li><li><p>将 OAI_CONFIG_LIST_sample 复制到 ./notebook 文件夹中，命名为 OAI_CONFIG_LIST，并设置正确的配置。</p></li><li><p>开始使用笔记本！</p></li></ol><p><em>注意：</em> OAI_CONFIG_LIST_sample 将 GPT-4 列为默认模型，因为这代表了我们当前的建议，并且已知与 AutoGen 配合良好。如果您使用的模型不是 GPT-4，您可能需要修改各种系统提示（特别是如果使用像 GPT-3.5-turbo 这样的较弱模型）。此外，如果使用的模型不是由 OpenAI 或 Azure 托管的模型，则可能会带来与对齐和安全性相关的额外风险。如果更新此默认设置，请谨慎操作。</p><h2 id="安装" tabindex="-1"><a class="header-anchor" href="#安装"><span>安装</span></a></h2><h3 id="选项-1-在-docker-中安装和运行-autogen" tabindex="-1"><a class="header-anchor" href="#选项-1-在-docker-中安装和运行-autogen"><span>选项 1. 在 Docker 中安装和运行 AutoGen</span></a></h3><p>详细说明请参见用户<a href="https://microsoft.github.io/autogen/docs/installation/Docker#step-1-install-docker" target="_blank" rel="noopener noreferrer">这里</a>，以及开发人员<a href="https://microsoft.github.io/autogen/docs/Contribute#docker-for-development" target="_blank" rel="noopener noreferrer">这里</a>。</p><h3 id="选项-2-本地安装-autogen" tabindex="-1"><a class="header-anchor" href="#选项-2-本地安装-autogen"><span>选项 2. 本地安装 AutoGen</span></a></h3><p>AutoGen 需要 <strong>Python 版本 &gt;= 3.8, &lt; 3.13</strong>。可以通过 pip 安装：</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-bash"><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">pip</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> install</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> pyautogen</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>安装了最小依赖项，没有额外选项。您可以根据需要安装其他功能的额外选项。</p><p>有关更多选项，请参见<a href="https://microsoft.github.io/autogen/docs/Installation#option-2-install-autogen-locally-using-virtual-environment" target="_blank" rel="noopener noreferrer">安装文档</a>。</p><h2 id="多代理对话框架" tabindex="-1"><a class="header-anchor" href="#多代理对话框架"><span>多代理对话框架</span></a></h2><p>AutoGen 使用通用的<a href="https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat" target="_blank" rel="noopener noreferrer">多代理对话</a>框架实现了下一代 LLM 应用。它提供了可定制和可对话的代理，这些代理集成了 LLM、工具和人类。<br> 通过在多个能力强大的代理之间自动进行聊天，人们可以轻松地使它们共同完成任务，无论是自主还是通过人类反馈，包括需要使用代码通过工具完成的任务。</p><p>此用例的功能包括：</p><ul><li>**多代</li></ul><p>理对话**：AutoGen 代理可以相互通信以解决任务。这使得可以开发比单个 LLM 更复杂和复杂的应用程序。</p><ul><li><strong>定制化</strong>：AutoGen 代理可以根据应用程序的特定需求进行定制。这包括选择要使用的 LLM、允许的人类输入类型以及要使用的工具。</li><li><strong>人类参与</strong>：AutoGen 可以无缝地允许人类参与。这意味着人类可以根据需要向代理提供输入和反馈。</li></ul><p>例如，</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> autogen </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> AssistantAgent, UserProxyAgent, config_list_from_json</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 从环境变量或文件加载 LLM 推断端点</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 参见 https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 和 OAI_CONFIG_LIST_sample</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">config_list </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> config_list_from_json</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">env_or_file</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;OAI_CONFIG_LIST&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 您也可以直接将 config_list 设置为列表，例如，config_list = [{&#39;model&#39;: &#39;gpt-4&#39;, &#39;api_key&#39;: &#39;&lt;your OpenAI API key here&gt;&#39;},]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">assistant </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> AssistantAgent</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;assistant&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">llm_config</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;config_list&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: config_list})</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">user_proxy </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> UserProxyAgent</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;user_proxy&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">code_execution_config</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;work_dir&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;coding&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;use_docker&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">False</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}) </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 重要提示：建议设置为 True 以在 Docker 中运行代码</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">user_proxy.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">initiate_chat</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(assistant, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">message</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;Plot a chart of NVDA and TESLA stock price change YTD.&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 这将在两个代理之间启动自动对话以解决任务</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>此示例可以通过以下方式运行：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">python test</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">twoagent.py</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>在克隆仓库后。<br> 下图显示了一个使用 AutoGen 的示例对话流程。<br><img src="https://github.com/microsoft/autogen/blob/main/website/static/img/chat_example.png" alt="Agent Chat Example" loading="lazy"></p><p>另外，这里的<a href="https://github.com/microsoft/autogen/blob/main/samples/simple_chat.py" target="_blank" rel="noopener noreferrer">示例代码</a>允许用户以 ChatGPT 风格与 AutoGen 代理进行交互。<br> 请找到更多<a href="https://microsoft.github.io/autogen/docs/Examples#automated-multi-agent-chat" target="_blank" rel="noopener noreferrer">代码示例</a>以获取此功能。</p><h2 id="增强的-llm-推断" tabindex="-1"><a class="header-anchor" href="#增强的-llm-推断"><span>增强的 LLM 推断</span></a></h2><p>AutoGen 还有助于最大限度地利用诸如 ChatGPT 和 GPT-4 等昂贵的 LLM。它提供了<a href="https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification" target="_blank" rel="noopener noreferrer">增强的 LLM 推断</a>，具有诸如缓存、错误处理、多配置推断和模板等功能。</p><h2 id="文档" tabindex="-1"><a class="header-anchor" href="#文档"><span>文档</span></a></h2><p>您可以在<a href="https://microsoft.github.io/autogen/" target="_blank" rel="noopener noreferrer">这里</a>找到有关 AutoGen 的详细文档。</p><p>此外，您还可以找到：</p><ul><li><a href="https://microsoft.github.io/autogen/docs/Research" target="_blank" rel="noopener noreferrer">研究</a>、<a href="https://microsoft.github.io/autogen/blog" target="_blank" rel="noopener noreferrer">博客文章</a>和<a href="https://github.com/microsoft/autogen/blob/main/TRANSPARENCY_FAQS.md" target="_blank" rel="noopener noreferrer">透明度 FAQ</a></li><li><a href="https://aka.ms/autogen-dc" target="_blank" rel="noopener noreferrer">Discord</a></li><li><a href="https://microsoft.github.io/autogen/docs/Contribute" target="_blank" rel="noopener noreferrer">贡献指南</a></li><li><a href="https://github.com/orgs/microsoft/projects/989/views/3" target="_blank" rel="noopener noreferrer">路线图</a></li></ul><h2 id="相关论文" tabindex="-1"><a class="header-anchor" href="#相关论文"><span>相关论文</span></a></h2><p><a href="https://arxiv.org/abs/2308.08155" target="_blank" rel="noopener noreferrer">AutoGen</a></p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>@inproceedings{wu2023autogen,</span></span>
<span class="line"><span>      title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework},</span></span>
<span class="line"><span>      author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Beibin Li and Erkang Zhu and Li Jiang and Xiaoyun Zhang and Shaokun Zhang and Jiale Liu and Ahmed Hassan Awadallah and Ryen W White and Doug Burger and Chi Wang},</span></span>
<span class="line"><span>      year={2023},</span></span>
<span class="line"><span>      eprint={2308.08155},</span></span>
<span class="line"><span>      archivePrefix={arXiv},</span></span>
<span class="line"><span>      primaryClass={cs.AI}</span></span>
<span class="line"><span>}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><a href="https://arxiv.org/abs/2303.04673" target="_blank" rel="noopener noreferrer">EcoOptiGen</a></p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>@inproceedings{wang2023EcoOptiGen,</span></span>
<span class="line"><span>    title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},</span></span>
<span class="line"><span>    author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},</span></span>
<span class="line"><span>    year={2023},</span></span>
<span class="line"><span>    booktitle={AutoML&#39;23},</span></span>
<span class="line"><span>}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><a href="https://arxiv.org/abs/2306.01337" target="_blank" rel="noopener noreferrer">MathChat</a></p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>@inproceedings{wu2023empirical,</span></span>
<span class="line"><span>    title={An Empirical Study on Challenging Math Problem Solving with GPT-4},</span></span>
<span class="line"><span>    author={Yiran Wu and Feiran Jia and Shaokun Zhang and Hangyu Li and Erkang Zhu and Yue Wang and Yin Tat Lee and Richard Peng and Qingyun Wu and Chi Wang},</span></span>
<span class="line"><span>    year={2023},</span></span>
<span class="line"><span>    booktitle={ArXiv preprint arXiv:2306.01337},</span></span>
<span class="line"><span>}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><a href="https://arxiv.org/pdf/2402.11359" target="_blank" rel="noopener noreferrer">AgentOptimizer</a></p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>@article{zhang2024training,</span></span>
<span class="line"><span>  title={Training Language Model Agents without Modifying Language Models},</span></span>
<span class="line"><span>  author={Zhang, Shaokun and Zhang, Jieyu and Liu, Jiale and Song, Linxin and Wang, Chi and Krishna, Ranjay and Wu, Qingyun},</span></span>
<span class="line"><span>  journal={ICML&#39;24},</span></span>
<span class="line"><span>  year={2024}</span></span>
<span class="line"><span>}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="贡献" tabindex="-1"><a class="header-anchor" href="#贡献"><span>贡献</span></a></h2><p>此项目欢迎贡献和建议。大多数贡献都要求您同意签署<a href="https://cla.opensource.microsoft.com" target="_blank" rel="noopener noreferrer">贡献者许可协议（CLA）</a>，声明您有权并且实际上授予我们使用您的贡献的权利。</p><p>如果您是 GitHub 新手，<a href="https://opensource.guide/how-to-contribute/#how-to-submit-a-contribution" target="_blank" rel="noopener noreferrer">这里</a>有关于在 GitHub 上参与开发的详细帮助资源。</p><p>当您提交拉取请求时，CLA 机器人将自动确定您是否需要提供 CLA，并适当地装饰 PR（例如，状态检查、注释）。只需按照机器人提供的说明操作。在使用我们的 CLA 的所有存储库中，您只需要执行一次此操作。</p><p>此项目已采</p><p>用<a href="https://opensource.microsoft.com/codeofconduct/" target="_blank" rel="noopener noreferrer">Microsoft 开放源代码行为准则</a>。更多信息，请参见<a href="https://opensource.microsoft.com/codeofconduct/faq/" target="_blank" rel="noopener noreferrer">行为准则 FAQ</a>，或通过电子邮件<a href="mailto:opencode@microsoft.com" target="_blank" rel="noopener noreferrer">opencode@microsoft.com</a>与我们联系。</p><h2 id="贡献者墙" tabindex="-1"><a class="header-anchor" href="#贡献者墙"><span>贡献者墙</span></a></h2><a href="https://github.com/microsoft/autogen/graphs/contributors"><img src="https://contrib.rocks/image?repo=microsoft/autogen&amp;max=204"></a><h1 id="法律声明" tabindex="-1"><a class="header-anchor" href="#法律声明"><span>法律声明</span></a></h1><p>Microsoft 和任何贡献者授予您对该存储库中的 Microsoft 文档和其他内容的许可，根据<a href="https://creativecommons.org/licenses/by/4.0/legalcode" target="_blank" rel="noopener noreferrer">知识共享署名 4.0 国际公共许可证</a>，请参阅<a href="LICENSE">LICENSE</a> 文件，并向您授予对该存储库中任何代码的许可，根据<a href="https://opensource.org/licenses/MIT" target="_blank" rel="noopener noreferrer">MIT 许可证</a>，请参阅<a href="LICENSE-CODE">LICENSE-CODE</a> 文件。</p><p>在文档中引用的 Microsoft、Windows、Microsoft Azure 和/或其他 Microsoft 产品和服务可能是 Microsoft 在美国和/或其他国家/地区的商标或已注册商标。此项目的许可不授予您使用任何 Microsoft 名称、标志或商标的权利。Microsoft 的一般商标指南可在 <a href="http://go.microsoft.com/fwlink/?LinkID=254653" target="_blank" rel="noopener noreferrer">http://go.microsoft.com/fwlink/?LinkID=254653</a> 上找到。</p><p>隐私信息可在 <a href="https://privacy.microsoft.com/en-us/" target="_blank" rel="noopener noreferrer">https://privacy.microsoft.com/en-us/</a> 找到。</p><p>Microsoft 和任何贡献者保留所有其他权利，无论是根据其各自的版权、专利还是商标权，无论是通过暗示、禁止还是以其他方式。</p><h1 id="参考资料" tabindex="-1"><a class="header-anchor" href="#参考资料"><span>参考资料</span></a></h1><p><a href="https://github.com/microsoft/autogen/blob/main/README.md" target="_blank" rel="noopener noreferrer">https://github.com/microsoft/autogen/blob/main/README.md</a></p>`,61)])])}const h=e(t,[["render",r]]),g=JSON.parse('{"path":"/posts/ai/2024-02-20-ai-15-autogen.html","title":"AI-15-autogen  通过多个可以相互对话的代理实现 LLM（Large Language Model）应用的开发。AutoGen 代理是可定制、可对话的，并且可以无缝地允许人类参与。","lang":"zh-CN","frontmatter":{"title":"AI-15-autogen  通过多个可以相互对话的代理实现 LLM（Large Language Model）应用的开发。AutoGen 代理是可定制、可对话的，并且可以无缝地允许人类参与。","categories":["AI"],"tags":["ai","aigc","chatgpt","gpt","sh"],"published":true,"description":"AutoGen简介 AutoGen 是一个框架，它通过多个可以相互对话的代理实现 LLM（Large Language Model）应用的开发。AutoGen 代理是可定制、可对话的，并且可以无缝地允许人类参与。它们可以以使用 LLM、人类输入和工具的各种模式运行。 AutoGen概览AutoGen概览 AutoGen 通过 多代理对话 实现了下一代 ...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"AI-15-autogen  通过多个可以相互对话的代理实现 LLM（Large Language Model）应用的开发。AutoGen 代理是可定制、可对话的，并且可以无缝地允许人类参与。\\",\\"image\\":[\\"https://github.com/microsoft/autogen/blob/main/website/static/img/autogen_agentchat.png\\",\\"https://github.com/codespaces/badge.svg\\",\\"https://github.com/microsoft/autogen/blob/main/website/static/img/chat_example.png\\"],\\"dateModified\\":\\"2025-12-27T05:14:38.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"老马啸西风\\",\\"url\\":\\"https://houbb.github.io\\"}]}"],["meta",{"property":"og:url","content":"https://houbb.github.io/blog-thinking/posts/ai/2024-02-20-ai-15-autogen.html"}],["meta",{"property":"og:site_name","content":"老马啸西风"}],["meta",{"property":"og:title","content":"AI-15-autogen  通过多个可以相互对话的代理实现 LLM（Large Language Model）应用的开发。AutoGen 代理是可定制、可对话的，并且可以无缝地允许人类参与。"}],["meta",{"property":"og:description","content":"AutoGen简介 AutoGen 是一个框架，它通过多个可以相互对话的代理实现 LLM（Large Language Model）应用的开发。AutoGen 代理是可定制、可对话的，并且可以无缝地允许人类参与。它们可以以使用 LLM、人类输入和工具的各种模式运行。 AutoGen概览AutoGen概览 AutoGen 通过 多代理对话 实现了下一代 ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://github.com/microsoft/autogen/blob/main/website/static/img/autogen_agentchat.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-27T05:14:38.000Z"}],["meta",{"property":"article:tag","content":"sh"}],["meta",{"property":"article:tag","content":"gpt"}],["meta",{"property":"article:tag","content":"chatgpt"}],["meta",{"property":"article:tag","content":"aigc"}],["meta",{"property":"article:tag","content":"ai"}],["meta",{"property":"article:modified_time","content":"2025-12-27T05:14:38.000Z"}]]},"git":{"createdTime":1766812269000,"updatedTime":1766812478000,"contributors":[{"name":"bbhou","username":"bbhou","email":"1557740299@qq.com","commits":2,"url":"https://github.com/bbhou"}]},"readingTime":{"minutes":7.02,"words":2105},"filePathRelative":"posts/ai/2024-02-20-ai-15-autogen.md","excerpt":"<h2>AutoGen简介</h2>\\n<p>AutoGen 是一个框架，它通过多个可以相互对话的代理实现 LLM（Large Language Model）应用的开发。AutoGen 代理是可定制、可对话的，并且可以无缝地允许人类参与。它们可以以使用 LLM、人类输入和工具的各种模式运行。</p>\\n<figure><img src=\\"https://github.com/microsoft/autogen/blob/main/website/static/img/autogen_agentchat.png\\" alt=\\"AutoGen概览\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>AutoGen概览</figcaption></figure>","autoDesc":true}');export{h as comp,g as data};
