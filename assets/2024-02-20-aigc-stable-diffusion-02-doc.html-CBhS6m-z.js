import{_ as s}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as a,a as e,o as n}from"./app-aSB-Rx6k.js";const t={};function l(r,i){return n(),a("div",null,[...i[0]||(i[0]=[e(`<h1 id="稳定扩散版本2" tabindex="-1"><a class="header-anchor" href="#稳定扩散版本2"><span>稳定扩散版本2</span></a></h1><p>此仓库包含从头开始训练的<a href="https://github.com/CompVis/stable-diffusion" target="_blank" rel="noopener noreferrer">Stable Diffusion</a>模型，并将持续更新新的检查点。以下是当前可用模型的概述。更多内容即将发布。</p><h2 id="最新动态" tabindex="-1"><a class="header-anchor" href="#最新动态"><span>最新动态</span></a></h2><p><strong>2023年3月24日</strong></p><p><em>Stable UnCLIP 2.1</em></p><ul><li><p>基于SD2.1-768，在768x768分辨率下的新的稳定扩散微调（<em>Stable unCLIP 2.1</em>，<a href="https://huggingface.co/stabilityai/" target="_blank" rel="noopener noreferrer">Hugging Face</a>）。此模型允许进行图像变体和混合操作，如<a href="https://arxiv.org/abs/2204.06125" target="_blank" rel="noopener noreferrer"><em>使用CLIP潜在空间的层次文本条件图像生成</em></a>中所述，并且由于其模块化，可以与其他模型（如<a href="https://github.com/kakaobrain/karlo" target="_blank" rel="noopener noreferrer">KARLO</a>）结合使用。有两种变体：<a href="https://huggingface.co/stabilityai/stable-diffusion-2-1-unclip/blob/main/sd21-unclip-l.ckpt" target="_blank" rel="noopener noreferrer">Stable unCLIP-L</a>和<a href="https://huggingface.co/stabilityai/stable-diffusion-2-1-unclip/blob/main/sd21-unclip-h.ckpt" target="_blank" rel="noopener noreferrer">Stable unCLIP-H</a>，分别以CLIP ViT-L和ViT-H图像嵌入为条件。</p></li><li><p>SD-unCLIP的公开演示已在<a href="https://clipdrop.co/stable-diffusion-reimagine" target="_blank" rel="noopener noreferrer">clipdrop.co/stable-diffusion-reimagine</a>提供。</p></li></ul><p><strong>2022年12月7日</strong></p><p><em>版本 2.1</em></p><ul><li>基于相同参数数量和架构的新稳定扩散模型(<em>Stable Diffusion 2.1-v</em>，<a href="https://huggingface.co/stabilityai/stable-diffusion-2-1" target="_blank" rel="noopener noreferrer">Hugging Face</a>)在768x768分辨率和(<em>Stable Diffusion 2.1-base</em>，<a href="https://huggingface.co/stabilityai/stable-diffusion-2-1-base" target="_blank" rel="noopener noreferrer">HuggingFace</a>)在512x512分辨率上，均基于2.0并在2.0的基础上微调，使用较少限制的<a href="https://laion.ai/blog/laion-5b/" target="_blank" rel="noopener noreferrer">LAION-5B</a>数据集进行NSFW过滤。默认情况下，如果未安装<code>xformers</code>，模型的注意操作将在全精度下评估。要启用fp16（这可能会导致v2.1模型的普通注意模块出现数值不稳定），请使用<code>ATTN_PRECISION=fp16 python &lt;thescript.py&gt;</code>运行脚本。</li></ul><p><strong>2022年11月24日</strong></p><p><em>版本 2.0</em></p><ul><li><p>新的稳定扩散模型(<em>Stable Diffusion 2.0-v</em>)在768x768分辨率下。U-Net参数数量与1.5相同，但使用<a href="https://github.com/mlfoundations/open_clip" target="_blank" rel="noopener noreferrer">OpenCLIP-ViT/H</a>作为文本编码器并从头开始训练。_SD 2.0-v_是所谓的<a href="https://arxiv.org/abs/2202.00512" target="_blank" rel="noopener noreferrer">v-prediction</a>模型。</p></li><li><p>上述模型从_SF 2.0-base_微调而来，后者作为标准噪声预测模型在512x512图像上训练，也提供。</p></li><li><p>增加了x4上采样潜在文本引导扩散模型</p></li><li><p>新的深度引导稳定扩散模型，从_SD 2.0-base_微调而来。模型基于通过<a href="https://github.com/isl-org/MiDaS" target="_blank" rel="noopener noreferrer">MiDaS</a>推断的单目深度估计，可以用于保持结构的img2img和形状条件合成。</p></li><li><p>文本引导修复模型，从_SD 2.0-base_微调而来。</p></li></ul><p>我们遵循<a href="https://github.com/CompVis/stable-diffusion" target="_blank" rel="noopener noreferrer">原始仓库</a>并提供基础推理脚本来从模型中采样。</p><hr><p><em>原始的Stable Diffusion模型是与<a href="https://arxiv.org/abs/2202.00512" target="_blank" rel="noopener noreferrer">CompVis</a>和<a href="https://runwayml.com/" target="_blank" rel="noopener noreferrer">RunwayML</a>合作创建的，基于以下工作：</em></p><p><a href="https://ommer-lab.com/research/latent-diffusion-models/" target="_blank" rel="noopener noreferrer"><strong>高分辨率图像合成与潜在扩散模型</strong></a><br><br><a href="https://github.com/rromb" target="_blank" rel="noopener noreferrer">Robin Rombach</a>*，<br><a href="https://github.com/ablattmann" target="_blank" rel="noopener noreferrer">Andreas Blattmann</a>*，<br><a href="https://github.com/qp-qp" target="_blank" rel="noopener noreferrer">Dominik Lorenz</a>\\，<br><a href="https://github.com/pesser" target="_blank" rel="noopener noreferrer">Patrick Esser</a>，<br><a href="https://hci.iwr.uni-heidelberg.de/Staff/bommer" target="_blank" rel="noopener noreferrer">Björn Ommer</a><br><br><em><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html" target="_blank" rel="noopener noreferrer">CVPR &#39;22 口头报告</a> |<br><a href="https://github.com/CompVis/latent-diffusion" target="_blank" rel="noopener noreferrer">GitHub</a> | <a href="https://arxiv.org/abs/2112.10752" target="_blank" rel="noopener noreferrer">arXiv</a> | <a href="https://ommer-lab.com/research/latent-diffusion-models/" target="_blank" rel="noopener noreferrer">项目页面</a></em></p><p>Stable Diffusion是一个潜在的文本到图像扩散模型。</p><hr><h2 id="要求" tabindex="-1"><a class="header-anchor" href="#要求"><span>要求</span></a></h2><p>您可以通过运行以下命令来更新现有的<a href="https://github.com/CompVis/latent-diffusion" target="_blank" rel="noopener noreferrer">潜在扩散</a>环境：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>conda install pytorch==1.12.1 torchvision==0.13.1 -c pytorch</span></span>
<span class="line"><span>pip install transformers==4.19.2 diffusers invisible-watermark</span></span>
<span class="line"><span>pip install -e .</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="xformers高效注意" tabindex="-1"><a class="header-anchor" href="#xformers高效注意"><span>xformers高效注意</span></a></h4><p>为了提高GPU上的效率和速度，<br> 我们强烈推荐安装<a href="https://github.com/facebookresearch/xformers" target="_blank" rel="noopener noreferrer">xformers</a>库。</p><p>已在带CUDA 11.4的A100上测试。<br> 安装需要较新的nvcc和gcc/g++版本，可以通过以下命令获得：</p><div class="language-commandline line-numbers-mode" data-highlighter="shiki" data-ext="commandline" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-commandline"><span class="line"><span>export CUDA_HOME=/usr/local/cuda-11.4</span></span>
<span class="line"><span>conda install -c nvidia/label/cuda-11.4.0 cuda-nvcc</span></span>
<span class="line"><span>conda install -c conda-forge gcc</span></span>
<span class="line"><span>conda install -c conda-forge gxx_linux-64==9.5.0</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>然后，运行以下命令（编译可能需要长达30分钟）。</p><div class="language-commandline line-numbers-mode" data-highlighter="shiki" data-ext="commandline" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-commandline"><span class="line"><span>cd ..</span></span>
<span class="line"><span>git clone https://github.com/facebookresearch/xformers.git</span></span>
<span class="line"><span>cd xformers</span></span>
<span class="line"><span>git submodule update --init --recursive</span></span>
<span class="line"><span>pip install -r requirements.txt</span></span>
<span class="line"><span>pip install -e .</span></span>
<span class="line"><span>cd ../stablediffusion</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>成功安装后，代码将在U-Net和自动编码器中的自注意和交叉注意层自动使用<a href="https://github.com/facebookresearch/xformers" target="_blank" rel="noopener noreferrer">内存高效注意</a>。</p><h2 id="一般免责声明" tabindex="-1"><a class="header-anchor" href="#一般免责声明"><span>一般免责声明</span></a></h2><p>Stable Diffusion模型是通用的文本到图像扩散模型，因此反映了其训练数据中的偏见和（误）概念。尽管已经努力减少明确的色情内容，但<strong>我们不推荐在没有额外安全机制和考虑的情况下将提供的权重用于服务或产品。权重是研究成果，应如此对待。</strong><br> 有关训练过程和数据的详细信息，以及模型的预期用途，请参见相应的<a href="https://huggingface.co/stabilityai/stable-diffusion-2" target="_blank" rel="noopener noreferrer">模型卡</a>。<br> 权重可通过<a href="https://huggingface.co/StabilityAI" target="_blank" rel="noopener noreferrer">Hugging Face上的StabilityAI组织</a>根据<a href="LICENSE-MODEL">CreativeML Open RAIL++-M License</a>获得。</p><h2 id="稳定扩散v2" tabindex="-1"><a class="header-anchor" href="#稳定扩散v2"><span>稳定扩散v2</span></a></h2><p>稳定扩散v2指的是使用下采样因子8自动编码器、865M UNet和OpenCLIP ViT-H/14文本编码器的特定模型配置。_SD 2-v_模型生成768x768像素的输出。</p><p>在不同的无分类器指导比例（1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0）和50个DDIM采样步骤下的评估显示了检查点的相对改进：</p><h3 id="文本到图像" tabindex="-1"><a class="header-anchor" href="#文本到图像"><span>文本到图像</span></a></h3><h4 id="用法" tabindex="-1"><a class="header-anchor" href="#用法"><span>用法</span></a></h4><p>以下展示了如何使用<a href="https://github.com/huggingface/diffusers" target="_blank" rel="noopener noreferrer">diffusers</a>库从_SF 2.0-v_模型中采样：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> torch</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> diffusers </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> StableDiffusionPipeline</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model_id </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;stabilityai/stable-diffusion-2&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">device </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;cuda&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">pipe </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> StableDiffusionPipeline.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">from_pretrained</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(model_id, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">torch_dtype</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">torch.float16)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">pipe </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> pipe.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">to</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(device)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">prompt </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;the dog is walking&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">image </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> pipe</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(prompt).images[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">image.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">save</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;dog.png&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>或者，可以使用我们提供的直接脚本：</p><div class="language-commandline line-numbers-mode" data-highlighter="shiki" data-ext="commandline" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-commandline"><span class="line"><span>python scripts/txt2img.py --prompt &quot;a beautiful cat&quot; --H 768 --W 768 --seed 27 --n_samples 1 --n_iter 1</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h3 id="深度条件稳定扩散" tabindex="-1"><a class="header-anchor" href="#深度条件稳定扩散"><span>深度条件稳定扩散</span></a></h3><p><strong>Stable Diffusion 2 depth-conditioned model</strong>对形状和深度信息敏感。其使用方式与文本到图像模型相似，不同的是需要额外的单目深度图。<br> 我们提供了用于生成这种深度图的代码片段，可以集成到推理管道中。</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> torch</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> diffusers </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> StableDiffusionDepth2ImgPipeline</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model_id </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;stabilityai/stable-diffusion-2-depth&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">device </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;cuda&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">pipe </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> StableDiffusionDepth2ImgPipeline.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">from_pretrained</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(model_id, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">torch_dtype</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">torch.float16)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">pipe </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> pipe.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">to</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(device)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">prompt </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;a luxurious mansion&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">depth_map </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> get_depth_map</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;input_image.jpg&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 生成深度图的函数</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">image </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> pipe</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(prompt, depth_map).images[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">image.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">save</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;mansion.png&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="图片修复" tabindex="-1"><a class="header-anchor" href="#图片修复"><span>图片修复</span></a></h3><p><em>Stable Diffusion 2.0 inpainting model</em> 是一个专门用于图片修复任务的模型。其使用方法与其他模式相似，但需要额外的掩码信息。<br> 我们提供了生成这种掩码的代码片段，可以集成到推理管道中。</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> torch</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> diffusers </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> StableDiffusionInpaintPipeline</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model_id </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;stabilityai/stable-diffusion-2-inpainting&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">device </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;cuda&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">pipe </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> StableDiffusionInpaintPipeline.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">from_pretrained</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(model_id, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">torch_dtype</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">torch.float16)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">pipe </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> pipe.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">to</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(device)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">prompt </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;a beautiful beach&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">init_image </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;path_to_image.png&quot;</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">  # 初始图像</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">mask_image </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;path_to_mask.png&quot;</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">   # 掩码图像</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">image </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> pipe</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(prompt, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">init_image</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">init_image, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">mask_image</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">mask_image).images[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">image.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">save</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;beach.png&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="图像上采样" tabindex="-1"><a class="header-anchor" href="#图像上采样"><span>图像上采样</span></a></h3><h4 id="用法-1" tabindex="-1"><a class="header-anchor" href="#用法-1"><span>用法</span></a></h4><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> torch</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> diffusers </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> StableDiffusionUpscalePipeline</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model_id </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;stabilityai/stable-diffusion-x4-upscaler&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">device </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;cuda&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">pipe </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> StableDiffusionUpscalePipeline.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">from_pretrained</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(model_id, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">torch_dtype</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">torch.float16)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">pipe </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> pipe.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">to</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(device)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">prompt </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;an ultra high resolution photo of a cat&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">low_res_image </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> get_low_res_image</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;path_to_image.png&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 生成低分辨率图像的函数</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">image </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> pipe</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(prompt, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">image</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">low_res_image).images[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">image.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">save</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;high_res_cat.png&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="参考" tabindex="-1"><a class="header-anchor" href="#参考"><span>参考</span></a></h3><ul><li>相关论文：<a href="https://arxiv.org/abs/2112.10752" target="_blank" rel="noopener noreferrer">高分辨率图像合成与潜在扩散模型</a></li><li>代码库：<a href="https://github.com/CompVis/stable-diffusion" target="_blank" rel="noopener noreferrer">https://github.com/CompVis/stable-diffusion</a></li><li>模型权重：<a href="https://huggingface.co/StabilityAI" target="_blank" rel="noopener noreferrer">Hugging Face上的StabilityAI组织</a></li></ul><h3 id="致谢" tabindex="-1"><a class="header-anchor" href="#致谢"><span>致谢</span></a></h3><p>我们对以下人和组织表示感谢：</p><ul><li><a href="https://ommer-lab.com/" target="_blank" rel="noopener noreferrer">CompVis组</a></li><li><a href="https://runwayml.com/" target="_blank" rel="noopener noreferrer">RunwayML</a></li><li><a href="https://github.com/mlfoundations/open_clip" target="_blank" rel="noopener noreferrer">OpenCLIP</a></li></ul><p>更多信息和更新，请访问我们的<a href="https://github.com/stabilityai/stable-diffusion" target="_blank" rel="noopener noreferrer">GitHub页面</a>和<a href="https://huggingface.co/stabilityai/stable-diffusion-2" target="_blank" rel="noopener noreferrer">Hugging Face页面</a>。</p><h2 id="快速入门" tabindex="-1"><a class="header-anchor" href="#快速入门"><span>快速入门</span></a></h2><p>Stable Diffusion是一个强大的工具，可以用来生成高质量的图像。以下是一个快速入门指南：</p><ol><li>安装必要的依赖库。</li><li>下载并加载模型。</li><li>输入提示词生成图像。</li></ol><p>更多详细信息，请参考上述每个部分的使用说明。祝您使用愉快！</p><h1 id="参考资料" tabindex="-1"><a class="header-anchor" href="#参考资料"><span>参考资料</span></a></h1><p><a href="https://github.com/Stability-AI/stablediffusion/blob/main/README.md" target="_blank" rel="noopener noreferrer">https://github.com/Stability-AI/stablediffusion/blob/main/README.md</a></p>`,60)])])}const k=s(t,[["render",l]]),o=JSON.parse('{"path":"/posts/aigc/2024-02-20-aigc-stable-diffusion-02-doc.html","title":"Stable Diffusion-01-入门概览","lang":"zh-CN","frontmatter":{"title":"Stable Diffusion-01-入门概览","date":"2024-02-20T00:00:00.000Z","categories":["AI"],"tags":["ai","aigc","sh"],"published":true,"description":"稳定扩散版本2 此仓库包含从头开始训练的Stable Diffusion模型，并将持续更新新的检查点。以下是当前可用模型的概述。更多内容即将发布。 最新动态 2023年3月24日 Stable UnCLIP 2.1 基于SD2.1-768，在768x768分辨率下的新的稳定扩散微调（Stable unCLIP 2.1，Hugging Face）。此模型...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Stable Diffusion-01-入门概览\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2024-02-20T00:00:00.000Z\\",\\"dateModified\\":\\"2025-12-27T05:15:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"老马啸西风\\",\\"url\\":\\"https://houbb.github.io\\"}]}"],["meta",{"property":"og:url","content":"https://houbb.github.io/awesome-ai-coding/posts/aigc/2024-02-20-aigc-stable-diffusion-02-doc.html"}],["meta",{"property":"og:site_name","content":"老马啸西风"}],["meta",{"property":"og:title","content":"Stable Diffusion-01-入门概览"}],["meta",{"property":"og:description","content":"稳定扩散版本2 此仓库包含从头开始训练的Stable Diffusion模型，并将持续更新新的检查点。以下是当前可用模型的概述。更多内容即将发布。 最新动态 2023年3月24日 Stable UnCLIP 2.1 基于SD2.1-768，在768x768分辨率下的新的稳定扩散微调（Stable unCLIP 2.1，Hugging Face）。此模型..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-27T05:15:15.000Z"}],["meta",{"property":"article:tag","content":"sh"}],["meta",{"property":"article:tag","content":"aigc"}],["meta",{"property":"article:tag","content":"ai"}],["meta",{"property":"article:published_time","content":"2024-02-20T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-27T05:15:15.000Z"}]]},"git":{"createdTime":1766812269000,"updatedTime":1766812515000,"contributors":[{"name":"bbhou","username":"bbhou","email":"1557740299@qq.com","commits":3,"url":"https://github.com/bbhou"}]},"readingTime":{"minutes":6.06,"words":1819},"filePathRelative":"posts/aigc/2024-02-20-aigc-stable-diffusion-02-doc.md","excerpt":"\\n<p>此仓库包含从头开始训练的<a href=\\"https://github.com/CompVis/stable-diffusion\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Stable Diffusion</a>模型，并将持续更新新的检查点。以下是当前可用模型的概述。更多内容即将发布。</p>\\n<h2>最新动态</h2>\\n<p><strong>2023年3月24日</strong></p>\\n<p><em>Stable UnCLIP 2.1</em></p>\\n<ul>\\n<li>\\n<p>基于SD2.1-768，在768x768分辨率下的新的稳定扩散微调（<em>Stable unCLIP 2.1</em>，<a href=\\"https://huggingface.co/stabilityai/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Hugging Face</a>）。此模型允许进行图像变体和混合操作，如<a href=\\"https://arxiv.org/abs/2204.06125\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\"><em>使用CLIP潜在空间的层次文本条件图像生成</em></a>中所述，并且由于其模块化，可以与其他模型（如<a href=\\"https://github.com/kakaobrain/karlo\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">KARLO</a>）结合使用。有两种变体：<a href=\\"https://huggingface.co/stabilityai/stable-diffusion-2-1-unclip/blob/main/sd21-unclip-l.ckpt\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Stable unCLIP-L</a>和<a href=\\"https://huggingface.co/stabilityai/stable-diffusion-2-1-unclip/blob/main/sd21-unclip-h.ckpt\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Stable unCLIP-H</a>，分别以CLIP ViT-L和ViT-H图像嵌入为条件。</p>\\n</li>\\n<li>\\n<p>SD-unCLIP的公开演示已在<a href=\\"https://clipdrop.co/stable-diffusion-reimagine\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">clipdrop.co/stable-diffusion-reimagine</a>提供。</p>\\n</li>\\n</ul>","autoDesc":true}');export{k as comp,o as data};
