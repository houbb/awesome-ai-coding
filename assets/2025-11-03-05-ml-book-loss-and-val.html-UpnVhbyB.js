import{_ as r}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as n,a as e,b as t,o as d}from"./app-CxH-55KZ.js";const s={};function h(o,a){return d(),n("div",null,[...a[0]||(a[0]=[e('<h2 id="第5章-优化与数值计算" tabindex="-1"><a class="header-anchor" href="#第5章-优化与数值计算"><span>第5章　优化与数值计算</span></a></h2><p>优化问题是机器学习的核心。</p><p>学习算法的本质是 <strong>寻找最优参数，使模型在给定任务上表现最佳</strong>。</p><p>这一过程几乎总可以形式化为一个“优化问题”：</p><p>[<br> \\min_\\theta ; L(\\theta)<br> ]</p><p>其中 ( \\theta ) 是模型参数，( L(\\theta) ) 是损失函数或目标函数。<br> 本章将介绍从损失函数设计到优化算法、再到数值陷阱与模型复杂度控制的系统思维。</p><hr><h3 id="_5-1-损失函数与目标函数" tabindex="-1"><a class="header-anchor" href="#_5-1-损失函数与目标函数"><span><strong>5.1 损失函数与目标函数</strong></span></a></h3><h4 id="✅-一、定义与作用" tabindex="-1"><a class="header-anchor" href="#✅-一、定义与作用"><span>✅ 一、定义与作用</span></a></h4><p>损失函数（Loss Function）是衡量模型输出与真实标签之间误差的函数。<br> 优化算法的任务，就是让损失函数尽可能小。</p><p>常见形式如下：</p>',11),t("table",null,[t("thead",null,[t("tr",null,[t("th",null,"任务类型"),t("th",null,"损失函数"),t("th",null,"数学表达式"),t("th")])]),t("tbody",null,[t("tr",null,[t("td",null,"回归"),t("td",null,"均方误差 (MSE)"),t("td",{N:""},"( L = \\frac{1}\\sum_i (y_i - \\hat{y}_i)^2 )"),t("td")]),t("tr",null,[t("td",null,"分类"),t("td",null,"交叉熵损失 (Cross Entropy)"),t("td",null,"( L = -\\sum_i y_i \\log(\\hat{y}_i) )"),t("td")]),t("tr",null,[t("td",null,"支持向量机"),t("td",null,"合页损失 (Hinge Loss)"),t("td",null,"( L = \\max(0, 1 - y_i w^T x_i) )"),t("td")]),t("tr",null,[t("td",null,"概率建模"),t("td",null,"对数似然损失 (Negative Log-Likelihood)"),t("td",null,"( L = -\\log P(y"),t("td",null,"x;\\theta) )")])])],-1),e('<h4 id="✅-二、目标函数与正则项" tabindex="-1"><a class="header-anchor" href="#✅-二、目标函数与正则项"><span>✅ 二、目标函数与正则项</span></a></h4><p>有时我们不仅希望拟合数据，还希望模型“泛化”得更好，于是引入正则化：</p><p>[<br> J(\\theta) = L(\\theta) + \\lambda \\Omega(\\theta)<br> ]</p><ul><li>( L(\\theta) )：经验风险（对训练样本的平均损失）</li><li>( \\Omega(\\theta) )：正则化项（控制模型复杂度）</li><li>( \\lambda )：权衡因子</li></ul><h4 id="✅-三、风险最小化原则" tabindex="-1"><a class="header-anchor" href="#✅-三、风险最小化原则"><span>✅ 三、风险最小化原则</span></a></h4><p>机器学习中两种典型思想：</p><ul><li><strong>经验风险最小化 (ERM)</strong>：直接最小化训练集损失</li><li><strong>结构风险最小化 (SRM)</strong>：在经验风险和模型复杂度之间做权衡（如SVM）</li></ul><hr><h3 id="_5-2-梯度下降与随机梯度下降" tabindex="-1"><a class="header-anchor" href="#_5-2-梯度下降与随机梯度下降"><span><strong>5.2 梯度下降与随机梯度下降</strong></span></a></h3><h4 id="✅-一、梯度下降-gradient-descent-gd" tabindex="-1"><a class="header-anchor" href="#✅-一、梯度下降-gradient-descent-gd"><span>✅ 一、梯度下降（Gradient Descent, GD）</span></a></h4><p>梯度下降是优化的核心算法。<br> 其思想简单直接：<strong>沿着梯度下降方向迭代更新参数</strong>。</p><p>[<br> \\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L(\\theta_t)<br> ]</p><ul><li>( \\eta )：学习率 (Learning Rate)</li><li>( \\nabla_\\theta L )：损失函数对参数的梯度</li></ul><p>当学习率太大 → 振荡甚至发散；<br> 当学习率太小 → 收敛过慢。</p><h4 id="✅-二、随机梯度下降-sgd" tabindex="-1"><a class="header-anchor" href="#✅-二、随机梯度下降-sgd"><span>✅ 二、随机梯度下降（SGD）</span></a></h4><p>全批量梯度下降对大数据集成本太高，于是出现了 <strong>随机梯度下降</strong>：</p><p>[<br> \\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L_i(\\theta_t)<br> ]</p><p>其中 ( L_i ) 是单个样本或小批量（mini-batch）的损失。<br> SGD 能够更快地更新参数、避免陷入局部最优。</p><h4 id="✅-三、改进算法" tabindex="-1"><a class="header-anchor" href="#✅-三、改进算法"><span>✅ 三、改进算法</span></a></h4><p>现代优化算法在SGD的基础上做了改进：</p><table><thead><tr><th>算法</th><th>特点</th></tr></thead><tbody><tr><td><strong>Momentum</strong></td><td>增加“惯性”，平滑梯度方向，加快收敛</td></tr><tr><td><strong>AdaGrad</strong></td><td>自适应学习率，适合稀疏特征</td></tr><tr><td><strong>RMSProp</strong></td><td>改进AdaGrad的学习率衰减问题</td></tr><tr><td><strong>Adam</strong></td><td>结合Momentum和RMSProp，成为深度学习默认选择</td></tr></tbody></table><p>Adam的更新规则：</p><p>[<br> m_t = \\beta_1 m_{t-1} + (1-\\beta_1)\\nabla_\\theta L_t<br> ]<br> [<br> v_t = \\beta_2 v_{t-1} + (1-\\beta_2)(\\nabla_\\theta L_t)^2<br> ]<br> [<br> \\theta_{t+1} = \\theta_t - \\eta \\frac{m_t}{\\sqrt{v_t} + \\epsilon}<br> ]</p><hr><h3 id="_5-3-正则化与模型复杂度控制" tabindex="-1"><a class="header-anchor" href="#_5-3-正则化与模型复杂度控制"><span><strong>5.3 正则化与模型复杂度控制</strong></span></a></h3><h4 id="✅-一、为什么需要正则化" tabindex="-1"><a class="header-anchor" href="#✅-一、为什么需要正则化"><span>✅ 一、为什么需要正则化</span></a></h4><p>在高维空间中，模型往往可以“完美”拟合训练数据，但在测试数据上表现很差——即 <strong>过拟合</strong>。<br> 正则化就是控制这种“过度灵活性”的手段。</p><h4 id="✅-二、常见正则化方法" tabindex="-1"><a class="header-anchor" href="#✅-二、常见正则化方法"><span>✅ 二、常见正则化方法</span></a></h4><table><thead><tr><th>方法</th><th>数学形式</th><th>作用</th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td><strong>L2 正则化 (Ridge)</strong></td><td>( \\Omega(\\theta) =</td><td></td><td>\\theta</td><td></td><td>_2^2 )</td><td>平滑参数，抑制大权重</td></tr><tr><td><strong>L1 正则化 (Lasso)</strong></td><td>( \\Omega(\\theta) =</td><td></td><td>\\theta</td><td></td><td>_1 )</td><td>稀疏化参数，实现特征选择</td></tr><tr><td><strong>Dropout</strong></td><td>随机丢弃神经元</td><td>提高模型鲁棒性</td><td></td><td></td><td></td><td></td></tr><tr><td><strong>Early Stopping</strong></td><td>提前停止训练</td><td>避免模型对训练集过拟合</td><td></td><td></td><td></td><td></td></tr><tr><td><strong>数据增强</strong></td><td>扩大样本空间</td><td>提高泛化能力</td><td></td><td></td><td></td><td></td></tr></tbody></table><h4 id="✅-三、模型复杂度与容量" tabindex="-1"><a class="header-anchor" href="#✅-三、模型复杂度与容量"><span>✅ 三、模型复杂度与容量</span></a></h4><ul><li>模型容量太小 → 欠拟合（bias高）</li><li>模型容量太大 → 过拟合（variance高）<br> → <strong>偏差-方差权衡</strong> 是正则化的核心思想。</li></ul><hr><h3 id="_5-4-优化陷阱-局部最优、鞍点、梯度消失" tabindex="-1"><a class="header-anchor" href="#_5-4-优化陷阱-局部最优、鞍点、梯度消失"><span><strong>5.4 优化陷阱：局部最优、鞍点、梯度消失</strong></span></a></h3><h4 id="✅-一、局部最优-local-minima" tabindex="-1"><a class="header-anchor" href="#✅-一、局部最优-local-minima"><span>✅ 一、局部最优（Local Minima）</span></a></h4><p>非凸优化问题（如神经网络）可能存在多个局部最优解。<br> 幸运的是，<strong>在高维空间中，局部最优通常表现接近全局最优</strong>。</p><h4 id="✅-二、鞍点-saddle-point" tabindex="-1"><a class="header-anchor" href="#✅-二、鞍点-saddle-point"><span>✅ 二、鞍点（Saddle Point）</span></a></h4><p>鞍点是指梯度为零但既非最小也非最大的位置。<br> 它在高维空间中比局部最优更常见。<br> 优化算法常会卡在鞍点处，导致训练停滞。</p><h4 id="✅-三、梯度消失与梯度爆炸" tabindex="-1"><a class="header-anchor" href="#✅-三、梯度消失与梯度爆炸"><span>✅ 三、梯度消失与梯度爆炸</span></a></h4><p>尤其在深层网络中：</p><ul><li>梯度<strong>消失</strong>：导致前层权重几乎不更新</li><li>梯度<strong>爆炸</strong>：导致训练不稳定</li></ul><p><strong>解决思路：</strong></p><ul><li>使用 <strong>ReLU</strong> 激活函数（避免梯度消失）</li><li>使用 <strong>Batch Normalization</strong>（稳定梯度分布）</li><li><strong>梯度裁剪 (Gradient Clipping)</strong>（控制爆炸）</li><li><strong>残差结构 (ResNet)</strong>（保证梯度可传播）</li></ul><h4 id="✅-四、数值稳定性与优化技巧" tabindex="-1"><a class="header-anchor" href="#✅-四、数值稳定性与优化技巧"><span>✅ 四、数值稳定性与优化技巧</span></a></h4><ul><li>使用对数技巧：(\\log(\\sum e^x)) 替换 (\\sum e^x)</li><li>在实现时避免浮点下溢/上溢</li><li>归一化输入特征，加快收敛</li><li>动态调整学习率（Learning Rate Scheduling）</li></ul><hr><h3 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span><strong>总结</strong></span></a></h3><table><thead><tr><th>模块</th><th>关键思想</th><th>实践意义</th></tr></thead><tbody><tr><td>损失函数</td><td>定义“学习目标”</td><td>衡量模型误差</td></tr><tr><td>优化算法</td><td>找到最优参数</td><td>决定模型能否学好</td></tr><tr><td>正则化</td><td>控制复杂度</td><td>提高泛化能力</td></tr><tr><td>优化陷阱</td><td>理解局部/鞍点问题</td><td>指导算法改进</td></tr><tr><td>数值稳定性</td><td>避免计算问题</td><td>保证训练稳定性</td></tr></tbody></table>',47)])])}const p=r(s,[["render",h]]),g=JSON.parse('{"path":"/posts/ml/2025-11-03-05-ml-book-loss-and-val.html","title":"第5章　优化与数值计算","lang":"zh-CN","frontmatter":{"title":"第5章　优化与数值计算","date":"2025-11-03T00:00:00.000Z","categories":["AI"],"tags":["ai","learn-note"],"published":true,"description":"第5章 优化与数值计算 优化问题是机器学习的核心。 学习算法的本质是 寻找最优参数，使模型在给定任务上表现最佳。 这一过程几乎总可以形式化为一个“优化问题”： [ \\\\min_\\\\theta ; L(\\\\theta) ] 其中 ( \\\\theta ) 是模型参数，( L(\\\\theta) ) 是损失函数或目标函数。 本章将介绍从损失函数设计到优化算法、再到数值陷...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"第5章　优化与数值计算\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-11-03T00:00:00.000Z\\",\\"dateModified\\":\\"2025-12-27T05:15:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"老马啸西风\\",\\"url\\":\\"https://houbb.github.io\\"}]}"],["meta",{"property":"og:url","content":"https://houbb.github.io/awesome-ai-coding/posts/ml/2025-11-03-05-ml-book-loss-and-val.html"}],["meta",{"property":"og:site_name","content":"老马啸西风"}],["meta",{"property":"og:title","content":"第5章　优化与数值计算"}],["meta",{"property":"og:description","content":"第5章 优化与数值计算 优化问题是机器学习的核心。 学习算法的本质是 寻找最优参数，使模型在给定任务上表现最佳。 这一过程几乎总可以形式化为一个“优化问题”： [ \\\\min_\\\\theta ; L(\\\\theta) ] 其中 ( \\\\theta ) 是模型参数，( L(\\\\theta) ) 是损失函数或目标函数。 本章将介绍从损失函数设计到优化算法、再到数值陷..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-27T05:15:15.000Z"}],["meta",{"property":"article:tag","content":"learn-note"}],["meta",{"property":"article:tag","content":"ai"}],["meta",{"property":"article:published_time","content":"2025-11-03T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-27T05:15:15.000Z"}]]},"git":{"createdTime":1766812269000,"updatedTime":1766812515000,"contributors":[{"name":"bbhou","username":"bbhou","email":"1557740299@qq.com","commits":3,"url":"https://github.com/bbhou"}]},"readingTime":{"minutes":4.47,"words":1341},"filePathRelative":"posts/ml/2025-11-03-05-ml-book-loss-and-val.md","excerpt":"<h2>第5章　优化与数值计算</h2>\\n<p>优化问题是机器学习的核心。</p>\\n<p>学习算法的本质是 <strong>寻找最优参数，使模型在给定任务上表现最佳</strong>。</p>\\n<p>这一过程几乎总可以形式化为一个“优化问题”：</p>\\n<p>[<br>\\n\\\\min_\\\\theta ; L(\\\\theta)<br>\\n]</p>\\n<p>其中 ( \\\\theta ) 是模型参数，( L(\\\\theta) ) 是损失函数或目标函数。<br>\\n本章将介绍从损失函数设计到优化算法、再到数值陷阱与模型复杂度控制的系统思维。</p>\\n<hr>\\n<h3><strong>5.1 损失函数与目标函数</strong></h3>","autoDesc":true}');export{p as comp,g as data};
