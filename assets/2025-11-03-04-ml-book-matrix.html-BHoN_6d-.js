import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as e,a as d,o as i}from"./app-BsNixVBM.js";const r={};function s(n,t){return i(),e("div",null,[...t[0]||(t[0]=[d(`<h2 id="🧭-标题备选" tabindex="-1"><a class="header-anchor" href="#🧭-标题备选"><span>🧭 标题备选</span></a></h2><ol><li><strong>别被矩阵吓到：机器学习的底层，其实全是线性代数</strong></li><li><strong>线性代数是机器学习的语言：从向量到PCA，一次讲透</strong></li><li><strong>机器学习的灵魂不是AI，而是矩阵</strong></li><li><strong>当你看懂向量、矩阵和PCA，机器学习才真正入门</strong></li><li><strong>数学不抽象：一文讲透机器学习背后的线性世界</strong></li></ol><hr><h2 id="🖼-封面文案" tabindex="-1"><a class="header-anchor" href="#🖼-封面文案"><span>🖼 封面文案</span></a></h2><blockquote><p>所有的智能背后，都是线性代数在默默支撑。<br> 理解矩阵，你就理解了机器学习的底层逻辑。</p></blockquote><p>（配图建议：矩阵格点、空间投影、线性几何感强的视觉）</p><hr><h2 id="✍️-摘要-引导点击" tabindex="-1"><a class="header-anchor" href="#✍️-摘要-引导点击"><span>✍️ 摘要（引导点击）</span></a></h2><blockquote><p>机器学习的底层不是代码，而是数学。<br> 而数学的核心语言，就是线性代数。<br> 本文带你从「向量」到「PCA」，一步步看清算法背后的几何世界。<br> 一旦看懂，你会发现：那些复杂模型，其实都在“算投影”。</p></blockquote><hr><h2 id="📖-正文-线性代数与机器学习的几何世界" tabindex="-1"><a class="header-anchor" href="#📖-正文-线性代数与机器学习的几何世界"><span>📖 正文：线性代数与机器学习的几何世界</span></a></h2><p>很多人学机器学习时，最困惑的部分就是线性代数。<br> 那一堆向量、矩阵、特征值、SVD，看似抽象又难以想象。</p><p>但等你深入一点，就会发现——<br> 几乎所有算法，从线性回归到神经网络，从推荐系统到图像压缩，<br> 底层全是线性代数在“操盘”。</p><hr><h3 id="一、向量-数据的最小单位" tabindex="-1"><a class="header-anchor" href="#一、向量-数据的最小单位"><span>一、向量：数据的最小单位</span></a></h3><p>在机器学习里，一个样本往往就表示成一个向量。<br> 比如一套房子的特征：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>x = [面积, 卧室数, 楼层, 城区编码]</span></span>
<span class="line"><span>x = [120, 3, 2, 5]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>这就是一个四维向量。</p><ul><li>每个数是一个特征</li><li>整个向量代表这个房子在“特征空间”中的位置</li></ul><p>可以这么理解：<br> 每个样本是一颗点，特征轴就是它所在的坐标系。<br> 所有样本组成的数据世界，就是一个高维空间。</p><hr><h3 id="二、特征空间-模型学习的战场" tabindex="-1"><a class="header-anchor" href="#二、特征空间-模型学习的战场"><span>二、特征空间：模型学习的战场</span></a></h3><p>“模型学习”这件事，其实就是在这个高维空间里，找一条分界线（或超平面）。</p><p>以逻辑回归为例，它学到的是：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>w₁x₁ + w₂x₂ + b = 0</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>这条“超平面”把点分成两边：一边是正类，一边是负类。</p><p>所以你可以这样理解：</p><blockquote><p>向量 = 数据点，<br> 模型 = 空间中的一条线或一个面，<br> 学习 = 不断调整这条线的位置。</p></blockquote><hr><h3 id="三、矩阵-批量样本的集合" tabindex="-1"><a class="header-anchor" href="#三、矩阵-批量样本的集合"><span>三、矩阵：批量样本的集合</span></a></h3><p>如果一个样本是向量，那么多个样本就组成矩阵：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>X =</span></span>
<span class="line"><span>[x₁₁ x₁₂ … x₁d</span></span>
<span class="line"><span> x₂₁ x₂₂ … x₂d</span></span>
<span class="line"><span> …</span></span>
<span class="line"><span> xn₁ xn₂ … xnd]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中：</p><ul><li>n：样本数</li><li>d：特征数</li></ul><p>几乎所有机器学习算法，起点都是这个矩阵 X。</p><p>比如线性回归：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>y = Xw + b</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>矩阵乘法在这里的意义非常直观：</p><blockquote><p>它在做「加权求和」——<br> 每个特征乘上权重，再把它们加起来，就是预测值。</p></blockquote><p>神经网络其实也是在不断地重复这个动作：<br> 线性变换 + 非线性激活。<br> 看似复杂，其实底层全是矩阵乘法。</p><hr><h3 id="四、矩阵的常见操作-它们各自干嘛的" tabindex="-1"><a class="header-anchor" href="#四、矩阵的常见操作-它们各自干嘛的"><span>四、矩阵的常见操作（它们各自干嘛的）</span></a></h3><table><thead><tr><th>操作</th><th>含义</th><th>用途</th></tr></thead><tbody><tr><td>转置 (A^T)</td><td>行列互换</td><td>求相似度、计算协方差</td></tr><tr><td>逆矩阵 (A^{-1})</td><td>方程求解</td><td>线性回归正规方程法</td></tr><tr><td>迹（Trace）</td><td>对角线之和</td><td>衡量方差总量</td></tr><tr><td>行列式（Det）</td><td>矩阵“体积”</td><td>判断是否可逆、PCA</td></tr></tbody></table><p>一句话总结：</p><blockquote><p>向量描述“点”，矩阵描述“变换”。</p></blockquote><hr><h3 id="五、特征值与特征向量-空间的-主方向" tabindex="-1"><a class="header-anchor" href="#五、特征值与特征向量-空间的-主方向"><span>五、特征值与特征向量：空间的“主方向”</span></a></h3><p>这一对概念听起来抽象，其实特别形象。</p><p>当矩阵 A 作用在一个向量 v 上，如果只改变长度，不改方向：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>Av = λv</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>那这个 v 就是特征向量，λ 是特征值。</p><p>也就是说：</p><blockquote><p>特征向量是「不被扭曲方向的轴」；<br> 特征值是「沿这个方向被拉伸的倍数」。</p></blockquote><p>这就是为什么 PCA（主成分分析）要做特征值分解——<br> 它就是在找数据中“变化最明显的方向”。</p><hr><h3 id="六、svd-通用的矩阵分解神器" tabindex="-1"><a class="header-anchor" href="#六、svd-通用的矩阵分解神器"><span>六、SVD：通用的矩阵分解神器</span></a></h3><p>SVD（奇异值分解）是线性代数里最强大的工具之一：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>A = U Σ Vᵀ</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>可以把它想象成：</p><blockquote><p>把任意矩阵拆成 “旋转 + 缩放 + 再旋转”。</p></blockquote><p>这玩意几乎无处不在：</p><ul><li>推荐系统：分解成“用户向量 × 商品向量”，预测偏好</li><li>图像压缩：保留前几个奇异值就能还原主要图像</li><li>NLP：潜在语义分析（LSA）</li><li>降噪：去掉小奇异值对应的“噪声维度”</li></ul><p>所以你能看到的很多“AI 应用”，本质都是矩阵分解的艺术。</p><hr><h3 id="七、pca-用最少维度保留最多信息" tabindex="-1"><a class="header-anchor" href="#七、pca-用最少维度保留最多信息"><span>七、PCA：用最少维度保留最多信息</span></a></h3><p>PCA（主成分分析）的目标很简单：</p><blockquote><p>把高维数据投影到低维空间，但尽量保留信息量。</p></blockquote><p>做法是：</p><ol><li>计算协方差矩阵</li><li>求出特征值和特征向量</li><li>选出前几个最大的方向</li><li>投影到这些方向上</li></ol><p>几何意义就一句话：</p><blockquote><p>把数据“旋转”到变化最明显的几个轴上。</p></blockquote><p>这样既能压缩维度，又能保留主要结构。<br> 在图像压缩、降噪、可视化里都用得上。</p><hr><h3 id="八、小结-所有智能都建立在线性世界之上" tabindex="-1"><a class="header-anchor" href="#八、小结-所有智能都建立在线性世界之上"><span>八、小结：所有智能都建立在线性世界之上</span></a></h3><table><thead><tr><th>概念</th><th>本质作用</th><th>在机器学习中的角色</th></tr></thead><tbody><tr><td>向量空间</td><td>表示样本与特征</td><td>特征表示、语义嵌入</td></tr><tr><td>矩阵运算</td><td>批量计算与线性变换</td><td>模型训练、预测</td></tr><tr><td>特征值分解</td><td>找主方向</td><td>PCA、稳定性分析</td></tr><tr><td>奇异值分解</td><td>通用矩阵分解</td><td>推荐、压缩、降噪</td></tr><tr><td>PCA</td><td>信息最大化投影</td><td>降维、特征提取</td></tr></tbody></table><p>机器学习离不开数学，而数学的底层，是线性代数。<br> 看懂矩阵和向量，你才真正理解模型背后的“逻辑结构”。<br> 那时你会发现：<br> 所有复杂的智能，其实都源自线性的优雅。</p><hr><h1 id="第4章-线性代数与矩阵运算" tabindex="-1"><a class="header-anchor" href="#第4章-线性代数与矩阵运算"><span>第4章　线性代数与矩阵运算</span></a></h1><p>非常好，这一章是很多人“入门机器学习”时最模糊、但“深入机器学习”后才恍然大悟的部分。</p><p>几乎所有的算法——从线性回归到神经网络，从聚类到推荐系统——底层都离不开 <strong>线性代数</strong>。</p><p>下面我会以「直观解释 + 数学形式 + 实际应用」的方式来系统讲解。</p><h2 id="_4-1-向量空间与特征表示" tabindex="-1"><a class="header-anchor" href="#_4-1-向量空间与特征表示"><span>4.1 向量空间与特征表示</span></a></h2><h3 id="🧩-向量-vector" tabindex="-1"><a class="header-anchor" href="#🧩-向量-vector"><span>🧩 向量（Vector）</span></a></h3><p>在机器学习中，<strong>向量 = 一个样本的特征集合</strong>。<br> 例如，一个房屋样本：<br> [<br> x = [\\text{面积}, \\text{卧室数}, \\text{楼层}, \\text{城区编码}]<br> ]<br> 可写作一个 4 维向量：<br> [<br> x = [120, 3, 2, 5]<br> ]</p><p>向量是机器学习中最基础的数据单位。</p><ul><li>每个维度表示一个特征（feature）</li><li>整个向量表示样本在特征空间中的位置</li></ul><hr><h3 id="🧠-向量空间-vector-space" tabindex="-1"><a class="header-anchor" href="#🧠-向量空间-vector-space"><span>🧠 向量空间（Vector Space）</span></a></h3><p>一组向量加上线性运算（加法、数乘）就构成一个向量空间。</p><p>例如，二维空间的所有点 ((x_1, x_2)) 构成一个二维向量空间。<br> 在机器学习中，这种空间常称为「<strong>特征空间（Feature Space）</strong>」。</p><hr><h3 id="📈-特征表示-feature-representation" tabindex="-1"><a class="header-anchor" href="#📈-特征表示-feature-representation"><span>📈 特征表示（Feature Representation）</span></a></h3><p>向量的几何意义非常重要：</p><ul><li>每个样本是一个点；</li><li>每个维度是一个特征轴；</li><li>模型的“学习”其实就是在高维空间中寻找“划分这些点的超平面”。</li></ul><p>例如，逻辑回归学习的其实就是一个超平面：<br> [<br> w_1x_1 + w_2x_2 + b = 0<br> ]<br> 它把空间分成“正类”和“负类”。</p><hr><h3 id="🧮-向量运算与几何意义" tabindex="-1"><a class="header-anchor" href="#🧮-向量运算与几何意义"><span>🧮 向量运算与几何意义</span></a></h3><table><thead><tr><th>运算</th><th>数学形式</th><th>含义</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>点积（Dot Product）</td><td>( a \\cdot b = \\sum_i a_i b_i )</td><td>衡量相似度（方向是否一致）</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>范数（Norm）</td><td>(</td><td></td><td>a</td><td></td><td>= \\sqrt{\\sum_i a_i^2} )</td><td>向量长度</td><td></td><td></td><td></td><td></td></tr><tr><td>余弦相似度</td><td>( \\cos\\theta = \\frac{a \\cdot b}{</td><td></td><td>a</td><td></td><td>,</td><td></td><td>b</td><td></td><td>} )</td><td>两个样本相似度</td></tr><tr><td>线性组合</td><td>( c = \\alpha a + \\beta b )</td><td>新特征的组合方式</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><p>💡 在推荐系统中，“用户向量”和“商品向量”的点积就代表偏好相似度（矩阵分解模型的核心）。</p><hr><h2 id="_4-2-矩阵运算在学习算法中的作用" tabindex="-1"><a class="header-anchor" href="#_4-2-矩阵运算在学习算法中的作用"><span>4.2 矩阵运算在学习算法中的作用</span></a></h2><h3 id="🧩-矩阵的定义" tabindex="-1"><a class="header-anchor" href="#🧩-矩阵的定义"><span>🧩 矩阵的定义</span></a></h3><p>矩阵就是一组向量的集合。<br> 如果每个样本是一个向量，那么所有样本组成的数据集就是一个矩阵：</p><p>[<br> X =<br> \\begin{bmatrix}<br> x_{11} &amp; x_{12} &amp; \\dots &amp; x_{1d} <br> x_{21} &amp; x_{22} &amp; \\dots &amp; x_{2d} <br> \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots <br> x_{n1} &amp; x_{n2} &amp; \\dots &amp; x_{nd}<br> \\end{bmatrix}<br> ]</p><p>其中：</p><ul><li>( n )：样本数</li><li>( d )：特征维度</li></ul><p>这是所有算法的起点。</p><hr><h3 id="🧮-矩阵乘法与模型训练" tabindex="-1"><a class="header-anchor" href="#🧮-矩阵乘法与模型训练"><span>🧮 矩阵乘法与模型训练</span></a></h3><p>矩阵乘法背后隐藏着「批量预测」与「特征加权」。</p><p>例如线性回归：<br> [<br> y = Xw + b<br> ]</p><ul><li>( X )：输入数据矩阵（n×d）</li><li>( w )：参数权重向量（d×1）</li><li>( y )：预测结果（n×1）</li></ul><p>矩阵乘法的几何意义：</p><blockquote><p>把原始数据投影到“权重方向”，得到模型输出。</p></blockquote><p>在神经网络中，权重层的计算其实就是不断地执行 ( XW + b ) 的线性变换，只是叠了很多层。</p><hr><h3 id="📘-矩阵的核心操作及意义" tabindex="-1"><a class="header-anchor" href="#📘-矩阵的核心操作及意义"><span>📘 矩阵的核心操作及意义</span></a></h3><table><thead><tr><th>操作</th><th>含义</th><th>实例</th></tr></thead><tbody><tr><td>转置（(A^T)）</td><td>行列互换</td><td>用于求相似度或协方差</td></tr><tr><td>逆矩阵（(A^{-1})）</td><td>线性方程求解</td><td>线性回归正规方程法</td></tr><tr><td>迹（Trace）</td><td>对角线和</td><td>协方差矩阵的方差和</td></tr><tr><td>行列式（Determinant）</td><td>矩阵体积（可逆性）</td><td>PCA、线性无关性判断</td></tr></tbody></table><p>💡 在优化算法中，矩阵求导（Jacobian/Hessian）决定了梯度下降的方向和收敛速度。</p><hr><h2 id="_4-3-特征值与奇异值分解" tabindex="-1"><a class="header-anchor" href="#_4-3-特征值与奇异值分解"><span>4.3 特征值与奇异值分解</span></a></h2><h3 id="🧠-特征值与特征向量-eigenvalue-eigenvector" tabindex="-1"><a class="header-anchor" href="#🧠-特征值与特征向量-eigenvalue-eigenvector"><span>🧠 特征值与特征向量（Eigenvalue &amp; Eigenvector）</span></a></h3><p>定义：<br> [<br> A v = \\lambda v<br> ]</p><p>表示：矩阵 ( A ) 作用在向量 ( v ) 上，只改变它的长度（λ倍），不改变方向。</p><ul><li>( v )：特征向量（方向不变的向量）</li><li>( \\lambda )：特征值（方向上的伸缩比例）</li></ul><p>📈 几何意义：<br> 矩阵（线性变换）会把空间拉伸/压缩，而特征向量就是那些「拉伸方向不变」的轴。</p><p>📘 应用：</p><ul><li>PCA 降维（找出方差最大方向）</li><li>图的谱聚类（Graph Laplacian 特征值分解）</li><li>动态系统稳定性分析（通过特征值判断系统是否收敛）</li></ul><hr><h3 id="🔹-奇异值分解-svd-singular-value-decomposition" tabindex="-1"><a class="header-anchor" href="#🔹-奇异值分解-svd-singular-value-decomposition"><span>🔹 奇异值分解（SVD, Singular Value Decomposition）</span></a></h3><p>SVD 是矩阵分解中最重要的一个：<br> [<br> A = U \\Sigma V^T<br> ]</p><p>其中：</p><ul><li>( U )：左奇异向量（样本空间）</li><li>( V )：右奇异向量（特征空间）</li><li>( \\Sigma )：奇异值（特征的重要性）</li></ul><p>💡 可以理解为：</p><blockquote><p>把任意矩阵分解成“旋转 + 缩放 + 再旋转”。</p></blockquote><p>📘 应用举例：</p><ul><li>信息检索中的 <strong>LSA（潜在语义分析）</strong></li><li>图像压缩（保留前几个奇异值即可）</li><li>推荐系统中的 <strong>矩阵分解（SVD）</strong></li><li>降维与噪声过滤（保留主要成分）</li></ul><hr><h2 id="_4-4-pca-的数学推导与降维本质" tabindex="-1"><a class="header-anchor" href="#_4-4-pca-的数学推导与降维本质"><span>4.4 PCA 的数学推导与降维本质</span></a></h2><h3 id="🎯-pca-主成分分析-的目标" tabindex="-1"><a class="header-anchor" href="#🎯-pca-主成分分析-的目标"><span>🎯 PCA（主成分分析）的目标</span></a></h3><blockquote><p>找到一组新的坐标轴，使数据在这些轴上的投影方差最大，同时维度尽量少。</p></blockquote><p>换句话说：</p><ul><li>我们希望在保留最多信息的同时，去掉冗余维度。</li></ul><hr><h3 id="🧮-pca-的数学推导" tabindex="-1"><a class="header-anchor" href="#🧮-pca-的数学推导"><span>🧮 PCA 的数学推导</span></a></h3><p>给定样本矩阵 ( X )（n×d），假设已中心化（均值为0）：</p><ol><li><p>计算协方差矩阵：<br> [<br> C = \\frac{1}{n} X^T X<br> ]</p></li><li><p>对 ( C ) 做特征值分解：<br> [<br> C v_i = \\lambda_i v_i<br> ]<br> 得到特征向量 ( v_i ) 与特征值 ( \\lambda_i )。</p></li><li><p>选择前 k 个最大特征值对应的向量组成矩阵 ( V_k )。</p></li><li><p>降维后的数据为：<br> [<br> X&#39; = X V_k<br> ]</p></li></ol><p>📘 几何意义：</p><ul><li>特征向量 = 数据最大方差方向</li><li>特征值 = 每个方向的信息量</li><li>投影后 = 把高维数据“旋转”到信息最集中的轴上</li></ul><hr><h3 id="📈-应用举例" tabindex="-1"><a class="header-anchor" href="#📈-应用举例"><span>📈 应用举例</span></a></h3><table><thead><tr><th>应用场景</th><th>说明</th></tr></thead><tbody><tr><td>图像压缩</td><td>保留前 50 个主成分即可恢复主要轮廓</td></tr><tr><td>降噪</td><td>去掉小特征值对应的噪声维度</td></tr><tr><td>可视化</td><td>将高维数据降到 2D / 3D</td></tr><tr><td>特征提取</td><td>作为深度学习前的预处理</td></tr></tbody></table><hr><h3 id="🧠-pca-与-svd-的关系" tabindex="-1"><a class="header-anchor" href="#🧠-pca-与-svd-的关系"><span>🧠 PCA 与 SVD 的关系</span></a></h3><p>事实上，PCA 可以用 SVD 实现：</p><p>[<br> X = U \\Sigma V^T<br> ]</p><p>则：</p><ul><li>协方差矩阵 ( C = X^T X = V \\Sigma^2 V^T )</li><li>主成分方向就是 ( V ) 的列向量</li></ul><hr><h2 id="🌍-小结" tabindex="-1"><a class="header-anchor" href="#🌍-小结"><span>🌍 小结</span></a></h2><table><thead><tr><th>概念</th><th>本质作用</th><th>机器学习中的角色</th></tr></thead><tbody><tr><td>向量空间</td><td>表示样本与特征</td><td>特征表示、语义嵌入</td></tr><tr><td>矩阵运算</td><td>批量计算与变换</td><td>模型训练、线性预测</td></tr><tr><td>特征值分解</td><td>方向与方差信息</td><td>稳定性、PCA、聚类</td></tr><tr><td>奇异值分解</td><td>通用分解工具</td><td>推荐系统、压缩、降噪</td></tr><tr><td>PCA</td><td>信息最大化投影</td><td>降维、特征提取、可视化</td></tr></tbody></table>`,161)])])}const h=a(r,[["render",s]]),o=JSON.parse('{"path":"/posts/ml/2025-11-03-04-ml-book-matrix.html","title":"第4章　线性代数与矩阵运算","lang":"zh-CN","frontmatter":{"title":"第4章　线性代数与矩阵运算","date":"2025-11-03T00:00:00.000Z","categories":["AI"],"tags":["ai","learn-note"],"published":true,"description":"🧭 标题备选 别被矩阵吓到：机器学习的底层，其实全是线性代数 线性代数是机器学习的语言：从向量到PCA，一次讲透 机器学习的灵魂不是AI，而是矩阵 当你看懂向量、矩阵和PCA，机器学习才真正入门 数学不抽象：一文讲透机器学习背后的线性世界 🖼 封面文案 所有的智能背后，都是线性代数在默默支撑。 理解矩阵，你就理解了机器学习的底层逻辑。 （配图建议：...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"第4章　线性代数与矩阵运算\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-11-03T00:00:00.000Z\\",\\"dateModified\\":\\"2025-12-27T05:15:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"老马啸西风\\",\\"url\\":\\"https://houbb.github.io\\"}]}"],["meta",{"property":"og:url","content":"https://houbb.github.io/awesome-ai-coding/posts/ml/2025-11-03-04-ml-book-matrix.html"}],["meta",{"property":"og:site_name","content":"老马啸西风"}],["meta",{"property":"og:title","content":"第4章　线性代数与矩阵运算"}],["meta",{"property":"og:description","content":"🧭 标题备选 别被矩阵吓到：机器学习的底层，其实全是线性代数 线性代数是机器学习的语言：从向量到PCA，一次讲透 机器学习的灵魂不是AI，而是矩阵 当你看懂向量、矩阵和PCA，机器学习才真正入门 数学不抽象：一文讲透机器学习背后的线性世界 🖼 封面文案 所有的智能背后，都是线性代数在默默支撑。 理解矩阵，你就理解了机器学习的底层逻辑。 （配图建议：..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-27T05:15:15.000Z"}],["meta",{"property":"article:tag","content":"learn-note"}],["meta",{"property":"article:tag","content":"ai"}],["meta",{"property":"article:published_time","content":"2025-11-03T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-27T05:15:15.000Z"}]]},"git":{"createdTime":1766812269000,"updatedTime":1766812515000,"contributors":[{"name":"bbhou","username":"bbhou","email":"1557740299@qq.com","commits":3,"url":"https://github.com/bbhou"}]},"readingTime":{"minutes":10.3,"words":3091},"filePathRelative":"posts/ml/2025-11-03-04-ml-book-matrix.md","excerpt":"<h2>🧭 标题备选</h2>\\n<ol>\\n<li><strong>别被矩阵吓到：机器学习的底层，其实全是线性代数</strong></li>\\n<li><strong>线性代数是机器学习的语言：从向量到PCA，一次讲透</strong></li>\\n<li><strong>机器学习的灵魂不是AI，而是矩阵</strong></li>\\n<li><strong>当你看懂向量、矩阵和PCA，机器学习才真正入门</strong></li>\\n<li><strong>数学不抽象：一文讲透机器学习背后的线性世界</strong></li>\\n</ol>\\n<hr>\\n<h2>🖼 封面文案</h2>\\n<blockquote>\\n<p>所有的智能背后，都是线性代数在默默支撑。<br>\\n理解矩阵，你就理解了机器学习的底层逻辑。</p>\\n</blockquote>","autoDesc":true}');export{h as comp,o as data};
