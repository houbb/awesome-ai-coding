import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as o,a as r,b as t,e as a,o as d}from"./app-CIr_XAMB.js";const i={};function p(h,e){return d(),o("div",null,[...e[0]||(e[0]=[r('<p>第 16 章「深度学习的兴起」是整本书的转折点——前面几章都在讲传统机器学习（依靠特征工程与统计建模），而从这里开始，进入了“自动特征学习”与“端到端优化”的新时代。</p><p>这一章不仅要讲技术，更要让读者理解：<strong>深度学习为什么是机器学习的必然演化方向。</strong></p><hr><h1 id="第16章-深度学习的兴起" tabindex="-1"><a class="header-anchor" href="#第16章-深度学习的兴起"><span>第16章　深度学习的兴起</span></a></h1><p>深度学习（Deep Learning）本质上是机器学习的一种方法，但它在“表达能力”“学习范式”“计算规模”上带来了革命性飞跃。</p><p>它让机器第一次具备了从数据中<strong>自动提取复杂模式</strong>的能力，推动了计算机视觉、语音识别、自然语言处理等领域的全面突破。</p><hr><h2 id="_16-1-神经网络的数学基础" tabindex="-1"><a class="header-anchor" href="#_16-1-神经网络的数学基础"><span><strong>16.1 神经网络的数学基础</strong></span></a></h2><h3 id="🧩-1️⃣-神经元模型-perceptron" tabindex="-1"><a class="header-anchor" href="#🧩-1️⃣-神经元模型-perceptron"><span>🧩 1️⃣ 神经元模型（Perceptron）</span></a></h3><p>最早的人工神经元模型由 McCulloch &amp; Pitts（1943）提出，用来模拟人脑神经元的行为。</p><p>一个最基本的神经元计算如下：<br> [<br> y = f(w_1x_1 + w_2x_2 + ... + w_nx_n + b)<br> ]<br> 其中：</p><ul><li>( x_i )：输入特征</li><li>( w_i )：权重（表示输入的重要性）</li><li>( b )：偏置项</li><li>( f )：激活函数（决定输出的非线性）</li></ul><p><strong>直觉理解</strong>：</p><ul><li>神经元就像一个「加权求和 + 激活开关」；</li><li>多个神经元层层组合，就能逼近任意复杂的非线性函数。</li></ul><hr><h3 id="🔹-2️⃣-激活函数-activation-function" tabindex="-1"><a class="header-anchor" href="#🔹-2️⃣-激活函数-activation-function"><span>🔹 2️⃣ 激活函数（Activation Function）</span></a></h3><p>激活函数为神经网络引入<strong>非线性</strong>，否则网络就退化为线性回归。</p><table><thead><tr><th>函数</th><th>表达式</th><th>特点</th></tr></thead><tbody><tr><td>Sigmoid</td><td>( f(x) = \\frac{1}{1+e^{-x}} )</td><td>输出在(0,1)，但易梯度消失</td></tr><tr><td>Tanh</td><td>( f(x) = \\frac{e^x - e<sup>{-x}}{e</sup>x + e^{-x}} )</td><td>输出在(-1,1)，收敛更快</td></tr><tr><td>ReLU</td><td>( f(x) = \\max(0, x) )</td><td>简单高效，是深度网络主流</td></tr><tr><td>Leaky ReLU / GELU</td><td>改进梯度问题</td><td>现代 Transformer 常用</td></tr></tbody></table><hr><h3 id="🔹-3️⃣-神经网络的结构组成" tabindex="-1"><a class="header-anchor" href="#🔹-3️⃣-神经网络的结构组成"><span>🔹 3️⃣ 神经网络的结构组成</span></a></h3><p>一个典型的神经网络由以下部分构成：</p><ul><li><strong>输入层</strong>：接收原始数据；</li><li><strong>隐藏层</strong>：通过加权连接与激活函数进行特征变换；</li><li><strong>输出层</strong>：输出预测结果（如分类概率、回归值）；</li><li><strong>权重矩阵</strong>：定义层与层之间的连接强度；</li><li><strong>损失函数</strong>：衡量预测结果与真实值的误差。</li></ul>',22),t("p",null,[a("📊 数学表示："),t("br"),a(" ["),t("br"),a(" \\begin{aligned}"),t("br"),a(" h^{(1)} &= f(W^{(1)}x + b^{(1)}) "),t("br"),a(" h^{(2)} &= f(W"),t("sup",{"(1)":""},"{(2)}h"),a(" + b^{(2)}) "),t("br"),a(" \\hat{y} &= g(W"),t("sup",{"(2)":""},"{(3)}h"),a(" + b^{(3)})"),t("br"),a(" \\end{aligned}"),t("br"),a(" ]")],-1),r('<p>这其实就是一个多层复合函数，深度学习的“深”——指的是<strong>这种复合层数的加深</strong>。</p><hr><h2 id="_16-2-bp算法与梯度传播" tabindex="-1"><a class="header-anchor" href="#_16-2-bp算法与梯度传播"><span><strong>16.2 BP算法与梯度传播</strong></span></a></h2><h3 id="🔹-1️⃣-为什么需要反向传播-backpropagation-bp" tabindex="-1"><a class="header-anchor" href="#🔹-1️⃣-为什么需要反向传播-backpropagation-bp"><span>🔹 1️⃣ 为什么需要反向传播（Backpropagation, BP）</span></a></h3><p>在多层网络中，权重非常多（可能上百万），我们不可能手动调。</p><p>BP算法通过<strong>链式法则（Chain Rule）</strong>，高效地计算每个参数对损失函数的梯度。</p><hr><h3 id="🔹-2️⃣-梯度传播的基本原理" tabindex="-1"><a class="header-anchor" href="#🔹-2️⃣-梯度传播的基本原理"><span>🔹 2️⃣ 梯度传播的基本原理</span></a></h3><p>损失函数 ( L ) 衡量预测 (\\hat{y}) 与真实 (y) 的误差。</p><p>目标是：<br> [<br> \\min_{\\theta} L(\\hat{y}, y)<br> ]<br> 通过梯度下降（Gradient Descent）更新参数：<br> [<br> \\theta \\leftarrow \\theta - \\eta \\frac{\\partial L}{\\partial \\theta}<br> ]<br> 其中 (\\eta) 为学习率。</p><p>反向传播算法利用链式法则：<br> [<br> \\frac{\\partial L}{\\partial w_i} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_i}<br> ]<br> 逐层计算梯度并更新参数。</p><hr><h3 id="🔹-3️⃣-直观理解-bp" tabindex="-1"><a class="header-anchor" href="#🔹-3️⃣-直观理解-bp"><span>🔹 3️⃣ 直观理解 BP</span></a></h3><p>可以把 BP 理解成一种「误差回传」机制：</p><blockquote><p>前向传播算输出，反向传播算“责怪”——谁对错误负责，就调整谁。</p></blockquote><p>这种思想让神经网络能像“经验学习者”一样，自动修正内部连接权重。</p><hr><h3 id="🔹-4️⃣-优化的关键要点" tabindex="-1"><a class="header-anchor" href="#🔹-4️⃣-优化的关键要点"><span>🔹 4️⃣ 优化的关键要点</span></a></h3><ul><li><strong>梯度消失</strong>：深层网络中梯度逐层衰减（Sigmoid 易出现）</li><li><strong>梯度爆炸</strong>：梯度过大导致权重震荡（需梯度裁剪）</li><li><strong>初始化策略</strong>：Xavier、He 初始化</li><li><strong>批量归一化（Batch Normalization）</strong>：稳定分布，加速训练</li></ul><hr><h2 id="_16-3-深度结构带来的表达能力" tabindex="-1"><a class="header-anchor" href="#_16-3-深度结构带来的表达能力"><span><strong>16.3 深度结构带来的表达能力</strong></span></a></h2><h3 id="🔹-1️⃣-为什么-深-很重要" tabindex="-1"><a class="header-anchor" href="#🔹-1️⃣-为什么-深-很重要"><span>🔹 1️⃣ 为什么“深”很重要？</span></a></h3><p>理论上，单层神经网络（感知机）只能表示线性可分函数；<br> 两层可以逼近任意连续函数（万能逼近定理）。<br> 但在实际中，“更深”带来的不是重复计算，而是<strong>层级抽象能力的提升</strong>。</p><hr><h3 id="🔹-2️⃣-层级表示-hierarchical-representation" tabindex="-1"><a class="header-anchor" href="#🔹-2️⃣-层级表示-hierarchical-representation"><span>🔹 2️⃣ 层级表示（Hierarchical Representation）</span></a></h3><p>深度结构允许网络从<strong>低层特征 → 高层语义</strong>逐步抽象：</p><table><thead><tr><th>层级</th><th>示例（图像）</th><th>示例（文本）</th></tr></thead><tbody><tr><td>低层</td><td>边缘、颜色、纹理</td><td>字符、词</td></tr><tr><td>中层</td><td>局部形状、部件</td><td>短语、句法结构</td></tr><tr><td>高层</td><td>物体、场景语义</td><td>语义含义、情感</td></tr></tbody></table><p>这就是深度学习区别于传统机器学习的根本所在：</p><blockquote><p>它不依赖人工特征，而能自动学习到逐层抽象的表示。</p></blockquote><hr><h3 id="🔹-3️⃣-表达能力的数学解释" tabindex="-1"><a class="header-anchor" href="#🔹-3️⃣-表达能力的数学解释"><span>🔹 3️⃣ 表达能力的数学解释</span></a></h3>',31),t("p",null,[a("每一层相当于对输入空间的非线性变换："),t("br"),a(" ["),t("br"),a(" h^{(l)} = f(W"),t("sup",{"(l-1)":""},"{(l)}h"),a(" + b^{(l)})"),t("br"),a(" ]"),t("br"),a(" 多层复合后，网络能学习到极为复杂的决策边界。")],-1),r('<p><strong>直觉上：</strong></p><ul><li>线性模型只能画“平面”；</li><li>浅层模型能画“曲线”；</li><li>深层模型能画出“任意复杂的形状”。</li></ul><hr><h3 id="🔹-4️⃣-表达能力与可解释性权衡" tabindex="-1"><a class="header-anchor" href="#🔹-4️⃣-表达能力与可解释性权衡"><span>🔹 4️⃣ 表达能力与可解释性权衡</span></a></h3><p>深度网络虽然强大，但：</p><ul><li>参数巨大，易过拟合；</li><li>难以解释（“黑盒”问题）；</li><li>对数据分布敏感；<br> 因此通常需要大规模数据与正则化技巧（Dropout、L2、BN）。</li></ul><hr><h2 id="_16-4-从传统特征工程到自动特征学习" tabindex="-1"><a class="header-anchor" href="#_16-4-从传统特征工程到自动特征学习"><span><strong>16.4 从传统特征工程到自动特征学习</strong></span></a></h2><h3 id="🧩-1️⃣-传统机器学习的痛点" tabindex="-1"><a class="header-anchor" href="#🧩-1️⃣-传统机器学习的痛点"><span>🧩 1️⃣ 传统机器学习的痛点</span></a></h3><p>在传统 ML 中：</p><ul><li>模型本身能力有限（如线性回归、SVM）；</li><li>性能好坏主要取决于“特征工程”质量；</li><li>人工特征需要专家知识（如图像的 SIFT、文本的 TF-IDF）。</li></ul><h3 id="🔹-2️⃣-深度学习的革命" tabindex="-1"><a class="header-anchor" href="#🔹-2️⃣-深度学习的革命"><span>🔹 2️⃣ 深度学习的革命</span></a></h3><p>深度网络通过<strong>端到端学习（End-to-End Learning）</strong>，<br> 直接从原始数据（像素、语音波形、字符序列）中自动提取特征。</p><p>[<br> \\text{Raw Data} \\rightarrow \\text{Neural Network} \\rightarrow \\text{Prediction}<br> ]</p><p>无需人工介入，就能自动学习最优特征表达。</p><p>📘 举例：</p><ul><li>图像：CNN 自动提取边缘 → 形状 → 物体；</li><li>文本：RNN/Transformer 自动提取语义特征；</li><li>语音：WaveNet 直接从波形中学语音结构。</li></ul><hr><h3 id="🔹-3️⃣-自动特征学习的本质" tabindex="-1"><a class="header-anchor" href="#🔹-3️⃣-自动特征学习的本质"><span>🔹 3️⃣ 自动特征学习的本质</span></a></h3><blockquote><p>深度学习是让“模型”替代“专家”，通过数据自动发现抽象结构。</p></blockquote><p>这不仅极大降低了领域经验的门槛，也让模型泛化能力更强。</p><hr><h3 id="🔹-4️⃣-机器学习范式的三次演变" tabindex="-1"><a class="header-anchor" href="#🔹-4️⃣-机器学习范式的三次演变"><span>🔹 4️⃣ 机器学习范式的三次演变</span></a></h3><table><thead><tr><th>阶段</th><th>核心特征</th><th>代表模型</th><th>特点</th></tr></thead><tbody><tr><td>统计学习阶段</td><td>手工特征 + 简单模型</td><td>SVM, LR, KNN</td><td>对特征敏感</td></tr><tr><td>特征学习阶段</td><td>半自动特征学习</td><td>Autoencoder</td><td>特征逐步抽象</td></tr><tr><td>深度学习阶段</td><td>端到端特征学习</td><td>CNN, RNN, Transformer</td><td>自动学习语义</td></tr></tbody></table><hr><h2 id="✅-小结" tabindex="-1"><a class="header-anchor" href="#✅-小结"><span>✅ 小结</span></a></h2><table><thead><tr><th>小节</th><th>关键思想</th><th>代表方法</th></tr></thead><tbody><tr><td>神经网络基础</td><td>模拟人脑神经元的非线性组合</td><td>感知机、激活函数</td></tr><tr><td>BP算法</td><td>链式法则反向传播梯度</td><td>Gradient Descent, SGD</td></tr><tr><td>深度结构</td><td>层级抽象带来更强表达力</td><td>多层前馈网络</td></tr><tr><td>自动特征学习</td><td>从人工特征到端到端学习</td><td>CNN, Transformer</td></tr></tbody></table>',27)])])}const c=n(i,[["render",p]]),b=JSON.parse('{"path":"/posts/ml/2025-11-03-16-ml-book-deeplearning.html","title":"第16章　深度学习的兴起","lang":"zh-CN","frontmatter":{"title":"第16章　深度学习的兴起","date":"2025-11-03T00:00:00.000Z","categories":["AI"],"tags":["ai","learn-note"],"published":true,"description":"第 16 章「深度学习的兴起」是整本书的转折点——前面几章都在讲传统机器学习（依靠特征工程与统计建模），而从这里开始，进入了“自动特征学习”与“端到端优化”的新时代。 这一章不仅要讲技术，更要让读者理解：深度学习为什么是机器学习的必然演化方向。 第16章 深度学习的兴起 深度学习（Deep Learning）本质上是机器学习的一种方法，但它在“表达能力...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"第16章　深度学习的兴起\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-11-03T00:00:00.000Z\\",\\"dateModified\\":\\"2025-12-27T05:15:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"老马啸西风\\",\\"url\\":\\"https://houbb.github.io\\"}]}"],["meta",{"property":"og:url","content":"https://houbb.github.io/awesome-ai-coding/posts/ml/2025-11-03-16-ml-book-deeplearning.html"}],["meta",{"property":"og:site_name","content":"老马啸西风"}],["meta",{"property":"og:title","content":"第16章　深度学习的兴起"}],["meta",{"property":"og:description","content":"第 16 章「深度学习的兴起」是整本书的转折点——前面几章都在讲传统机器学习（依靠特征工程与统计建模），而从这里开始，进入了“自动特征学习”与“端到端优化”的新时代。 这一章不仅要讲技术，更要让读者理解：深度学习为什么是机器学习的必然演化方向。 第16章 深度学习的兴起 深度学习（Deep Learning）本质上是机器学习的一种方法，但它在“表达能力..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-27T05:15:15.000Z"}],["meta",{"property":"article:tag","content":"learn-note"}],["meta",{"property":"article:tag","content":"ai"}],["meta",{"property":"article:published_time","content":"2025-11-03T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-27T05:15:15.000Z"}]]},"git":{"createdTime":1766812269000,"updatedTime":1766812515000,"contributors":[{"name":"bbhou","username":"bbhou","email":"1557740299@qq.com","commits":3,"url":"https://github.com/bbhou"}]},"readingTime":{"minutes":5.92,"words":1777},"filePathRelative":"posts/ml/2025-11-03-16-ml-book-deeplearning.md","excerpt":"<p>第 16 章「深度学习的兴起」是整本书的转折点——前面几章都在讲传统机器学习（依靠特征工程与统计建模），而从这里开始，进入了“自动特征学习”与“端到端优化”的新时代。</p>\\n<p>这一章不仅要讲技术，更要让读者理解：<strong>深度学习为什么是机器学习的必然演化方向。</strong></p>\\n<hr>\\n<h1>第16章　深度学习的兴起</h1>\\n<p>深度学习（Deep Learning）本质上是机器学习的一种方法，但它在“表达能力”“学习范式”“计算规模”上带来了革命性飞跃。</p>\\n<p>它让机器第一次具备了从数据中<strong>自动提取复杂模式</strong>的能力，推动了计算机视觉、语音识别、自然语言处理等领域的全面突破。</p>","autoDesc":true}');export{c as comp,b as data};
